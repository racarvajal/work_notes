{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to gather data from $z > 5.5$ QSOs.\n",
    "\n",
    "We want to obtain data from several surveys and catalogs in order to  \n",
    "manipulate them and try to obtain meaningful correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the packages to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the first line working, you need  \n",
    "to run the following lines:\n",
    "\n",
    "```bash\n",
    " conda install nodejs\n",
    " pip install ipympl\n",
    " pip install --upgrade jupyterlab\n",
    " jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    " jupyter labextension install jupyter-matplotlib\n",
    " jupyter nbextension enable --py widgetsnbextension\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Static plots\n",
    "#%matplotlib ipympl\n",
    "# Interactive plots\n",
    "import numpy as np\n",
    "# import matplotlib.cm as cm\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.colors as mcolors\n",
    "# import matplotlib.patheffects as mpe\n",
    "# import matplotlib.patheffects as path_effects\n",
    "# from matplotlib.ticker import FormatStrFormatter\n",
    "# from matplotlib.patches import Polygon\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy.table import Column\n",
    "from astropy.table import MaskedColumn\n",
    "from astropy.table import hstack\n",
    "from astropy.table import vstack\n",
    "from astropy.table import join\n",
    "from astropy.wcs import WCS\n",
    "from astropy import units as u\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astroquery.simbad import Simbad\n",
    "from astroquery.ned import Ned\n",
    "import getpass\n",
    "import progressbar\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the spectral index $\\alpha$ from different sources  \n",
    "to be used in the luminosity calculations (K-correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_first = 0.5  # From FIRST data (Bornancini+2010)\n",
    "alpha_RG    = 1.0  # For radio galaxies (Verkhodanov & Khabibullina, 2010)\n",
    "alpha_alex  = 0.8  # Star-forming galaxies (Alexander+2003)\n",
    "alpha_smol  = 0.7  # Mean value from VLA-COSMOS 3GHz sample (Smolčić et al. 2017)\n",
    "alpha_butl  = 0.75  # From Butler et al., 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose one of the spectral indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_used  = alpha_butl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the cosmological properties to calculate luminosity distances and other quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmo       = FlatLambdaCDM(H0=70, Om0=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lum_from_flux(flux, redshift):  # Flux in mJy\n",
    "    lum_distance = cosmo.luminosity_distance(redshift).to(u.m).value  # in m\n",
    "    luminosity   = 4 * np.pi * lum_distance**2 * flux * 1e-3  * 1e-26 * (1 + redshift)**(alpha_used - 1)  # in W/Hz\n",
    "    return luminosity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One function, to derive luminosity distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def luminosity_distance(z_i, H0=70., WM=0.3, WV=0.7):\n",
    "    z_i   = np.array([z_i], dtype='float64').flatten()\n",
    "    c     = 299792.458 # velocity of light in km/sec\n",
    "    h     = H0 / 100.\n",
    "    WR    = 4.165E-5 / (h * h)   # includes 3 massless neutrino species, T0 = 2.72528\n",
    "    WK    = 1 - WM - WR - WV\n",
    "    azs   = 1.0 / (1 + z_i)\n",
    "    DTT   = 0.0\n",
    "    DCMR  = 0.0\n",
    "    # do integral over a=1/(1+z) from az to 1 in n steps, midpoint rule\n",
    "    n     = 1000  # number of points in integrals\n",
    "    DL_Mpcs = np.zeros_like(z_i)\n",
    "    for count, az in enumerate(azs):\n",
    "        a     = az + (1 - az) * (np.arange(0, n) + 0.5) / n\n",
    "        adot  = np.sqrt(WK + (WM / a) + (WR / (a * a)) + (WV * a * a))\n",
    "        for i in range(n):\n",
    "            # a    = az + (1 - az) * (i + 0.5) / n\n",
    "            # adot = math.sqrt(WK + (WM / a) + (WR / (a * a)) + (WV * a * a))\n",
    "            DTT  = DTT + 1. / adot[i]\n",
    "            DCMR = DCMR + 1. / (a[i] * adot[i])\n",
    "        DTT   = (1. - az) * DTT / n\n",
    "        DCMR  = (1. - az) * DCMR / n\n",
    "        # tangential comoving distance\n",
    "        ratio = 1.00\n",
    "        x     = np.sqrt(abs(WK)) * DCMR\n",
    "        if x > 0.1:\n",
    "            if WK > 0:\n",
    "                ratio =  0.5 * (np.exp(x) - np.exp(-x)) / x \n",
    "            else:\n",
    "                ratio = np.sin(x) / x\n",
    "        else:\n",
    "            y = x * x\n",
    "        if WK < 0: y = -y\n",
    "        ratio  = 1. + y / 6. + y * y / 120.\n",
    "        DCMT   = ratio * DCMR\n",
    "        DA     = az * DCMT\n",
    "        DL     = DA / (az * az)\n",
    "        DL_Mpc = (c / H0) * DL\n",
    "        DL_Mpcs[count] = DL_Mpc\n",
    "    return DL_Mpcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step, reading our data.  \n",
    "Most of the data files have been created using Topcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine  = getpass.getuser()\n",
    "# cat_path = '/home/' + machine + '/Documentos/Data/'\n",
    "cat_path = ''  # relative path to the same directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Read data from FIRST+MILLIQUAS catalogs cross-matched.~~  \n",
    "Read data from FIRST (from its version `14dec17`) crossmatched with  \n",
    "SDSS Quasar Catalog (`SDSSDR14Q_v4_4`).\n",
    "\n",
    "**FIRST + (SDSSQ)**   \n",
    "\n",
    "Redshift values have been retrieved from the column `pipeline_redshift` in SDSS DR12  \n",
    "The procedure to obtain these values is explained in **Bolton+2012**  \n",
    "\n",
    "We also select sources with have explicitely data in both SDSS Quasar Catalog and  \n",
    "in the FIRST survey (`first_match_flag = 1`).  \n",
    "\n",
    "In order to get only the best redshift values, we select sources with  \n",
    "`pipeline_redshift_flag = 0`.\n",
    "\n",
    "Thus, we can use 11238 objects from the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: UnitsWarning: 'erg/s/cm2' contains multiple slashes, which is discouraged by the FITS standard [astropy.units.format.generic]\n",
      "WARNING: UnitsWarning: 'W/m2/Hz' contains multiple slashes, which is discouraged by the FITS standard [astropy.units.format.generic]\n"
     ]
    }
   ],
   "source": [
    "# sdss_milli = Table.read(cat_path + 'tables_matches_milli_sdss_apr.fits')\n",
    "# sdss_milli = Table.read(cat_path + 'milliquas_sdssquasar_jun2020.fits')\n",
    "\n",
    "# L_14GHz_filter = np.array((sdss_milli['flux_20_cm'] > 0.0) * (sdss_milli['first_offset'] < 1.0)) ;  # sdss + milliquasar\n",
    "\n",
    "sdss_milli = Table.read(cat_path + 'first_14dec17_DR14Q_v4_4_zwarn_0.fits')  # SDSSQDR14+FIRST17Dec14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdss_milli.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load data from the VLA-COSMOS 3 GHz Large Project (**Smolčić et al. 2017**),  \n",
    "which has been correlated with the sources from the VLA-COSMOS Large Project 1.4-GHz Source Catalog (**Schinnerer et al., 2007**).\n",
    "\n",
    "Thus, we have only considered sources with observed (not estimated) $1.4$ GHz fluxes. That is, with valid  \n",
    "values in the table column `flux_20cm`.\n",
    "\n",
    "We can, also, separate sources which have an AGN X-ray counterpart\n",
    "with the flag `cosm_xray_flag`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosmos_data         = Table.read(cat_path + 'vla_xmmcosmos_smolcic.fits')\n",
    "cosmos_data         = Table.read(cat_path + 'vlacosxoid_vlacos3gh_cosmos_jun2020.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want, also, to add $z > 6$ QSOs from the list in  \n",
    "Table 3 in the review of **Inayoshi, Visbal, and Haiman, 2020**.  \n",
    "Six of them have $z > 7$\n",
    "\n",
    "Not all of them have $1.4$ GHz measurements. Others have  \n",
    "measurements in different frequencies which can be translated  \n",
    "into the desired frequency using, for instance, the relation  \n",
    "from **Butler et al., 2018**:\n",
    "\n",
    "$$S_{a} = S_{b} \\times (\\frac{\\nu_{b}}{\\nu_{a}})^{\\alpha}$$  \n",
    "\n",
    "We load the data from these sources. Fluxes from different frequencies than $1.4$ GHz are translated to the needed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_z_ra         = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[1],  dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_dec        = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[2],  dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_zs         = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[3],  dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_zs_e       = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[4],  dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_f_14GHz    = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[6],  dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_f_14GHz_e  = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[7],  dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_f_3GHz     = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[8],  dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_f_3GHz_e   = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[9],  dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_f_15GHz    = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[10], dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_f_15GHz_e  = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[11], dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_f_250GHz   = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[12], dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_f_250GHz_e = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[13], dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_mass_1450  = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[14], dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_f_6cm      = np.zeros_like(high_z_zs)  # Keep empty\n",
    "high_z_f_6cm_e    = np.zeros_like(high_z_zs)  # Keep empty\n",
    "high_z_xmm_flux   = np.zeros_like(high_z_zs)\n",
    "high_z_xmm_flux_e = np.zeros_like(high_z_zs)\n",
    "high_z_names      = np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[0], dtype='str', delimiter=';')\n",
    "high_z_lum_d      = luminosity_distance(high_z_zs) * 3.086e22  # in m\n",
    "high_z_up_lim     = np.array([val == '<' for val in np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[5], dtype='str', delimiter=';')])\n",
    "high_z_origin_flg = np.full_like(high_z_zs, fill_value=4, dtype=np.int)  # Flag to know origin of sources (0: SDSS, 1: COSMOS, 2: RADIO, 3: Stripe82, 4:Inayoshi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulate values into one array except 250GHz data.  \n",
    "Millimetre luminosities will be used separately since we cannot be completely  \n",
    "sure that they represent, fully, non-thermal emission (from AGN) and not dust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_z_14   = high_z_14GHz + high_z_3GHz + high_z_15GHz\n",
    "# high_z_14_e = high_z_14GHz_e + high_z_3GHz_e + high_z_15GHz_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complement the dataset, we also load four $z > 5.5$ sources which  \n",
    "come from the `radio` catalog in the `Heasarc` database.  \n",
    "We queried the objects which have $1.4$ GHz observations that  \n",
    "are within a $2.5$ arcsec of an object from the `SDSS QUASAR DR12`  \n",
    "catalog. We discard the sources that are already included in the `FIRST`  \n",
    "catalog, to avoid repetition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enlarge the size of the sample, we can also query  \n",
    "the same sample, but extending the redshift range to all positive  \n",
    "values. We can include, too, the exclusion of ***low quality*** redshift_values (or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# radio_sources        = Table.read(cat_path + 'radio_cat_sdss_z_55_spec.fits')       # high redshift\n",
    "# radio_sources        = Table.read(cat_path + 'radio_cat_sdss_z_all_spec.fits')      # all redshift, high z quality\n",
    "radio_sources        = Table.read(cat_path + 'radio_sdssquasar_jun2020.fits')      # all redshift, high z quality\n",
    "# radio_sources        = Table.read(cat_path + 'radio_cat_sdss_z_all_all_spec.fits')  # all redshift, all z quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also include sources from the field `Stripe82` with `VLA` observations.  \n",
    "The data has been obtained from **Hodge et al., 2011** (`VLASS821P4`; VLA SDSS Stripe 82 Survey 1.4-GHz Source Catalog).  \n",
    "These sources have been crossmatched with the `SDSSQUASAR` catalog to obtain their redshift value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripe82_sources     = Table.read(cat_path + 'vlass821p4_sdssquasar_jun2020.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate luminosities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate luminosities (in W/Hz) for different datasets  \n",
    "using the expression\n",
    "\n",
    "$$L_{1.4\\mathrm{GHz}} = 4 \\pi \\mathrm{d}^{2}_{L} f_{1.4\\mathrm{GHz}} (1 + z)^{\\alpha - 1}$$\n",
    "\n",
    "which comes from Alexander et al. 2003\n",
    "\n",
    "We can also obtain that luminosity from the flux in $3$ GHz as\n",
    "\n",
    "$$L_{1.4\\mathrm{GHz}} = 4 \\pi \\mathrm{d}^{2}_{L} {(\\frac{3}{1.4})}^{\\alpha} f_{3\\mathrm{GHz}} (1 + z)^{\\alpha - 1}$$\n",
    "\n",
    "This expression comes from Delhaize et al. 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_21cm           = lum_from_flux(sdss_milli['FINT'], sdss_milli['Z_PIPE'])\n",
    "L_21cm_e         = np.abs(L_21cm) / (sdss_milli['FINT'] / sdss_milli['RMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_21cm_radio     = lum_from_flux(radio_sources['radio_20cm_flux'], radio_sources['sdss_z'])\n",
    "L_21cm_radio_e   = np.abs(L_21cm_radio) * radio_sources['radio_20cm_flux_err'] / radio_sources['radio_20cm_flux']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_14GHz_cosmos   = lum_from_flux(cosmos_data['flux_20cm'], cosmos_data['cosm_z'])\n",
    "L_14GHz_cosmos_e = np.abs(L_14GHz_cosmos) * cosmos_data['flux_20cm_err'] / cosmos_data['flux_20cm']\n",
    "# L_14GHz_cosmos_e[~np.isfinite(L_14GHz_cosmos_e)] = 0.0  # Repair (make zero) error values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_14GHz_strp82   = lum_from_flux(stripe82_sources['strp82_20cm_flux'], stripe82_sources['sdss_z'])\n",
    "L_14GHz_strp82_e = np.abs(L_14GHz_strp82) * stripe82_sources['strp82_20cm_flux_err'] / stripe82_sources['strp82_20cm_flux'];\n",
    "L_14GHz_strp82_e[~np.isfinite(L_14GHz_strp82_e)] = 0.0  # Repair (make zero) error values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining 1.4 GHz luminosities from 1.4, 3, 1.5 and 250 GHz fluxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_z_lum_14    = lum_from_flux(high_z_14GHz  * 1e3, high_z_zs)\n",
    "# high_z_lum_3     = lum_from_flux(high_z_3GHz   * 1e3, high_z_zs) * (3/1.4)**alpha_used\n",
    "# high_z_lum_15    = lum_from_flux(high_z_15GHz  * 1e3, high_z_zs) * (1.5/1.4)**alpha_used\n",
    "# high_z_lum_250   = lum_from_flux(high_z_250GHz * 1e3, high_z_zs) * (250/1.4)**(alpha_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate their respective luminosities, without a transformation of frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_z_lum_14GHz  = lum_from_flux(high_z_f_14GHz  * 1e3, high_z_zs)  # 1.4 GHz\n",
    "high_z_lum_3GHz   = lum_from_flux(high_z_f_3GHz   * 1e3, high_z_zs)  # 3 GHz\n",
    "high_z_lum_15GHz  = lum_from_flux(high_z_f_15GHz  * 1e3, high_z_zs)  # 1.5 GHz\n",
    "high_z_lum_250GHz = lum_from_flux(high_z_f_250GHz * 1e3, high_z_zs)  # 250 GHz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mix all luminosities (different bands) to obtain single value (adding zeroes).  \n",
    "Millimetre luminosities will be used separately since we cannot be completely  \n",
    "sure that they represent, fully, non-thermal emission (from AGN) and not dust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_z_lum_14GHz   = high_z_lum_14 + high_z_lum_3 + high_z_lum_15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous step is not completely needed since we will combine values  \n",
    "in a much more future step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also determine error values for these luminosities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_z_lum_14GHz_e  = np.zeros_like(high_z_lum_14GHz)\n",
    "high_z_lum_3GHz_e   = np.zeros_like(high_z_lum_3GHz)\n",
    "high_z_lum_15GHz_e  = np.zeros_like(high_z_lum_15GHz)\n",
    "high_z_lum_250GHz_e = np.zeros_like(high_z_lum_250GHz)\n",
    "for counter, element in enumerate(high_z_lum_14GHz):\n",
    "    if element == 0: continue\n",
    "    high_z_lum_14GHz_e[counter] = np.abs(element) * high_z_f_14GHz_e[counter] / high_z_f_14GHz[counter]\n",
    "for counter, element in enumerate(high_z_lum_3GHz):\n",
    "    if element == 0: continue\n",
    "    high_z_lum_3GHz_e[counter] = np.abs(element) * high_z_f_3GHz_e[counter] / high_z_f_3GHz[counter]\n",
    "for counter, element in enumerate(high_z_lum_15GHz):\n",
    "    if element == 0: continue\n",
    "    high_z_lum_15GHz_e[counter] = np.abs(element) * high_z_f_15GHz_e[counter] / high_z_f_15GHz[counter]\n",
    "for counter, element in enumerate(high_z_lum_250GHz):\n",
    "    if element == 0: continue\n",
    "    high_z_lum_250GHz_e[counter] = np.abs(element) * high_z_f_250GHz_e[counter] / high_z_f_250GHz[counter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a filter to plot, when needed, only the sources which  \n",
    "have mm data but not radio observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_250GHz = np.array((high_z_lum_250GHz > 0) * (high_z_lum_14GHz == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the points we are interested in. Our sample from **Inayoshi et al., 2020** and the  \n",
    "sources from **SDSS+FIRST** with $z>5.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option to display the data is, instead of showing redshift in the  \n",
    "horizontal axis, have the mass of the observed objects.\n",
    "\n",
    "**Inayoshi et al., 2020** use the rest-frame UV magnitude $\\mathrm{M}_{1450}$  \n",
    "to calculate the mass as:\n",
    "\n",
    "$$M = 10^{[-(\\mathrm{M}_{1450} + 3.459) / 2.5]} [\\mathrm{M}_{\\odot}]$$\n",
    "\n",
    "which yields, on average, the published virial mass estimates for those available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create formal arrays from catalogs to merge them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays from `SDSS+FIRST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit_z_sdss          = 5.5\n",
    "limit_z_sdss          = 0.0\n",
    "filter_sdss_z         = np.array(sdss_milli['Z_PIPE'] > limit_z_sdss)\n",
    "xmm_freq              = 1.47e18  # Hz. Frequency for 0.2-12 keV observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_sdss_L          = L_21cm[filter_sdss_z]\n",
    "upper_sdss_L_e        = L_21cm_e[filter_sdss_z]\n",
    "upper_sdss_u_lim      = np.zeros_like(upper_sdss_L, dtype=np.bool)\n",
    "upper_sdss            = sdss_milli[np.array(sdss_milli['Z_PIPE'] > limit_z_sdss)]\n",
    "upper_sdss_ra         = upper_sdss['RA_1']\n",
    "upper_sdss_dec        = upper_sdss['DEC_1']\n",
    "upper_sdss_z          = upper_sdss['Z_PIPE']\n",
    "upper_sdss_z_e        = upper_sdss['Z_PIPE_ERR']\n",
    "upper_sdss_f_20cm     = upper_sdss['FINT']  # mJy\n",
    "upper_sdss_f_20cm_e   = upper_sdss['RMS']  # mJy\n",
    "upper_sdss_f_6cm      = np.zeros_like(upper_sdss_L)\n",
    "upper_sdss_f_6cm_e    = np.zeros_like(upper_sdss_L)\n",
    "upper_sdss_L_6cm      = np.zeros_like(upper_sdss_L)\n",
    "upper_sdss_L_6cm_e    = np.zeros_like(upper_sdss_L)\n",
    "upper_sdss_f_3GHz     = np.zeros_like(upper_sdss_L)\n",
    "upper_sdss_f_3GHz_e   = np.zeros_like(upper_sdss_L)\n",
    "upper_sdss_L_3GHz     = np.zeros_like(upper_sdss_L)\n",
    "upper_sdss_L_3GHz_e   = np.zeros_like(upper_sdss_L)\n",
    "upper_sdss_f_15GHz    = np.zeros_like(upper_sdss_L)  # 1.5 GHz\n",
    "upper_sdss_f_15GHz_e  = np.zeros_like(upper_sdss_L)\n",
    "upper_sdss_L_15GHz    = np.zeros_like(upper_sdss_L)\n",
    "upper_sdss_L_15GHz_e  = np.zeros_like(upper_sdss_L)\n",
    "upper_sdss_f_250GHz   = np.zeros_like(upper_sdss_L)\n",
    "upper_sdss_f_250GHz_e = np.zeros_like(upper_sdss_L)\n",
    "upper_sdss_L_250GHz   = np.zeros_like(upper_sdss_L)\n",
    "upper_sdss_L_250GHz_e = np.zeros_like(upper_sdss_L)\n",
    "upper_sdss_xmm_flux   = upper_sdss['FLUX_0.2_2.0keV'] / xmm_freq * 1e23 * 1e3     # in erg cm-2 s-1. Need to convert to mJy\n",
    "upper_sdss_xmm_flux_e = upper_sdss['FLUX_0.2_2.0keV_ERR'] / xmm_freq * 1e23 * 1e3 # in erg cm-2 s-1. Need to convert to mJy\n",
    "upper_sdss_mass_1450  = np.zeros_like(upper_sdss_L)\n",
    "upper_sdss_origin_flg = np.zeros_like(upper_sdss_L, dtype=np.int)  # Flag to know origin of sources (0: SDSS, 1: COSMOS, 2: RADIO, 3: Stripe82, 4:Inayoshi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays from the `COSMOS` Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rcarvajal/.miniconda3/lib/python3.7/site-packages/astropy/table/column.py:1020: RuntimeWarning: invalid value encountered in greater\n",
      "  result = getattr(super(), op)(other)\n"
     ]
    }
   ],
   "source": [
    "upper_cosmos_L          = L_14GHz_cosmos[np.array(cosmos_data['cosm_z'] > limit_z_sdss)]  # Calculated with our spectral index\n",
    "upper_cosmos_L_e        = L_14GHz_cosmos_e[np.array(cosmos_data['cosm_z'] > limit_z_sdss)]\n",
    "upper_cosmos_u_lim      = np.zeros_like(upper_cosmos_L, dtype=np.bool)\n",
    "upper_cosmos            = cosmos_data[np.array(cosmos_data['cosm_z'] > limit_z_sdss)]\n",
    "upper_cosmos_ra         = upper_cosmos['cosm_ra']\n",
    "upper_cosmos_dec        = upper_cosmos['cosm_dec']\n",
    "upper_cosmos_z          = upper_cosmos['cosm_z']\n",
    "upper_cosmos_z_e        = np.zeros_like(upper_cosmos_L)\n",
    "upper_cosmos_f_20cm     = upper_cosmos['flux_20cm']  # mJy\n",
    "upper_cosmos_f_20cm_e   = upper_cosmos['flux_20cm_err']  # mJy\n",
    "upper_cosmos_f_3GHz     = upper_cosmos['cosm_f_3ghz']\n",
    "upper_cosmos_f_3GHz_e   = upper_cosmos['cosm_f_3ghz_err']\n",
    "upper_cosmos_L_3GHz     = upper_cosmos['cosm_lum_3ghz']  # W/Hz\n",
    "upper_cosmos_L_3GHz_e   = np.abs(upper_cosmos_L_3GHz) * upper_cosmos_f_3GHz_e / upper_cosmos_f_3GHz\n",
    "upper_cosmos_f_6cm      = np.zeros_like(upper_cosmos_L)\n",
    "upper_cosmos_f_6cm_e    = np.zeros_like(upper_cosmos_L)\n",
    "upper_cosmos_L_6cm      = np.zeros_like(upper_cosmos_L)\n",
    "upper_cosmos_L_6cm_e    = np.zeros_like(upper_cosmos_L)\n",
    "upper_cosmos_f_15GHz    = np.zeros_like(upper_cosmos_L)  # 1.5 GHz\n",
    "upper_cosmos_f_15GHz_e  = np.zeros_like(upper_cosmos_L)\n",
    "upper_cosmos_L_15GHz    = np.zeros_like(upper_cosmos_L)\n",
    "upper_cosmos_L_15GHz_e  = np.zeros_like(upper_cosmos_L)\n",
    "upper_cosmos_f_250GHz   = np.zeros_like(upper_cosmos_L)\n",
    "upper_cosmos_f_250GHz_e = np.zeros_like(upper_cosmos_L)\n",
    "upper_cosmos_L_250GHz   = np.zeros_like(upper_cosmos_L)\n",
    "upper_cosmos_L_250GHz_e = np.zeros_like(upper_cosmos_L)\n",
    "upper_cosmos_xmm_flux   = np.zeros_like(upper_cosmos_L)          # in erg cm-2 s-1. Need to convert to mJy\n",
    "upper_cosmos_xmm_flux_e = np.zeros_like(upper_cosmos_L)          # in erg cm-2 s-1. Need to convert to mJy\n",
    "upper_cosmos_mass_1450  = np.zeros_like(upper_cosmos_L)\n",
    "upper_cosmos_origin_flg = np.ones_like(upper_cosmos_L, dtype=np.int)  # Flag to know origin of sources (0: SDSS, 1: COSMOS, 2: RADIO, 3: Stripe82, 4:Inayoshi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays from `radio`catalog (`Heasarc`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_sdss_ra         = radio_sources['sdss_ra']\n",
    "radio_sdss_dec        = radio_sources['sdss_dec']\n",
    "radio_sdss_u_lim      = np.zeros_like(L_21cm_radio, dtype=np.bool)\n",
    "radio_sdss_z          = radio_sources['sdss_z']\n",
    "radio_sdss_z_e        = radio_sources['sdss_z_err']\n",
    "radio_sdss_f_20cm     = radio_sources['radio_20cm_flux']\n",
    "radio_sdss_f_20cm_e   = radio_sources['radio_20cm_flux_err']\n",
    "radio_sdss_f_6cm      = radio_sources['radio_6cm_flux']        # mJy\n",
    "radio_sdss_f_6cm_e    = radio_sources['radio_6cm_flux_err']        # mJy\n",
    "radio_sdss_L_6cm      = lum_from_flux(radio_sdss_f_6cm, radio_sdss_z)  # W/Hz\n",
    "radio_sdss_L_6cm_e    = np.abs(radio_sdss_L_6cm) * radio_sdss_f_6cm_e / radio_sdss_f_6cm\n",
    "radio_sdss_f_3GHz     = np.zeros_like(L_21cm_radio)\n",
    "radio_sdss_f_3GHz_e   = np.zeros_like(L_21cm_radio)\n",
    "radio_sdss_L_3GHz     = np.zeros_like(L_21cm_radio)\n",
    "radio_sdss_L_3GHz_e   = np.zeros_like(L_21cm_radio)\n",
    "radio_sdss_f_15GHz    = np.zeros_like(L_21cm_radio)\n",
    "radio_sdss_f_15GHz_e  = np.zeros_like(L_21cm_radio)\n",
    "radio_sdss_L_15GHz    = np.zeros_like(L_21cm_radio)\n",
    "radio_sdss_L_15GHz_e  = np.zeros_like(L_21cm_radio)\n",
    "radio_sdss_f_250GHz   = np.zeros_like(L_21cm_radio)\n",
    "radio_sdss_f_250GHz_e = np.zeros_like(L_21cm_radio)\n",
    "radio_sdss_L_250GHz   = np.zeros_like(L_21cm_radio)\n",
    "radio_sdss_L_250GHz_e = np.zeros_like(L_21cm_radio)\n",
    "radio_sdss_xmm_flux   = radio_sources['xmm_flux'] / xmm_freq * 1e23 * 1e3      # in erg cm-2 s-1. Need to convert to mJy\n",
    "radio_sdss_xmm_flux_e = radio_sources['xmm_flux_err'] / xmm_freq * 1e23 * 1e3  # in erg cm-2 s-1. Need to convert to mJy\n",
    "radio_sdss_mass_1450  = np.zeros_like(L_21cm_radio)\n",
    "radio_sdss_origin_flg = np.full_like(L_21cm_radio, fill_value=2, dtype=np.int)  # Flag to know origin of sources (0: SDSS, 1: COSMOS, 2: RADIO, 3: Stripe82, 4:Inayoshi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays from `vlass821p4` catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "strp82_ra            = stripe82_sources['sdss_ra']               # in degree\n",
    "strp82_dec           = stripe82_sources['sdss_dec']              # in degree\n",
    "strp82_u_lim         = np.zeros_like(strp82_ra, dtype=np.bool)\n",
    "strp82_z             = stripe82_sources['sdss_z']\n",
    "strp82_z_e           = stripe82_sources['sdss_z_err']\n",
    "strp82_20cm_flux     = stripe82_sources['strp82_20cm_flux']      # in mJy\n",
    "strp82_20cm_flux_e   = stripe82_sources['strp82_20cm_flux_err']  # in mJy\n",
    "strp82_f_3GHz        = np.zeros_like(strp82_z)\n",
    "strp82_f_3GHz_e      = np.zeros_like(strp82_z)\n",
    "strp82_L_3GHz        = np.zeros_like(strp82_z)  # W/Hz\n",
    "strp82_L_3GHz_e      = np.zeros_like(strp82_z)\n",
    "strp82_f_6cm         = np.zeros_like(strp82_z)\n",
    "strp82_f_6cm_e       = np.zeros_like(strp82_z)\n",
    "strp82_L_6cm         = np.zeros_like(strp82_z)\n",
    "strp82_L_6cm_e       = np.zeros_like(strp82_z)\n",
    "strp82_f_15GHz       = np.zeros_like(strp82_z)  # 1.5 GHz\n",
    "strp82_f_15GHz_e     = np.zeros_like(strp82_z)\n",
    "strp82_L_15GHz       = np.zeros_like(strp82_z)\n",
    "strp82_L_15GHz_e     = np.zeros_like(strp82_z)\n",
    "strp82_f_250GHz      = np.zeros_like(strp82_z)\n",
    "strp82_f_250GHz_e    = np.zeros_like(strp82_z)\n",
    "strp82_L_250GHz      = np.zeros_like(strp82_z)\n",
    "strp82_L_250GHz_e    = np.zeros_like(strp82_z)\n",
    "strp82_xmm_flux      = stripe82_sources['xmm_flux'] / xmm_freq * 1e23 * 1e3      # in erg cm-2 s-1. Need to convert to mJy\n",
    "strp82_xmm_flux_e    = stripe82_sources['xmm_flux_err'] / xmm_freq * 1e23 * 1e3  # in erg cm-2 s-1. Need to convert to mJy\n",
    "strp82_spec_index    = stripe82_sources['spec_index']\n",
    "strp82_mass_1450     = np.zeros_like(strp82_z)\n",
    "strp82_origin_flg    = np.full_like(strp82_z, fill_value=3, dtype=np.int)  # Flag to know origin of sources (0: SDSS, 1: COSMOS, 2: RADIO, 3: Stripe82, 4:Inayoshi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging `SDSS+FIRST` and `radio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_large_sample_ra         = np.append(upper_sdss_ra,         radio_sdss_ra)           # deg\n",
    "radio_large_sample_dec        = np.append(upper_sdss_dec,        radio_sdss_dec)          # deg\n",
    "radio_large_sample_L          = np.append(upper_sdss_L,          L_21cm_radio)            # W/Hz\n",
    "radio_large_sample_L_e        = np.append(upper_sdss_L_e,        L_21cm_radio_e)          # W/Hz\n",
    "radio_large_sample_u_lim      = np.append(upper_sdss_u_lim,      radio_sdss_u_lim)\n",
    "# radio_large_sample_L_250GHz   = np.append(upper_sdss_L_250GHz,   radio_sdss_L_250GHz)     # W/Hz\n",
    "# radio_large_sample_L_250GHz_e = np.append(upper_sdss_L_250GHz_e, radio_sdss_L_250GHz_e)   # W/Hz\n",
    "radio_large_sample_f_20cm     = np.append(upper_sdss_f_20cm,     radio_sdss_f_20cm)       # mJy\n",
    "radio_large_sample_f_20cm_e   = np.append(upper_sdss_f_20cm_e,   radio_sdss_f_20cm_e)     # mJy\n",
    "\n",
    "radio_large_sample_f_6cm      = np.append(upper_sdss_f_6cm,      radio_sdss_f_6cm)        # mJy\n",
    "radio_large_sample_f_6cm_e    = np.append(upper_sdss_f_6cm_e,    radio_sdss_f_6cm_e)      # mJy\n",
    "radio_large_sample_f_3GHz     = np.append(upper_sdss_f_3GHz,     radio_sdss_f_3GHz)       # mJy\n",
    "radio_large_sample_f_3GHz_e   = np.append(upper_sdss_f_3GHz_e,   radio_sdss_f_3GHz_e)     # mJy\n",
    "radio_large_sample_f_15GHz    = np.append(upper_sdss_f_15GHz,    radio_sdss_f_15GHz)      # mJy\n",
    "radio_large_sample_f_15GHz_e  = np.append(upper_sdss_f_15GHz_e,  radio_sdss_f_15GHz_e)    # mJy\n",
    "\n",
    "radio_large_sample_f_250GHz   = np.append(upper_sdss_f_250GHz,   radio_sdss_f_250GHz)     # mJy\n",
    "radio_large_sample_f_250GHz_e = np.append(upper_sdss_f_250GHz_e, radio_sdss_f_250GHz_e)   # mJy\n",
    "radio_large_sample_z          = np.append(upper_sdss_z,          radio_sdss_z)\n",
    "radio_large_sample_z_e        = np.append(upper_sdss_z_e,        radio_sdss_z_e)\n",
    "radio_large_sample_xmm_flux   = np.append(upper_sdss_xmm_flux,   radio_sdss_xmm_flux)     # in erg cm-2 s-1. Need to convert to mJy\n",
    "radio_large_sample_xmm_flux_e = np.append(upper_sdss_xmm_flux_e, radio_sdss_xmm_flux_e)   # in erg cm-2 s-1. Need to convert to mJy\n",
    "radio_large_sample_mass_1450  = np.append(upper_sdss_mass_1450,  radio_sdss_mass_1450)    # M_sun\n",
    "radio_large_sample_origin_flg = np.append(upper_sdss_origin_flg, radio_sdss_origin_flg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging `SDSS+FIRST`+`radio` and the catalog from **Inayoshi et al., 2020**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_sample_ra         = np.append(radio_large_sample_ra,         high_z_ra[np.array(high_z_lum_14GHz>0)])               # deg\n",
    "medium_sample_dec        = np.append(radio_large_sample_dec,        high_z_dec[np.array(high_z_lum_14GHz>0)])              # deg\n",
    "medium_sample_L          = np.append(radio_large_sample_L,          high_z_lum_14GHz[np.array(high_z_lum_14GHz>0)])        # W/Hz\n",
    "medium_sample_L_e        = np.append(radio_large_sample_L_e,        high_z_lum_14GHz_e[np.array(high_z_lum_14GHz>0)])      # W/Hz\n",
    "medium_sample_u_lim      = np.append(radio_large_sample_u_lim,      high_z_up_lim[np.array(high_z_lum_14GHz>0)])\n",
    "# medium_sample_L_250      = np.append(radio_large_sample_L_250GHz,   high_z_lum_250[np.array(high_z_lum_14GHz>0)])          # W/Hz\n",
    "# medium_sample_L_250_e    = np.append(radio_large_sample_L_250GHz_e, high_z_lum_250GHz_e[np.array(high_z_lum_14GHz>0)])     # W/Hz\n",
    "medium_sample_f20cm      = np.append(radio_large_sample_f_20cm,     high_z_f_14GHz[np.array(high_z_lum_14GHz>0)] * 1e-3)        # mJy\n",
    "medium_sample_f20cm_e    = np.append(radio_large_sample_f_20cm_e,   high_z_f_14GHz_e[np.array(high_z_lum_14GHz>0)] * 1e-3)      # mJy\n",
    "\n",
    "medium_sample_f_6cm      = np.append(radio_large_sample_f_6cm,      high_z_f_6cm[np.array(high_z_lum_14GHz>0)])                                          # mJy\n",
    "medium_sample_f_6cm_e    = np.append(radio_large_sample_f_6cm_e,    high_z_f_6cm_e[np.array(high_z_lum_14GHz>0)])                                        # mJy\n",
    "medium_sample_f_3GHz     = np.append(radio_large_sample_f_3GHz,     high_z_f_3GHz[np.array(high_z_lum_14GHz>0)] * 1e-3)    # mJy\n",
    "medium_sample_f_3GHz_e   = np.append(radio_large_sample_f_3GHz_e,   high_z_f_3GHz_e[np.array(high_z_lum_14GHz>0)] * 1e-3)  # mJy\n",
    "medium_sample_f_15GHz    = np.append(radio_large_sample_f_15GHz,    high_z_f_15GHz[np.array(high_z_lum_14GHz>0)] * 1e-3)   # mJy\n",
    "medium_sample_f_15GHz_e  = np.append(radio_large_sample_f_15GHz_e,  high_z_f_15GHz_e[np.array(high_z_lum_14GHz>0)] * 1e-3) # mJy\n",
    "\n",
    "medium_sample_f250GHz    = np.append(radio_large_sample_f_250GHz,   high_z_f_250GHz[np.array(high_z_lum_14GHz>0)] * 1e-3)    # mJy\n",
    "medium_sample_f250GHz_e  = np.append(radio_large_sample_f_250GHz_e, high_z_f_250GHz_e[np.array(high_z_lum_14GHz>0)] * 1e-3)  # mJy\n",
    "medium_sample_z          = np.append(radio_large_sample_z,          high_z_zs[np.array(high_z_lum_14GHz>0)])\n",
    "medium_sample_z_e        = np.append(radio_large_sample_z_e,        high_z_zs_e[np.array(high_z_lum_14GHz>0)])\n",
    "medium_sample_xmm_flux   = np.append(radio_large_sample_xmm_flux,   high_z_xmm_flux[np.array(high_z_lum_14GHz>0)])         # in erg cm-2 s-1. Need to convert to mJy\n",
    "medium_sample_xmm_flux_e = np.append(radio_large_sample_xmm_flux_e, high_z_xmm_flux_e[np.array(high_z_lum_14GHz>0)])       # in erg cm-2 s-1. Need to convert to mJy\n",
    "medium_sample_mass_1450  = np.append(radio_large_sample_mass_1450,  high_z_mass_1450[np.array(high_z_lum_14GHz>0)])        # M_sun\n",
    "medium_sample_origin_flg = np.append(radio_large_sample_origin_flg, high_z_origin_flg[np.array(high_z_lum_14GHz>0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging `SDSS+FIRST`+`radio`+**Inayoshi et al., 2020** and `VLASS821P4` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium82_sample_ra         = np.append(medium_sample_ra,         strp82_ra)          # deg\n",
    "medium82_sample_dec        = np.append(medium_sample_dec,        strp82_dec)         # deg\n",
    "medium82_sample_L          = np.append(medium_sample_L,          L_14GHz_strp82)           # W/Hz\n",
    "medium82_sample_L_e        = np.append(medium_sample_L_e,        L_14GHz_strp82_e)         # W/Hz\n",
    "medium82_sample_u_lim      = np.append(medium_sample_u_lim,      strp82_u_lim)\n",
    "# medium82_sample_L_250      = np.append(medium_sample_L_250,      strp82_L_250GHz)    # W/Hz\n",
    "# medium82_sample_L_250_e    = np.append(medium_sample_L_250_e,    strp82_L_250GHz_e)  # W/Hz\n",
    "medium82_sample_f20cm      = np.append(medium_sample_f20cm,      strp82_20cm_flux)      # mJy\n",
    "medium82_sample_f20cm_e    = np.append(medium_sample_f20cm_e,    strp82_20cm_flux_e)    # mJy\n",
    "\n",
    "medium82_sample_f_6cm      = np.append(medium_sample_f_6cm,      strp82_f_6cm)       # mJy\n",
    "medium82_sample_f_6cm_e    = np.append(medium_sample_f_6cm_e,    strp82_f_6cm_e)     # mJy\n",
    "medium82_sample_f_3GHz     = np.append(medium_sample_f_3GHz,     strp82_f_3GHz)      # mJy\n",
    "medium82_sample_f_3GHz_e   = np.append(medium_sample_f_3GHz_e,   strp82_f_3GHz_e)    # mJy\n",
    "medium82_sample_f_15GHz    = np.append(medium_sample_f_15GHz,    strp82_f_15GHz)     # mJy\n",
    "medium82_sample_f_15GHz_e  = np.append(medium_sample_f_15GHz_e,  strp82_f_15GHz_e)   # mJy\n",
    "\n",
    "medium82_sample_f250GHz    = np.append(medium_sample_f250GHz,    strp82_f_250GHz)    # mJy\n",
    "medium82_sample_f250GHz_e  = np.append(medium_sample_f250GHz_e,  strp82_f_250GHz_e)  # mJy\n",
    "medium82_sample_z          = np.append(medium_sample_z,          strp82_z)\n",
    "medium82_sample_z_e        = np.append(medium_sample_z_e,        strp82_z_e)\n",
    "medium82_sample_xmm_flux   = np.append(medium_sample_xmm_flux,   strp82_xmm_flux)    # in erg cm-2 s-1. Need to convert to mJy\n",
    "medium82_sample_xmm_flux_e = np.append(medium_sample_xmm_flux_e, strp82_xmm_flux_e)  # in erg cm-2 s-1. Need to convert to mJy\n",
    "medium82_sample_mass_1450  = np.append(medium_sample_mass_1450,  strp82_mass_1450)   # M_sun\n",
    "medium82_sample_origin_flg = np.append(medium_sample_origin_flg, strp82_origin_flg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging `SDSS+FIRST`+`radio`+**Inayoshi et al., 2020**+`VLASS821P4` and `COSMOS` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_sample_ra         = np.append(medium82_sample_ra,         upper_cosmos_ra)          # deg\n",
    "large_sample_dec        = np.append(medium82_sample_dec,        upper_cosmos_dec)         # deg\n",
    "large_sample_L          = np.append(medium82_sample_L,          upper_cosmos_L)           # W/Hz\n",
    "large_sample_L_e        = np.append(medium82_sample_L_e,        upper_cosmos_L_e)         # W/Hz\n",
    "large_sample_u_lim      = np.append(medium82_sample_u_lim,      upper_cosmos_u_lim)\n",
    "# large_sample_L_250      = np.append(medium82_sample_L_250,      upper_cosmos_L_250GHz)    # W/Hz\n",
    "# large_sample_L_250_e    = np.append(medium82_sample_L_250_e,    upper_cosmos_L_250GHz_e)  # W/Hz\n",
    "large_sample_f20cm      = np.append(medium82_sample_f20cm,      upper_cosmos_f_20cm)      # mJy\n",
    "large_sample_f20cm_e    = np.append(medium82_sample_f20cm_e,    upper_cosmos_f_20cm_e)    # mJy\n",
    "\n",
    "large_sample_f_6cm      = np.append(medium82_sample_f_6cm,      upper_cosmos_f_6cm)       # mJy\n",
    "large_sample_f_6cm_e    = np.append(medium82_sample_f_6cm_e,    upper_cosmos_f_6cm_e)     # mJy\n",
    "large_sample_f_3GHz     = np.append(medium82_sample_f_3GHz,     upper_cosmos_f_3GHz)      # mJy\n",
    "large_sample_f_3GHz_e   = np.append(medium82_sample_f_3GHz_e,   upper_cosmos_f_3GHz_e)    # mJy\n",
    "large_sample_f_15GHz    = np.append(medium82_sample_f_15GHz,    upper_cosmos_f_15GHz)     # mJy\n",
    "large_sample_f_15GHz_e  = np.append(medium82_sample_f_15GHz_e,  upper_cosmos_f_15GHz_e)   # mJy\n",
    "\n",
    "large_sample_f250GHz    = np.append(medium82_sample_f250GHz,    upper_cosmos_f_250GHz)    # mJy\n",
    "large_sample_f250GHz_e  = np.append(medium82_sample_f250GHz_e,  upper_cosmos_f_250GHz_e)  # mJy\n",
    "large_sample_z          = np.append(medium82_sample_z,          upper_cosmos_z)\n",
    "large_sample_z_e        = np.append(medium82_sample_z_e,        upper_cosmos_z_e)\n",
    "large_sample_xmm_flux   = np.append(medium82_sample_xmm_flux,   upper_cosmos_xmm_flux)    # in erg cm-2 s-1. Need to convert to mJy\n",
    "large_sample_xmm_flux_e = np.append(medium82_sample_xmm_flux_e, upper_cosmos_xmm_flux_e)  # in erg cm-2 s-1. Need to convert to mJy\n",
    "large_sample_mass_1450  = np.append(medium82_sample_mass_1450,  upper_cosmos_mass_1450)   # M_sun\n",
    "large_sample_origin_flg = np.append(medium82_sample_origin_flg, upper_cosmos_origin_flg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we also want to obtain more properties from the selected  \n",
    "sources (**Inayoshi et al., 2020** + **SDSS+FIRST**). We will use `astroquery` to  \n",
    "obtain information from `simbad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14235,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(large_sample_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we obtain the names and coordinates of our sources to query them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_radio_sdss    = np.append(radio_sources['sdss_name'], upper_sdss['SDSS_NAME'])\n",
    "medium_sample_names = np.append(names_radio_sdss, upper_cosmos['cosm_name'])\n",
    "large_sample_names  = np.append(medium_sample_names, high_z_names[np.array(high_z_lum_14GHz>0)])\n",
    "large_sample_coords = SkyCoord(ra=large_sample_ra, dec=large_sample_dec, unit=(u.deg, u.deg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11635,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(names_radio_sdss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can query the database to obtain the desired data.  In this point,  \n",
    "we also add more columns to be queried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_simbad_flag               = False\n",
    "load_simbad_flag                = False\n",
    "create_simbad_inayoshi_flag     = False\n",
    "read_simbad_inayoshi_flag       = False\n",
    "query_ned_names_flag            = False\n",
    "query_ned_photometry_flag       = False\n",
    "create_phot_bands_list_flag     = False\n",
    "load_phot_bands_list_flag       = False\n",
    "order_ned_photometry_flag       = False\n",
    "create_simbad_inayoshi_ned_flag = False\n",
    "read_simbad_inayoshi_ned_flag   = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    customSimbad   = Simbad()\n",
    "    initial_fields = customSimbad.get_votable_fields()\n",
    "\n",
    "    if 'coordinates' in initial_fields:\n",
    "        customSimbad.remove_votable_fields('coordinates')\n",
    "        customSimbad.add_votable_fields('ra(d)', 'dec(d)')\n",
    "    if 'z_value' not in initial_fields:\n",
    "        customSimbad.add_votable_fields('z_value')\n",
    "    for band in ['B','V','R','I','J','K']:\n",
    "        if f'fluxdata({band})' not in initial_fields:\n",
    "            customSimbad.add_votable_fields(f'flux({band})', f'flux_error({band})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All sources but those from `COSMOS` catalog have meaningful (for `simbad`) names.  \n",
    "Thus, separate queries will be executed. And, to standardize results, queries  \n",
    "will only be based on coordinates (not names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    limit_set            = int(np.floor(np.shape(medium_sample_L)[0]/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    result_table_simbad  = Table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_table = customSimbad.query_objects(large_sample_names)\n",
    "# customSimbad.TIMEOUT = 240\n",
    "\n",
    "# result_table_job_a   = customSimbad.query_objects(large_sample_names[:limit_set])\n",
    "\n",
    "# result_table_job_b   = customSimbad.query_objects(large_sample_names[limit_set:(limit_set*2)])\n",
    "\n",
    "# result_table_job_c   = customSimbad.query_objects(large_sample_names[(limit_set*2):(limit_set*3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    query_error = 0\n",
    "    final_error = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query `SDSS+FIRST` (`milliquasar`) + `RADIO` + **Inayoshi et al., 2020** sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60% (2340 of 3884) |############        | Elapsed Time: 0:07:12 ETA:   0:06:00/Users/rcarvajal/.miniconda3/lib/python3.7/site-packages/astroquery/simbad/core.py:138: UserWarning: Warning: The script line number 3 raised an error (recorded in the `errors` attribute of the result table): '11:43:11.026 +46:57:22.6453': No astronomical object found :\n",
      "  (error.line, error.msg))\n",
      " 72% (2806 of 3884) |##############      | Elapsed Time: 0:09:10 ETA:   0:04:30/Users/rcarvajal/.miniconda3/lib/python3.7/site-packages/astroquery/simbad/core.py:138: UserWarning: Warning: The script line number 3 raised an error (recorded in the `errors` attribute of the result table): '14:20:53.148 +44:24:16.3264': No astronomical object found :\n",
      "  (error.line, error.msg))\n",
      "100% (3884 of 3884) |####################| Elapsed Time: 0:14:20 Time:  0:14:20\n"
     ]
    }
   ],
   "source": [
    "if query_simbad_flag:\n",
    "    with progressbar.ProgressBar(min_value=0, max_value=(limit_set - 1)) as bar:\n",
    "        for index, coord in enumerate(large_sample_coords[:limit_set]):\n",
    "            temp_table      = Table()\n",
    "            try:\n",
    "                temp_table      = customSimbad.query_region(coord, radius=2.5*u.arcsec)\n",
    "                if len(temp_table) == 0:\n",
    "                    temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                    temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "            except:\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                #print(f'Error with element {(index)} of the sample')\n",
    "                query_error += 1\n",
    "            try:\n",
    "                if temp_table['RA_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['RA_d'].data, dtype=result_table_simbad['RA_d'].dtype, unit=result_table_simbad['RA_d'].unit, format=result_table_simbad['RA_d'].format, description=result_table_simbad['RA_d'].description)\n",
    "                    temp_table['RA_d']  = replacement_col\n",
    "                if temp_table['DEC_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['DEC_d'].data, dtype=result_table_simbad['DEC_d'].dtype, unit=result_table_simbad['DEC_d'].unit, format=result_table_simbad['DEC_d'].format, description=result_table_simbad['DEC_d'].description)\n",
    "                    temp_table['DEC_d']  = replacement_col\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "            except:\n",
    "                print(f'Error with element {(index)} of the sample')\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "                final_error += 1\n",
    "            bar.update(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (7769 of 7769) |####################| Elapsed Time: 0:28:02 Time:  0:28:02\n"
     ]
    }
   ],
   "source": [
    "if query_simbad_flag:\n",
    "    with progressbar.ProgressBar(min_value=limit_set, max_value=(limit_set*2 - 1)) as bar:\n",
    "        for index, coord in enumerate(large_sample_coords[limit_set:(limit_set*2)]):\n",
    "            temp_table      = Table()\n",
    "            try:\n",
    "                temp_table      = customSimbad.query_region(coord, radius=2.5*u.arcsec)\n",
    "                if len(temp_table) == 0:\n",
    "                    temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                    temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "            except:\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                #print(f'Error with element {(index + limit_set)} of the sample')\n",
    "                query_error += 1\n",
    "            try:\n",
    "                if temp_table['RA_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['RA_d'].data, dtype=result_table_simbad['RA_d'].dtype, unit=result_table_simbad['RA_d'].unit, format=result_table_simbad['RA_d'].format, description=result_table_simbad['RA_d'].description)\n",
    "                    temp_table['RA_d']  = replacement_col\n",
    "                if temp_table['DEC_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['DEC_d'].data, dtype=result_table_simbad['DEC_d'].dtype, unit=result_table_simbad['DEC_d'].unit, format=result_table_simbad['DEC_d'].format, description=result_table_simbad['DEC_d'].description)\n",
    "                    temp_table['DEC_d']  = replacement_col\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "            except:\n",
    "                print(f'Error with element {(index + limit_set)} of the sample')\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "                final_error += 1\n",
    "            bar.update(index + limit_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26% (8811 of 11655) |##############     | Elapsed Time: 0:09:53 ETA:   0:27:15"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with element 8812 of the sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70% (10506 of 11655) |################  | Elapsed Time: 0:28:54 ETA:   0:15:23"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with element 10507 of the sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79% (10860 of 11655) |################  | Elapsed Time: 0:33:13 ETA:   0:09:12"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with element 10861 of the sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (11655 of 11655) |##################| Elapsed Time: 0:42:17 Time:  0:42:17\n"
     ]
    }
   ],
   "source": [
    "if query_simbad_flag:\n",
    "    with progressbar.ProgressBar(min_value=(limit_set*2), max_value=(limit_set*3)) as bar:\n",
    "        for index, coord in enumerate(large_sample_coords[(limit_set*2):(limit_set*3 + 1)]):\n",
    "            temp_table      = Table()\n",
    "            try:\n",
    "                temp_table      = customSimbad.query_region(coord, radius=2.5*u.arcsec)\n",
    "                if len(temp_table) == 0:\n",
    "                    temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                    temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "            except:\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                # print(f'Error with element {(index + limit_set*2)} of the sample')\n",
    "                query_error += 1\n",
    "            try:\n",
    "                if temp_table['RA_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['RA_d'].data, dtype=result_table_simbad['RA_d'].dtype, unit=result_table_simbad['RA_d'].unit, format=result_table_simbad['RA_d'].format, description=result_table_simbad['RA_d'].description)\n",
    "                    temp_table['RA_d']  = replacement_col\n",
    "                if temp_table['DEC_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['DEC_d'].data, dtype=result_table_simbad['DEC_d'].dtype, unit=result_table_simbad['DEC_d'].unit, format=result_table_simbad['DEC_d'].format, description=result_table_simbad['DEC_d'].description)\n",
    "                    temp_table['DEC_d']  = replacement_col\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "            except:\n",
    "                print(f'Error with element {(index + limit_set*2)} of the sample')\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "                final_error += 1\n",
    "            bar.update(index + limit_set*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query for `VLASS821P4` and `COSMOS` sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    limit_set_cosmos = int(np.floor((np.shape(large_sample_L)[0] - np.shape(medium_sample_L)[0])/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (12515 of 12515) |##################| Elapsed Time: 0:10:22 Time:  0:10:22\n"
     ]
    }
   ],
   "source": [
    "if query_simbad_flag:\n",
    "    with progressbar.ProgressBar(min_value=(limit_set*3 + 1), max_value=(limit_set*3 + limit_set_cosmos)) as bar:\n",
    "        for index, coord in enumerate(large_sample_coords[(limit_set*3 + 1):(limit_set*3 + 1 + limit_set_cosmos)]):\n",
    "            temp_table      = Table()\n",
    "            try:\n",
    "                temp_table      = customSimbad.query_region(coord, radius=2.5*u.arcsec)\n",
    "                if len(temp_table) == 0:\n",
    "                    temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                    temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "            except:\n",
    "                # print(f'Error with element {(index + limit_set*3 + 1)} of the sample')\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                query_error += 1\n",
    "            try:\n",
    "                if temp_table['RA_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['RA_d'].data, dtype=result_table_simbad['RA_d'].dtype, unit=result_table_simbad['RA_d'].unit, format=result_table_simbad['RA_d'].format, description=result_table_simbad['RA_d'].description)\n",
    "                    temp_table['RA_d']  = replacement_col\n",
    "                if temp_table['DEC_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['DEC_d'].data, dtype=result_table_simbad['DEC_d'].dtype, unit=result_table_simbad['DEC_d'].unit, format=result_table_simbad['DEC_d'].format, description=result_table_simbad['DEC_d'].description)\n",
    "                    temp_table['DEC_d']  = replacement_col\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "            except:\n",
    "                print(f'Error with element {(index + limit_set*3 + 1)} of the sample')\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "                final_error += 1\n",
    "            bar.update(index + limit_set*3 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (13375 of 13375) |##################| Elapsed Time: 0:12:00 Time:  0:12:00\n"
     ]
    }
   ],
   "source": [
    "if query_simbad_flag:\n",
    "    with progressbar.ProgressBar(min_value=(limit_set*3 + 1 + limit_set_cosmos), max_value=(limit_set*3 + limit_set_cosmos*2)) as bar:\n",
    "        for index, coord in enumerate(large_sample_coords[(limit_set*3 + 1 + limit_set_cosmos):(limit_set*3 + 1 + limit_set_cosmos*2)]):\n",
    "            temp_table      = Table()\n",
    "            try:\n",
    "                temp_table      = customSimbad.query_region(coord, radius=2.5*u.arcsec)\n",
    "                if len(temp_table) == 0:\n",
    "                    temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                    temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "            except:\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                # print(f'Error with element {(index + limit_set*3 + 1 + limit_set_cosmos)} of the sample')\n",
    "                query_error += 1\n",
    "            try:\n",
    "                if temp_table['RA_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['RA_d'].data, dtype=result_table_simbad['RA_d'].dtype, unit=result_table_simbad['RA_d'].unit, format=result_table_simbad['RA_d'].format, description=result_table_simbad['RA_d'].description)\n",
    "                    temp_table['RA_d']  = replacement_col\n",
    "                if temp_table['DEC_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['DEC_d'].data, dtype=result_table_simbad['DEC_d'].dtype, unit=result_table_simbad['DEC_d'].unit, format=result_table_simbad['DEC_d'].format, description=result_table_simbad['DEC_d'].description)\n",
    "                    temp_table['DEC_d']  = replacement_col\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "            except:\n",
    "                print(f'Error with element {(index + limit_set*3 + 1 + limit_set_cosmos)} of the sample')\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "                final_error += 1\n",
    "            bar.update(index + limit_set*3 + 1 + limit_set_cosmos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14235 of 14235) |##################| Elapsed Time: 0:11:42 Time:  0:11:42\n"
     ]
    }
   ],
   "source": [
    "if query_simbad_flag:\n",
    "    with progressbar.ProgressBar(min_value=(limit_set*3 + 1 + limit_set_cosmos*2), max_value=np.shape(large_sample_coords)[0]) as bar:\n",
    "        for index, coord in enumerate(large_sample_coords[(limit_set*3 + 1 + limit_set_cosmos*2):]):\n",
    "            temp_table      = Table()\n",
    "            try:\n",
    "                temp_table      = customSimbad.query_region(coord, radius=2.5*u.arcsec)\n",
    "                if len(temp_table) == 0:\n",
    "                    temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                    temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "            except:\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                # print(f'Error with element {(index + limit_set*3 + 1 + limit_set_cosmos*2)} of the sample')\n",
    "                query_error += 1\n",
    "            try:\n",
    "                if temp_table['RA_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['RA_d'].data, dtype=result_table_simbad['RA_d'].dtype, unit=result_table_simbad['RA_d'].unit, format=result_table_simbad['RA_d'].format, description=result_table_simbad['RA_d'].description)\n",
    "                    temp_table['RA_d']  = replacement_col\n",
    "                if temp_table['DEC_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['DEC_d'].data, dtype=result_table_simbad['DEC_d'].dtype, unit=result_table_simbad['DEC_d'].unit, format=result_table_simbad['DEC_d'].format, description=result_table_simbad['DEC_d'].description)\n",
    "                    temp_table['DEC_d']  = replacement_col\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "            except:\n",
    "                print(f'Error with element {(index + limit_set*3 + 1 + limit_set_cosmos*2)} of the sample')\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "                final_error += 1\n",
    "            bar.update(index + limit_set*3 + 1 + limit_set_cosmos*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query remaining sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the query to a file for future executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    result_table_simbad.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    result_table_simbad.write(cat_path + 'large_cat_simbad_query_jun2020.csv', format='ascii.csv', overwrite=True, serialize_method='data_mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to create a copy of table to save it as `fits` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    copy_simbad_to_write = result_table_simbad\n",
    "    str_id = copy_simbad_to_write['MAIN_ID'].astype('str')\n",
    "    copy_simbad_to_write.replace_column('MAIN_ID', str_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    copy_simbad_to_write.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: VerifyWarning: Keyword name 'description' is greater than 8 characters or contains characters not allowed by the FITS standard; a HIERARCH card will be created. [astropy.io.fits.card]\n"
     ]
    }
   ],
   "source": [
    "if query_simbad_flag:\n",
    "    copy_simbad_to_write.write(cat_path + 'large_cat_simbad_query_jun2020.fits', format='fits', overwrite=True, serialize_method='data_mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save running time, we can load the data from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if load_simbad_flag:\n",
    "#    result_table_simbad = Table.read(cat_path + 'large_cat_simbad_query.csv', format='ascii.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_simbad_flag:\n",
    "    result_table_simbad     = Table.read(cat_path + 'large_cat_simbad_query_jun2020.fits');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_table_simbad.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point, we merge the data from the query to `simbad` with the  \n",
    "values from this notebook (**Inayoshi et al., 2020** and **SDSS+FIRST**).  \n",
    "In order to do this, we convert the data into `astropy` columns, and then  \n",
    "into `astropy` tables. They will be ready to be exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_flag:\n",
    "    coords_simbad_inayoshi = SkyCoord(result_table_simbad['RA_d'], result_table_simbad['DEC_d'], unit=u.deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_flag:\n",
    "    column_cat_index    = MaskedColumn(np.arange(np.shape(large_sample_z)[0]), name='INDEX', dtype='int',\\\n",
    "                                       description='Idx_n')\n",
    "    # column_cat_name     = MaskedColumn(large_sample_names, name='CAT_NAME', dtype='str', description='Name in this catalog',\\\n",
    "    #                                    mask=np.array(large_sample_names == ''))\n",
    "    column_cat_coords   = MaskedColumn(coords_simbad_inayoshi.to_string('decimal', precision=8), name='COORD',\\\n",
    "                                       dtype='str', description='Coords',\\\n",
    "                                       mask=np.array(coords_simbad_inayoshi.to_string('decimal', precision=8) == ''))\n",
    "    column_z_own        = MaskedColumn(large_sample_z, name='Z_OWN', unit='', description='z',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_z == 0))\n",
    "    column_z_own_err    = MaskedColumn(large_sample_z_e, name='Z_OWN_ERR', unit='',\\\n",
    "                                       description='z err', fill_value=np.nan,\\\n",
    "                                       mask=np.array(large_sample_z_e == 0))\n",
    "    # column_L_14GHz      = MaskedColumn(large_sample_L, name='L_20CM', unit='W/Hz', description='1.4 GHz lum',\\\n",
    "    #                                     fill_value=np.nan, mask=np.array(large_sample_L == 0))\n",
    "    # column_L_14GHz_err  = MaskedColumn(large_sample_L_e, name='L_20CM_ERR', unit='W/Hz', description='1.4 GHz lum err',\\\n",
    "    #                                    fill_value=np.nan, mask=np.array(large_sample_L_e == 0))\n",
    "    column_L_14GHz_up   = MaskedColumn(large_sample_u_lim, name='L_20CM_UP_LIM', dtype='bool', description='L_20CMUlim')\n",
    "    # column_L_250GHz     = MaskedColumn(large_sample_L_250, name='L_250GHZ', unit='W/Hz', description='250 GHz lum',\\\n",
    "    #                                    fill_value=np.nan, mask=np.array(large_sample_L_250 == 0))\n",
    "    # column_L_250GHz_err = MaskedColumn(large_sample_L_250_e, name='L_250GHZ_ERR', unit='W/Hz', description='250 GHz lum err',\\\n",
    "    #.                                   fill_value=np.nan, mask=np.array(large_sample_L_250_e == 0))\n",
    "    column_f_20cm       = MaskedColumn(large_sample_f20cm, name='F_20CM', unit='mJy', description='20cmF',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_f20cm == 0))\n",
    "    column_f_20cm_err   = MaskedColumn(large_sample_f20cm_e, name='F_20CM_ERR', unit='mJy', description='20cmF_e',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_f20cm_e == 0))\n",
    "    column_f_6cm        = MaskedColumn(large_sample_f_6cm, name='F_6CM', unit='mJy', description='6cmF',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_f_6cm == 0))\n",
    "    column_f_6cm_err    = MaskedColumn(large_sample_f_6cm_e, name='F_6CM_ERR', unit='mJy', description='6cmF_e',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_f_6cm_e == 0))\n",
    "    column_f_3GHz       = MaskedColumn(large_sample_f_3GHz, name='F_3GHZ', unit='mJy', description='3GHzF',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_f_3GHz == 0))\n",
    "    column_f_3GHz_err   = MaskedColumn(large_sample_f_3GHz_e, name='F_3GHZ_ERR', unit='mJy', description='3GHzF_e',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_f_3GHz_e == 0))\n",
    "    column_f_15GHz      = MaskedColumn(large_sample_f_15GHz, name='F_1.5GHZ', unit='mJy', description='1.5GHzF',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_f_15GHz == 0))\n",
    "    column_f_15GHz_err  = MaskedColumn(large_sample_f_15GHz_e, name='F_1.5GHZ_ERR', unit='mJy', description='1.5GHzF_e',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_f_15GHz_e == 0))\n",
    "    column_f_xmm        = MaskedColumn(large_sample_xmm_flux, name='F_XMM', unit='mJy', description='0.2-12keVF',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_xmm_flux == 0))\n",
    "    column_f_xmm_err    = MaskedColumn(large_sample_xmm_flux_e, name='F_XMM_ERR', unit='mJy', description='0.2-12keVF_e',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_xmm_flux_e == 0))\n",
    "    column_f_250GHz     = MaskedColumn(large_sample_f250GHz, name='F_250GHZ', unit='mJy', description='250GHzF',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_f250GHz == 0))\n",
    "    column_f_250GHz_err = MaskedColumn(large_sample_f250GHz_e, name='F_250GHZ_ERR', unit='mJy', description='250GHzF_e',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_f250GHz_e == 0))\n",
    "    column_mass_1450    = MaskedColumn(large_sample_mass_1450, name='MASS_1450', unit='Msun', description='1450A (UV) mass',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_mass_1450 == 0))\n",
    "    column_origin       = MaskedColumn(large_sample_origin_flg, name='ORIGIN', unit='', description='survey origin',\\\n",
    "                                       dtype='str', fill_value=np.nan, mask=~np.isfinite(large_sample_origin_flg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_flag:\n",
    "    np.shape(result_table_simbad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_flag:\n",
    "    result_table_simbad.add_columns([column_cat_index, column_cat_coords, column_z_own, column_z_own_err,\\\n",
    "                                     column_L_14GHz_up, column_f_20cm, column_f_20cm_err, column_f_6cm, column_f_6cm_err,\\\n",
    "                                     column_f_3GHz, column_f_3GHz_err, column_f_15GHz, column_f_15GHz_err, column_f_xmm,\\\n",
    "                                     column_f_xmm_err, column_f_250GHz, column_f_250GHz_err, column_mass_1450, column_origin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_flag:\n",
    "    str_id = result_table_simbad['MAIN_ID'].astype('str')\n",
    "    result_table_simbad.replace_column('MAIN_ID', str_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_table = result_table_simbad.filled(fill_value=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the table into a file. It can be `.fits`, `.votable`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy_table.write('high_z_qsos.ecsv', format='ascii.ecsv', overwrite=True, serialize_method='data_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_flag:\n",
    "    result_table_simbad.write(cat_path + 'large_cat_simbad_query_inayoshi_jun2020.fits', format='fits', overwrite=True, serialize_method='data_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_flag:\n",
    "    result_table_simbad.write(cat_path + 'large_cat_simbad_query_inayoshi_jun2020.csv', format='ascii.csv', overwrite=True, serialize_method='data_mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save running time, we can load the data from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if read_simbad_inayoshi_flag:\n",
    "#    result_table_simbad = Table.read(cat_path + 'large_cat_simbad_query_inayoshi.csv', format='ascii.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_simbad_inayoshi_flag:\n",
    "    result_table_simbad = Table.read(cat_path + 'large_cat_simbad_query_inayoshi_jun2020.fits');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_simbad_inayoshi_flag or create_simbad_inayoshi_flag:\n",
    "    result_table_simbad.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the objects of the table in other catalogs and services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from astroquery.heasarc import Heasarc\n",
    "#Heasarc.query_mission_cols(mission='radio')\n",
    "#tabb = Heasarc.query(large_sample_names, mission='radio', timeout=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "customNed        = Ned()\n",
    "fields_to_remove = ['No.', 'Photometry Measurement', 'Uncertainty', 'Units', 'Significance', 'Published frequency', 'Frequency Mode', 'Coordinates Targeted', 'Spatial Mode', 'Qualifiers', 'Comments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying sources with name in `Ned`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty_counter = 0\n",
    "# res_tab       = {}\n",
    "# for name in large_sample_names:\n",
    "#     try:\n",
    "#         res_tab[name] = customNed.get_table(name, output_table_format=1)\n",
    "#         res_tab[name].remove_columns(fields_to_remove)\n",
    "#     except:\n",
    "#         res_tab[name] = Table()\n",
    "#         empty_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can do it with coordinates.  \n",
    "\n",
    "First, we query the coordinates. If we found something,  \n",
    "we use the name of the source to obtain it photometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_simbad_inayoshi_flag or create_simbad_inayoshi_flag:\n",
    "    coords_simbad_inayoshi = SkyCoord(result_table_simbad['RA_d'], result_table_simbad['DEC_d'], unit=u.deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_tab_name_counter  = 0\n",
    "empty_tab_photo_counter = 0\n",
    "error_tab_name_counter  = 0\n",
    "error_tab_photo_counter = 0\n",
    "ned_tables              = {}\n",
    "ned_info                = {}\n",
    "ned_names               = []\n",
    "#ned_names               = np.array([''  for x in np.arange(np.shape(large_sample_names)[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14235 of 14235) |##################| Elapsed Time: 0:24:39 Time:  0:24:39\n"
     ]
    }
   ],
   "source": [
    "if query_ned_names_flag:\n",
    "    with progressbar.ProgressBar(min_value=0, max_value=np.shape(coords_simbad_inayoshi)[0]) as bar:\n",
    "        for index, coord in enumerate(coords_simbad_inayoshi):\n",
    "            try:\n",
    "                init_table            = customNed.query_region(coords_simbad_inayoshi[index], radius=2.5*u.arcsec)\n",
    "                if len(init_table) == 0:\n",
    "                    # init_table        = Table(names=('Object Name', 'RA', 'DEC'), dtype=('str', 'float', 'float'), masked=True)\n",
    "                    # init_table.add_row(('No Name', coords_simbad_inayoshi[index].ra.deg, coords_simbad_inayoshi[index].dec.deg), mask=[True, False, False])\n",
    "                    first_row         = [['No Name'], [coords_simbad_inayoshi[index].ra.deg], [coords_simbad_inayoshi[index].dec.deg]]\n",
    "                    init_table        = Table(first_row, names=('Object Name', 'RA', 'DEC'), dtype=('str', 'float', 'float'), masked=True)\n",
    "                    ned_info[index]   = init_table\n",
    "                    ned_names.append('No Name')\n",
    "                    empty_tab_name_counter += 1\n",
    "                    bar.update(index)\n",
    "                    continue\n",
    "                # init_table.remove_columns(['Magnitude and Filter', 'Positions', 'Diameter Points'])\n",
    "                used_source_idx   = np.nanargmin(init_table['Separation'])  # Index of element with lowest separation from coords\n",
    "                ned_info[index]   = Table(init_table[used_source_idx])\n",
    "                init_name         = init_table['Object Name'][used_source_idx]\n",
    "                # ned_names[index]  = init_name\n",
    "                ned_names.append(init_name)\n",
    "            except:\n",
    "                # init_table        = Table(names=('Object Name', 'RA', 'DEC'), dtype=('str', 'float', 'float'), masked=True)\n",
    "                # init_table.add_row(('No Name', coords_simbad_inayoshi[index].ra.deg, coords_simbad_inayoshi[index].dec.deg), mask=[True, False, False])\n",
    "                first_row         = [['No Name'], [coords_simbad_inayoshi[index].ra.deg], [coords_simbad_inayoshi[index].dec.deg]]\n",
    "                init_table        = Table(first_row, names=('Object Name', 'RA', 'DEC'), dtype=('str', 'float', 'float'), masked=True)\n",
    "                ned_info[index]   = init_table\n",
    "                ned_names.append('No Name')\n",
    "                error_tab_name_counter += 1\n",
    "            bar.update(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_table            = customNed.query_region(coords_simbad_inayoshi[18022], radius=4.0*u.arcsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_ned_names_flag:\n",
    "    ned_names = np.array(ned_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14235 of 14235) |##################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "ned_redshifts = []\n",
    "with progressbar.ProgressBar(min_value=0, max_value=np.shape(ned_names)[0]) as bar:\n",
    "    for key in ned_info:\n",
    "        if np.shape(ned_info[key].colnames)[0] < 4:\n",
    "            ned_redshifts.append(np.nan)\n",
    "            bar.update(key)\n",
    "            continue\n",
    "        ned_redshifts.append(ned_info[key]['Redshift'])\n",
    "        bar.update(key)\n",
    "ned_redshifts = np.array(ned_redshifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_count = 0\n",
    "indices_non   = []\n",
    "for index, name in enumerate(ned_names):\n",
    "    if name != 'No Name':\n",
    "        counter_count += 1\n",
    "for index, name in enumerate(ned_names):\n",
    "    if name == 'No Name':\n",
    "        indices_non.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_ned_names_flag:\n",
    "    ned_names[1234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_tab_temp   = customNed.get_table(str(ned_names[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14235 of 14235) |##################| Elapsed Time: 0:07:19 Time:  0:07:19\n"
     ]
    }
   ],
   "source": [
    "if query_ned_photometry_flag:\n",
    "    with progressbar.ProgressBar(min_value=0, max_value=np.shape(ned_names)[0]) as bar:\n",
    "        for index, name in enumerate(ned_names):\n",
    "            try:\n",
    "                if name == 'No Name':\n",
    "                    # phot_table        = Table(names=('Observed Passband',), dtype=('str',), masked=True)\n",
    "                    # phot_table.add_row(('No Passband',), mask=(True,))\n",
    "                    first_row         = [[0], ['No Passband']]\n",
    "                    phot_table        = Table(first_row, names=('No.', 'Observed Passband'), dtype=('int', 'str'), masked=True)\n",
    "                    ned_tables[index] = phot_table\n",
    "                    empty_tab_photo_counter += 1\n",
    "                    bar.update(index)\n",
    "                    continue\n",
    "                phot_table            = customNed.get_table(name, table='photometry', output_table_format=3)\n",
    "                # phot_table.remove_columns(fields_to_remove)\n",
    "                ned_tables[index]     = phot_table\n",
    "            except:\n",
    "                # phot_table            = Table(names=('Observed Passband',), dtype=('str',), masked=True)\n",
    "                # phot_table.add_row(('No Passband',), mask=(True,))\n",
    "                first_row             = [[0], ['No Passband']]\n",
    "                phot_table            = Table(first_row, names=('No.', 'Observed Passband'), dtype=('int', 'str'), masked=True)\n",
    "                ned_tables[index]     = phot_table\n",
    "                error_tab_photo_counter += 1\n",
    "            bar.update(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_tab_temp   = customNed.get_table('Himiko', table='photometry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save multiple data tables into one file with `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    pass_band_name = ned_tables[key]['Observed Passband'].astype('str')\n",
    "    ned_tables[key].replace_column('Observed Passband', pass_band_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gather the names and frequencies of all columns present in tables from `Ned`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14235 of 14235) |##################| Elapsed Time: 0:00:16 Time:  0:00:16\n"
     ]
    }
   ],
   "source": [
    "if create_phot_bands_list_flag:\n",
    "    band_names_array = []\n",
    "    \n",
    "    n_t_str          = '\\t'\n",
    "    \n",
    "    with progressbar.ProgressBar(min_value=0, max_value=np.shape(ned_names)[0]) as bar:\n",
    "        for key in ned_tables:\n",
    "            if ned_tables[key]['Observed Passband'][0] == 'No Passband':\n",
    "                bar.update(key)\n",
    "                continue\n",
    "            pass_band_name = ned_tables[key]['Observed Passband'].astype('str')\n",
    "            ned_tables[key].replace_column('Observed Passband', pass_band_name)\n",
    "            temp_table    = ned_tables[key]['Observed Passband', 'Frequency'].as_array().data\n",
    "            for temp_pair in temp_table:\n",
    "                band_names_array.append([str(temp_pair[0]), f'{temp_pair[1]:.4e}'])\n",
    "            bar.update(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique rows (name + frequency pair) are retrieved from the previous list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11205\n"
     ]
    }
   ],
   "source": [
    "useful_number = 0\n",
    "for key in ned_tables:\n",
    "    if np.shape(ned_tables[key].colnames)[0] >= 3:\n",
    "        useful_number += 1\n",
    "print(useful_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_phot_bands_list_flag:\n",
    "    unique_rows_band_names_array = np.unique(band_names_array, axis=0)\n",
    "    unique_rows_band_names_array = unique_rows_band_names_array[unique_rows_band_names_array[:, 1].astype(np.float).argsort()]  # Order by frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the size of this new array. It represents the number of  \n",
    "unique passband configurations gathered from querying `Ned`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(911, 2)\n"
     ]
    }
   ],
   "source": [
    "if create_phot_bands_list_flag:\n",
    "    print(np.shape(unique_rows_band_names_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_rows_band_names_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving this list into a plain text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_phot_bands_list_flag:\n",
    "    # np.savetxt(cat_path + 'all_ned_band_names.txt', unique_rows_band_names_array, fmt='%s %s', delimiter='\\t')\n",
    "    # np.savetxt(cat_path + 'all_ned_band_names_2_5arcsec.txt', unique_rows_band_names_array, fmt='%s %s', delimiter='\\t')\n",
    "    np.savetxt(cat_path + 'all_ned_band_names_jun2020.txt', unique_rows_band_names_array, fmt='%s %s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new file can also be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_phot_bands_list_flag:\n",
    "    unique_rows_band_names_array = np.genfromtxt(cat_path + 'all_ned_band_names_jun2020.txt', delimiter='\\t', dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a unique table from all the individual photometry tables obtained after  \n",
    "querying `Ned`. We discard measurements which have already been reported (for each individual source)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leave uncertainty values as string to retain information about possible upper/lower limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "if order_ned_photometry_flag:\n",
    "    limit_set_ned = int(np.floor(np.shape(ned_names)[0]/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order_ned_photometry_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14235 of 14235) |##################| Elapsed Time: 0:40:16 Time:  0:40:16\n"
     ]
    }
   ],
   "source": [
    "if order_ned_photometry_flag:\n",
    "    temp_table_ned_photo             = Table()\n",
    "    chunk_size                       = 300  # Number of elements to calculate before dumping results to external table\n",
    "    with progressbar.ProgressBar(min_value=0, max_value=np.shape(ned_names)[0]) as bar:\n",
    "        for index, source_name in enumerate(ned_names):  # Some names will be 'No Name'\n",
    "            band_names_str           = []\n",
    "            column_names_str         = []\n",
    "            band_frequencies         = []\n",
    "            measure_names            = ned_tables[index].colnames[1:]\n",
    "            # init_table = Table(names=('MAIN_ID',), dtype=('str',), masked=True)\n",
    "            cord_str                 = coords_simbad_inayoshi[index].to_string('decimal')\n",
    "            init_table               = Table(data=np.array([index, cord_str, source_name]), names=('INDEX', 'COORD', 'MAIN_ID'), dtype=('int', 'str', 'str'), masked=True)\n",
    "            if source_name == 'No Name':\n",
    "                # init_table = Table(('No Name',), names=('MAIN_ID',), dtype=('str',), masked=True)\n",
    "                # init_table.add_row(('No Name',), mask=(True,)) # Mask values in the last step instead\n",
    "                if index == 0 or (index % chunk_size == 1 and index > 1):\n",
    "                    init_table_large     = Table(init_table)\n",
    "                    bar.update(index)\n",
    "                    continue\n",
    "                init_table_large     = vstack([init_table_large, init_table])\n",
    "                if index % chunk_size == 0 and index > 0:\n",
    "                    temp_table_ned_photo = vstack([temp_table_ned_photo, init_table_large])\n",
    "                    # result_table_simbad_copy = join(result_table_simbad_copy, init_table_large, keys='COORD', join_type='outer')\n",
    "                bar.update(index)\n",
    "                continue\n",
    "            if len(measure_names) == 0 or len(measure_names) == 1:\n",
    "                # init_table = Table(('No Name',), names=('MAIN_ID',), dtype=('str',), masked=True)\n",
    "                # init_table.add_row((source_name,), mask=(True,))\n",
    "                if index == 0 or (index % chunk_size == 1 and index > 1):\n",
    "                    init_table_large     = Table(init_table)\n",
    "                    bar.update(index)\n",
    "                    continue\n",
    "                init_table_large     = vstack([init_table_large, init_table])\n",
    "                if index % chunk_size == 0 and index > 0:\n",
    "                    temp_table_ned_photo = vstack([temp_table_ned_photo, init_table_large])\n",
    "                    # result_table_simbad_copy = join(result_table_simbad_copy, init_table_large, keys='COORD', join_type='outer')\n",
    "                bar.update(index)\n",
    "                continue\n",
    "            # init_table = Table((str(source_name),), names=('MAIN_ID',), dtype=('str',), masked=True)\n",
    "            # init_table.add_row((str(source_name),))\n",
    "            for row in ned_tables[index]:\n",
    "                # band_name_str        = re.sub(r' \\(.*', '', str(row['Observed Passband'].decode('utf-8')))  # Eliminate differences\n",
    "                # band_name_str        = str(row['Observed Passband'].decode('utf-8'))  # Eliminate differences\n",
    "                if type(row['Observed Passband']) != bytes:\n",
    "                    band_name_str        = str(row['Observed Passband'])  # Eliminate differences\n",
    "                if type(row['Observed Passband']) == bytes:\n",
    "                    band_name_str        = str(row['Observed Passband'].decode('utf-8'))  # Eliminate differences\n",
    "                # if str(band_name_str) not in band_names_str and row['Frequency'] not in band_frequencies:\n",
    "                if str(band_name_str) not in band_names_str:\n",
    "                    band_frequencies.append(row['Frequency'])\n",
    "                    band_names_str.append(str(band_name_str))\n",
    "                    column_name_flux = 'Flux Density ' + band_name_str\n",
    "                    column_name_err  = 'NED Uncertainty ' + band_name_str\n",
    "                    if column_name_flux not in column_names_str:\n",
    "                        column_names_str.append(column_name_flux)\n",
    "                        column_flux  = MaskedColumn(row['Flux Density'], name=column_name_flux, unit=ned_tables[index]['Flux Density'].unit, dtype='float')\n",
    "                        column_err   = MaskedColumn(row['NED Uncertainty'], name=column_name_err, dtype='str')\n",
    "                        init_table.add_columns((column_flux, column_err))\n",
    "            #init_table.remove_column('MAIN_ID')\n",
    "            if index == 0 or (index % chunk_size == 1 and index > 1):\n",
    "                init_table_large     = Table(init_table)\n",
    "                bar.update(index)\n",
    "                continue\n",
    "            init_table_large         = vstack([init_table_large, init_table])\n",
    "            if index % chunk_size == 0 and index > 0:\n",
    "                temp_table_ned_photo = vstack([temp_table_ned_photo, init_table_large])\n",
    "                # result_table_simbad_copy = join(result_table_simbad_copy, init_table_large, keys='COORD', join_type='outer')\n",
    "            bar.update(index)\n",
    "    temp_table_ned_photo = vstack([temp_table_ned_photo, init_table_large])\n",
    "    # result_table_simbad_copy = join(result_table_simbad_copy, init_table_large, keys='COORD', join_type='outer')\n",
    "    #result_table_simbad_copy         = Table(result_table_simbad)\n",
    "    #result_table_simbad_copy.add_column(coords_simbad_inayoshi.to_string('decimal'), name='COORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "if order_ned_photometry_flag:\n",
    "    len(measure_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new merged column shows measurements for the following number of bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "if order_ned_photometry_flag:\n",
    "    len(temp_table_ned_photo.colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data completeness, we mask entries for `MAIN_ID` and `COORD` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    column_coord = MaskedColumn(temp_table_ned_photo['COORD'], name='COORD', mask=np.array(temp_table_ned_photo['COORD'] == ''))\n",
    "    column_id    = MaskedColumn(temp_table_ned_photo['MAIN_ID'], name='MAIN_ID', description='identify object', mask=np.array(temp_table_ned_photo['MAIN_ID'] == 'No Name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    temp_table_ned_photo.replace_column('COORD', column_coord)\n",
    "    temp_table_ned_photo.replace_column('MAIN_ID', column_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform `astropy` tables into `pandas` data frames for further calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    df_ned    = temp_table_ned_photo.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    df_simbad = result_table_simbad.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this point, we can merge both `simbad` and `Ned` tables into a larger one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    merged_ned_simbad = pd.merge(df_simbad, df_ned, on='INDEX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_ned_simbad.columns[0:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save this new table into a `HDF5`-format file for future access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rcarvajal/.miniconda3/lib/python3.7/site-packages/pandas/core/generic.py:2505: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block4_values] [items->Index(['MAIN_ID_x', 'COORD_x', 'ORIGIN', 'COORD_y', 'MAIN_ID_y',\n",
      "       'NED Uncertainty u (SDSS PSF) AB', 'NED Uncertainty u (SDSS CModel) AB',\n",
      "       'NED Uncertainty u (SDSS Model) AB', 'NED Uncertainty g (SDSS PSF) AB',\n",
      "       'NED Uncertainty g (SDSS CModel) AB',\n",
      "       ...\n",
      "       'NED Uncertainty H{beta} (Keck II)',\n",
      "       'NED Uncertainty H{alpha} (Keck II)', 'NED Uncertainty i+ (Subaru) AB',\n",
      "       'NED Uncertainty K_s (CFHT)', 'NED Uncertainty [NeIII] 3869 VIRUS-P',\n",
      "       'NED Uncertainty H{gamma} (VIRUS-P)', 'NED Uncertainty F160W (HST) AB',\n",
      "       'NED Uncertainty K_s (VLT)', 'NED Uncertainty [Ne III] 3869 (VLT)',\n",
      "       'NED Uncertainty i (CFHT)'],\n",
      "      dtype='object', length=849)]\n",
      "\n",
      "  encoding=encoding,\n"
     ]
    }
   ],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    merged_ned_simbad.to_hdf(cat_path + 'large_cat_simbad_inayoshi_ned_jun2020.h5', 'df')\n",
    "    # merged_ned_simbad.to_hdf(cat_path + 'large_cat_simbad_inayoshi_ned_all_cols.h5', 'df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, we can read the table from an external file to avoid extra running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_simbad_inayoshi_ned_flag:\n",
    "    power_test = pd.read_hdf(cat_path + 'large_cat_simbad_inayoshi_ned_jun2020.h5', 'df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this `pandas` data frame can be transformed into an `astropy` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# power_table = Table.from_pandas(power_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
