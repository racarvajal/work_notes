{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to gather data from $z > 5.5$ QSOs.\n",
    "\n",
    "We want to obtain data from several surveys and catalogs in order to  \n",
    "manipulate them and try to obtain meaningful correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the packages to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the first line working, you need  \n",
    "to run the following lines:\n",
    "\n",
    "```bash\n",
    " conda install nodejs\n",
    " pip install ipympl\n",
    " pip install --upgrade jupyterlab\n",
    " jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    " jupyter labextension install jupyter-matplotlib\n",
    " jupyter nbextension enable --py widgetsnbextension\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Static plots\n",
    "#%matplotlib ipympl\n",
    "# Interactive plots\n",
    "import numpy as np\n",
    "# import matplotlib.cm as cm\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.colors as mcolors\n",
    "# import matplotlib.patheffects as mpe\n",
    "# import matplotlib.patheffects as path_effects\n",
    "# from matplotlib.ticker import FormatStrFormatter\n",
    "# from matplotlib.patches import Polygon\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy.table import Column\n",
    "from astropy.table import MaskedColumn\n",
    "from astropy.table import hstack\n",
    "from astropy.table import vstack\n",
    "from astropy.table import join\n",
    "from astropy.wcs import WCS\n",
    "from astropy import units as u\n",
    "# from astropy.visualization import hist\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astroquery.simbad import Simbad\n",
    "from astroquery.ned import Ned\n",
    "import getpass\n",
    "import progressbar\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One function, to derive luminosity distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def luminosity_distance(z_i, H0=70., WM=0.3, WV=0.7):\n",
    "    z_i   = np.array([z_i], dtype='float64').flatten()\n",
    "    c     = 299792.458 # velocity of light in km/sec\n",
    "    h     = H0 / 100.\n",
    "    WR    = 4.165E-5 / (h * h)   # includes 3 massless neutrino species, T0 = 2.72528\n",
    "    WK    = 1 - WM - WR - WV\n",
    "    azs   = 1.0 / (1 + z_i)\n",
    "    DTT   = 0.0\n",
    "    DCMR  = 0.0\n",
    "    # do integral over a=1/(1+z) from az to 1 in n steps, midpoint rule\n",
    "    n     = 1000  # number of points in integrals\n",
    "    DL_Mpcs = np.zeros_like(z_i)\n",
    "    for count, az in enumerate(azs):\n",
    "        a     = az + (1 - az) * (np.arange(0, n) + 0.5) / n\n",
    "        adot  = np.sqrt(WK + (WM / a) + (WR / (a * a)) + (WV * a * a))\n",
    "        for i in range(n):\n",
    "            # a    = az + (1 - az) * (i + 0.5) / n\n",
    "            # adot = math.sqrt(WK + (WM / a) + (WR / (a * a)) + (WV * a * a))\n",
    "            DTT  = DTT + 1. / adot[i]\n",
    "            DCMR = DCMR + 1. / (a[i] * adot[i])\n",
    "        DTT   = (1. - az) * DTT / n\n",
    "        DCMR  = (1. - az) * DCMR / n\n",
    "        # tangential comoving distance\n",
    "        ratio = 1.00\n",
    "        x     = np.sqrt(abs(WK)) * DCMR\n",
    "        if x > 0.1:\n",
    "            if WK > 0:\n",
    "                ratio =  0.5 * (np.exp(x) - np.exp(-x)) / x \n",
    "            else:\n",
    "                ratio = np.sin(x) / x\n",
    "        else:\n",
    "            y = x * x\n",
    "        if WK < 0: y = -y\n",
    "        ratio  = 1. + y / 6. + y * y / 120.\n",
    "        DCMT   = ratio * DCMR\n",
    "        DA     = az * DCMT\n",
    "        DL     = DA / (az * az)\n",
    "        DL_Mpc = (c / H0) * DL\n",
    "        DL_Mpcs[count] = DL_Mpc\n",
    "    return DL_Mpcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the spectral index $\\alpha$ from different sources  \n",
    "to be used in the luminosity calculations (K-correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_first = 0.5  # From FIRST data (Bornancini+2010)\n",
    "alpha_RG    = 1.0  # For radio galaxies (Verkhodanov & Khabibullina, 2010)\n",
    "alpha_alex  = 0.8  # Star-forming galaxies (Alexander+2003)\n",
    "alpha_smol  = 0.7  # Mean value from VLA-COSMOS 3GHz sample (Smolčić et al. 2017)\n",
    "alpha_butl  = 0.75  # From Butler et al., 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose one of the spectral indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_used  = alpha_butl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step, reading our data.  \n",
    "Most of the data files have been created using Topcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine  = getpass.getuser()\n",
    "# cat_path = '/home/' + machine + '/Documentos/Data/'\n",
    "cat_path = ''  # relative path to the same directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from FIRST+MILLIQUAS catalogs cross-matched.  \n",
    "\n",
    "**MILLIQUAS + FIRST (SDSS)**   \n",
    "\n",
    "Redshift values have been retrieved from the column `pipeline_redshift` in SDSS DR12  \n",
    "The procedure to obtain these values is explained in **Bolton+2012**  \n",
    "\n",
    "We also select sources with have explicitely data in both SDSS Quasar Catalog and  \n",
    "in the FIRST survey (`first_match_flag = 1`).  \n",
    "\n",
    "In order to get only the best redshift values, we select sources with  \n",
    "`pipeline_redshift_flag = 0`.\n",
    "\n",
    "Thus, we can use 9161 objects from the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hdu_list = fits.open(cat_path + 'tables_matches_milli_sdss_apr.fits');\n",
    "#sdss_milli  = Table(hdu_list[1].data);\n",
    "#hdu_list.close();\n",
    "\n",
    "sdss_milli = Table.read(cat_path + 'tables_matches_milli_sdss_apr.fits')\n",
    "\n",
    "L_14GHz_filter = np.array((sdss_milli['L_14GHz'] > 0.0) * (sdss_milli['first_offset'] < 1.0)) ;  # sdss + milliquasar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load data from the VLA-COSMOS 3 GHz Large Project\n",
    "**Smolčić et al. 2017**\n",
    "\n",
    "We can, also, separate sources which have an AGN X-ray counterpart\n",
    "with the flag `cosm_xray_flag`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdu_cosmos          = fits.open(cat_path + 'vla_xmmcosmos_smolcic.fits')\n",
    "# cosmos_data         = Table(hdu_cosmos[1].data)\n",
    "# hdu_cosmos.close()\n",
    "\n",
    "cosmos_data         = Table.read(cat_path + 'vla_xmmcosmos_smolcic.fits')\n",
    "\n",
    "x_ray_flag          = cosmos_data['cosm_xray_flag']\n",
    "redshift_cosmos     = cosmos_data['cosm_z']\n",
    "redshift_cosmos[~np.isfinite(redshift_cosmos)] = 0.0  # Repair NaN values\n",
    "int_14GHz_cosmos    = cosmos_data['cosm_lum_14ghz']  # Using their own spectral indices\n",
    "cosmos_3GHz_flux    = cosmos_data['cosm_f_3ghz']  # mJy\n",
    "cosmos_3GHz_flux_e  = cosmos_data['cosm_f_3ghz_err']\n",
    "cosmos_3GHz_flux_e[~np.isfinite(cosmos_3GHz_flux_e)] = 0.0  # Repair NaN values\n",
    "cosmos_14GHz_flux   = cosmos_3GHz_flux_e * (3/1.4)**alpha_used\n",
    "cosmos_14GHz_flux[~np.isfinite(cosmos_14GHz_flux)] = 0.0  # Repair NaN values\n",
    "cosmos_14GHz_flux_e = np.abs(cosmos_14GHz_flux) * cosmos_3GHz_flux_e / cosmos_3GHz_flux\n",
    "lum_dist_cosmos     = cosmos_data['D_lum']  # in m\n",
    "lum_dist_cosmos[~np.isfinite(lum_dist_cosmos)] = 0.0  # Repair NaN values\n",
    "\n",
    "redshift_cosmos_x   = cosmos_data['cosm_z'][(cosmos_data['cosm_z'] > 0) * (cosmos_data['cosm_xray_flag'] == 'T')]\n",
    "int_14GHz_cosmos_x  = cosmos_data['cosm_lum_14ghz'][(cosmos_data['cosm_z'] > 0) * (cosmos_data['cosm_xray_flag'] == 'T')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want, also, to add $z > 6$ QSOs from the list in  \n",
    "Table 3 in the review of **Inayoshi, Visbal, and Haiman, 2020**.  \n",
    "Six of them have $z > 7$\n",
    "\n",
    "Not all of them have $1.4$ GHz measurements. Others have  \n",
    "measurements in different frequencies which can be translated  \n",
    "into the desired frequency using, for instance, the relation  \n",
    "from **Butler et al., 2018**:\n",
    "\n",
    "$$S_{a} = S_{b} \\times (\\frac{\\nu_{b}}{\\nu_{a}})^{\\alpha}$$  \n",
    "\n",
    "We load the data from these sources. Fluxes from different frequencies than $1.4$ GHz are translated to the needed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_z_ra        = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[1],  dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_dec       = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[2],  dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_zs        = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[3],  dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_zs_e      = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[4],  dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_14GHz     = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[6],  dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_14GHz_e   = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[7],  dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_3GHz      = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[8],  dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_3GHz_e    = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[9],  dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_15GHz     = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[10], dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_15GHz_e   = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[11], dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_250GHz    = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[12], dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_250GHz_e  = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[13], dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_mass_1450 = np.char.replace(np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[14], dtype='str', delimiter=';'), ',', '.').astype(np.float)\n",
    "high_z_names     = np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[0], dtype='str', delimiter=';')\n",
    "high_z_lum_d     = luminosity_distance(high_z_zs) * 3.086e22  # in m\n",
    "high_z_up_lim    = np.array([val == '<' for val in np.loadtxt(cat_path + 'high_z_qso_props.csv', usecols=[5], dtype='str', delimiter=';')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulate values into one array except 250GHz data.  \n",
    "Millimetre luminosities will be used separately since we cannot be completely  \n",
    "sure that they represent, fully, non-thermal emission (from AGN) and not dust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_z_14   = high_z_14GHz + high_z_3GHz + high_z_15GHz\n",
    "high_z_14_e = high_z_14GHz_e + high_z_3GHz_e + high_z_15GHz_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complement the dataset, we also load four $z > 5.5$ sources which  \n",
    "come from the `radio` catalog in the `Heasarc` database.  \n",
    "We queried the objects which have $1.4$ GHz observations that  \n",
    "are within a $2.5$ arcsec of an object from the `SDSS QUASAR DR12`  \n",
    "catalog. We discard the sources that are already included in the `FIRST`  \n",
    "catalog, to avoid repetition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enlarge the size of the sample, we can also query  \n",
    "the same sample, but extending the redshift range to all positive  \n",
    "values. We can include, too, the exclusion of ***low quality*** redshift_values (or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdu_radio_z_55       = fits.open(cat_path + 'radio_cat_sdss_z_55.fits');  # high redshift\n",
    "# hdu_radio_z_all_spec = fits.open(cat_path + 'radio_cat_sdss_z_all_spec.fits');  # all redshift, high z quality\n",
    "#hdu_radio_z_all_all  = fits.open(cat_path + 'radio_cat_sdss_z_all_all.fits');  # all redshift, all z quality\n",
    "# radio_sources        = Table(hdu_radio_z_55[1].data);\n",
    "# radio_sources        = Table(hdu_radio_z_all_spec[1].data);\n",
    "# hdu_radio_z_55.close();\n",
    "# hdu_radio_z_all_spec.close();\n",
    "# hdu_radio_z_all_all.close();\n",
    "\n",
    "# radio_sources        = Table.read(cat_path + 'radio_cat_sdss_z_55_spec.fits')       # high redshift\n",
    "radio_sources        = Table.read(cat_path + 'radio_cat_sdss_z_all_spec.fits')      # all redshift, high z quality\n",
    "# radio_sources        = Table.read(cat_path + 'radio_cat_sdss_z_all_all_spec.fits')  # all redshift, all z quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate luminosities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate luminosities (in W/Hz) for different datasets  \n",
    "using the expression\n",
    "\n",
    "$$L_{1.4\\mathrm{GHz}} = 4 \\pi \\mathrm{d}^{2}_{L} f_{1.4\\mathrm{GHz}} (1 + z)^{\\alpha - 1}$$\n",
    "\n",
    "which comes from Alexander et al. 2003\n",
    "\n",
    "We can also obtain that luminosity from the flux in $3$ GHz as\n",
    "\n",
    "$$L_{1.4\\mathrm{GHz}} = 4 \\pi \\mathrm{d}^{2}_{L} {(\\frac{3}{1.4})}^{\\alpha} f_{3\\mathrm{GHz}} (1 + z)^{\\alpha - 1}$$\n",
    "\n",
    "This expression comes from Delhaize et al. 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_21cm           = 4 * np.pi * (sdss_milli['D_lum'][L_14GHz_filter])**2 * sdss_milli['flux_20_cm'][L_14GHz_filter] * 1e-3 * 1e-26 * (1 + sdss_milli['redshift'][L_14GHz_filter])**(alpha_used - 1)\n",
    "L_21cm_e         = np.abs(L_21cm) / sdss_milli['snr_20_cm'][L_14GHz_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_21cm_radio     = 4 * np.pi * (radio_sources['D_lum'])**2 * radio_sources['radio_flux'] * 1e-3 * 1e-26 * (1 + radio_sources['sdss_z'])**(alpha_used - 1)\n",
    "L_21cm_radio_e   = np.abs(L_21cm_radio) * radio_sources['radio_flux_err'] / radio_sources['radio_flux']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rcarvajalp/.anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "L_14GHz_cosmos   = 4 * np.pi * (lum_dist_cosmos)**2 * cosmos_14GHz_flux * 1e-3 * 1e-26 * (1 + redshift_cosmos)**(alpha_used - 1)\n",
    "L_14GHz_cosmos_e = np.abs(L_14GHz_cosmos) * cosmos_14GHz_flux_e / cosmos_14GHz_flux;\n",
    "L_14GHz_cosmos_e[~np.isfinite(L_14GHz_cosmos_e)] = 0.0  # Repair (make zero) error values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_z_lum_14    = 4 * np.pi * high_z_lum_d**2 * high_z_14GHz  * 1e-6 * 1e-26 * (1 + high_z_zs)**(alpha_used - 1)\n",
    "high_z_lum_3     = 4 * np.pi * high_z_lum_d**2 * high_z_3GHz   * 1e-6 * 1e-26 * (1 + high_z_zs)**(alpha_used - 1) * (3/1.4)**alpha_used\n",
    "high_z_lum_15    = 4 * np.pi * high_z_lum_d**2 * high_z_15GHz  * 1e-6 * 1e-26 * (1 + high_z_zs)**(alpha_used - 1) * (1.5/1.4)**alpha_used\n",
    "high_z_lum_250   = 4 * np.pi * high_z_lum_d**2 * high_z_250GHz * 1e-6 * 1e-26 * (1 + high_z_zs)**(alpha_used - 1) * (250/1.4)**(alpha_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mix all luminosities (different bands) to obtain single value (adding zeroes).  \n",
    "Millimetre luminosities will be used separately since we cannot be completely  \n",
    "sure that they represent, fully, non-thermal emission (from AGN) and not dust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_z_lum_14GHz   = high_z_lum_14 + high_z_lum_3 + high_z_lum_15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also determine error values for these luminosities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_z_lum_14GHz_e  = np.zeros_like(high_z_lum_14GHz)\n",
    "high_z_lum_250GHz_e = np.zeros_like(high_z_lum_250)\n",
    "for counter, element in enumerate(high_z_lum_14GHz):\n",
    "    if element == 0: continue\n",
    "    high_z_lum_14GHz_e[counter] = np.abs(element) * high_z_14_e[counter] / high_z_14[counter]\n",
    "for counter, element in enumerate(high_z_lum_250):\n",
    "    if element == 0: continue\n",
    "    high_z_lum_250GHz_e[counter] = np.abs(element) * high_z_250GHz_e[counter] / high_z_250GHz[counter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a filter to plot, when needed, only the sources which  \n",
    "have mm data but not radio observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_250GHz = np.array((high_z_lum_250 > 0) * (high_z_lum_14GHz == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the points we are interested in. Our sample from **Inayoshi et al., 2020** and the  \n",
    "sources from **SDSS+FIRST** with $z>5.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option to display the data is, instead of showing redshift in the  \n",
    "horizontal axis, have the mass of the observed objects.\n",
    "\n",
    "**Inayoshi et al., 2020** use the rest-frame UV magnitude $\\mathrm{M}_{1450}$  \n",
    "to calculate the mass as:\n",
    "\n",
    "$$M = 10^{[-(\\mathrm{M}_{1450} + 3.459) / 2.5]} [\\mathrm{M}_{\\odot}]$$\n",
    "\n",
    "which yields, on average, the published virial mass estimates for those available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create formal arrays from catalogs to merge them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays from `SDSS+FIRST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit_z_sdss          = 5.5\n",
    "limit_z_sdss          = 0.0\n",
    "filter_sdss_z         = np.array(sdss_milli['redshift'][L_14GHz_filter] > limit_z_sdss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_sdss_L          = L_21cm[filter_sdss_z]\n",
    "upper_sdss_L_e        = L_21cm_e[filter_sdss_z]\n",
    "upper_sdss_u_lim      = np.zeros_like(upper_sdss_L, dtype=np.bool)\n",
    "upper_sdss            = sdss_milli[np.array(sdss_milli['redshift'] > limit_z_sdss) * L_14GHz_filter]\n",
    "upper_sdss_ra         = upper_sdss['sdss_ra']\n",
    "upper_sdss_dec        = upper_sdss['sdss_dec']\n",
    "upper_sdss_z          = upper_sdss['redshift']\n",
    "upper_sdss_z_e        = upper_sdss['redshift_err']\n",
    "upper_sdss_f_20cm     = upper_sdss['flux_20_cm']  # mJy\n",
    "upper_sdss_f_20cm_e   = upper_sdss['flux_20_cm'] / upper_sdss['snr_20_cm']  # mJy\n",
    "upper_sdss_f_250GHz   = np.zeros_like(upper_sdss_L)\n",
    "upper_sdss_f_250GHz_e = np.zeros_like(upper_sdss_L)\n",
    "upper_sdss_L_250GHz   = np.zeros_like(upper_sdss_L)\n",
    "upper_sdss_L_250GHz_e = np.zeros_like(upper_sdss_L)\n",
    "upper_sdss_mass_1450  = np.zeros_like(upper_sdss_L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays from the `COSMOS` Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upper_cosmos_L          = int_14GHz_cosmos[np.array(cosmos_data['cosm_z'] > limit_z_sdss)]  # Calculated by them\n",
    "upper_cosmos_L          = L_14GHz_cosmos[np.array(redshift_cosmos > limit_z_sdss)]  # Calculated with our spectral index\n",
    "upper_cosmos_L_e        = L_14GHz_cosmos_e[np.array(redshift_cosmos > limit_z_sdss)]\n",
    "upper_cosmos_u_lim      = np.zeros_like(upper_cosmos_L, dtype=np.bool)\n",
    "upper_cosmos            = cosmos_data[np.array(redshift_cosmos > limit_z_sdss)]\n",
    "upper_cosmos_ra         = upper_cosmos['cosm_ra']\n",
    "upper_cosmos_dec        = upper_cosmos['cosm_dec']\n",
    "upper_cosmos_z          = upper_cosmos['cosm_z']\n",
    "upper_cosmos_z_e        = np.zeros_like(upper_cosmos_L)\n",
    "upper_cosmos_f_20cm     = cosmos_14GHz_flux[np.array(redshift_cosmos > limit_z_sdss)]  # mJy\n",
    "upper_cosmos_f_20cm_e   = cosmos_14GHz_flux_e[np.array(redshift_cosmos > limit_z_sdss)]  # mJy\n",
    "upper_cosmos_f_250GHz   = np.zeros_like(upper_cosmos_L)\n",
    "upper_cosmos_f_250GHz_e = np.zeros_like(upper_cosmos_L)\n",
    "upper_cosmos_L_250GHz   = np.zeros_like(upper_cosmos_L)\n",
    "upper_cosmos_L_250GHz_e = np.zeros_like(upper_cosmos_L)\n",
    "upper_cosmos_mass_1450  = np.zeros_like(upper_cosmos_L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays from `radio`catalog (`Heasarc`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_sdss_ra         = radio_sources['sdss_ra']\n",
    "radio_sdss_dec        = radio_sources['sdss_dec']\n",
    "radio_sdss_u_lim      = np.zeros_like(L_21cm_radio, dtype=np.bool)\n",
    "radio_sdss_z          = radio_sources['sdss_z']\n",
    "radio_sdss_z_e        = radio_sources['sdss_z_err']\n",
    "radio_sdss_f_20cm     = radio_sources['radio_flux']\n",
    "radio_sdss_f_20cm_e   = radio_sources['radio_flux_err']\n",
    "radio_sdss_f_250GHz   = np.zeros_like(L_21cm_radio)\n",
    "radio_sdss_f_250GHz_e = np.zeros_like(L_21cm_radio)\n",
    "radio_sdss_L_250GHz   = np.zeros_like(L_21cm_radio)\n",
    "radio_sdss_L_250GHz_e = np.zeros_like(L_21cm_radio)\n",
    "radio_sdss_mass_1450  = np.zeros_like(L_21cm_radio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging `SDSS+FIRST` and `radio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_large_sample_ra         = np.append(upper_sdss_ra,         radio_sdss_ra)          # deg\n",
    "radio_large_sample_dec        = np.append(upper_sdss_dec,        radio_sdss_dec)         # deg\n",
    "radio_large_sample_L          = np.append(upper_sdss_L,          L_21cm_radio)           # W/Hz\n",
    "radio_large_sample_L_e        = np.append(upper_sdss_L_e,        L_21cm_radio_e)         # W/Hz\n",
    "radio_large_sample_u_lim      = np.append(upper_sdss_u_lim,      radio_sdss_u_lim)\n",
    "radio_large_sample_L_250GHz   = np.append(upper_sdss_L_250GHz,   radio_sdss_L_250GHz)    # W/Hz\n",
    "radio_large_sample_L_250GHz_e = np.append(upper_sdss_L_250GHz_e, radio_sdss_L_250GHz_e)  # W/Hz\n",
    "radio_large_sample_f_20cm     = np.append(upper_sdss_f_20cm,     radio_sdss_f_20cm)      # mJy\n",
    "radio_large_sample_f_20cm_e   = np.append(upper_sdss_f_20cm_e,   radio_sdss_f_20cm_e)    # mJy\n",
    "radio_large_sample_f_250GHz   = np.append(upper_sdss_f_250GHz,   radio_sdss_f_250GHz)    # mJy\n",
    "radio_large_sample_f_250GHz_e = np.append(upper_sdss_f_250GHz_e, radio_sdss_f_250GHz_e)  # mJy\n",
    "radio_large_sample_z          = np.append(upper_sdss_z,          radio_sdss_z)\n",
    "radio_large_sample_z_e        = np.append(upper_sdss_z_e,        radio_sdss_z_e)\n",
    "radio_large_sample_mass_1450  = np.append(upper_sdss_mass_1450,  radio_sdss_mass_1450)   # M_sun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging `SDSS+FIRST`+`radio` and the catalog from **Inayoshi et al., 2020**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_sample_ra        = np.append(radio_large_sample_ra,         high_z_ra[np.array(high_z_lum_14GHz>0)])               # deg\n",
    "medium_sample_dec       = np.append(radio_large_sample_dec,        high_z_dec[np.array(high_z_lum_14GHz>0)])              # deg\n",
    "medium_sample_L         = np.append(radio_large_sample_L,          high_z_lum_14GHz[np.array(high_z_lum_14GHz>0)])        # W/Hz\n",
    "medium_sample_L_e       = np.append(radio_large_sample_L_e,        high_z_lum_14GHz_e[np.array(high_z_lum_14GHz>0)])      # W/Hz\n",
    "medium_sample_u_lim     = np.append(radio_large_sample_u_lim,      high_z_up_lim[np.array(high_z_lum_14GHz>0)])\n",
    "medium_sample_L_250     = np.append(radio_large_sample_L_250GHz,   high_z_lum_250[np.array(high_z_lum_14GHz>0)])          # W/Hz\n",
    "medium_sample_L_250_e   = np.append(radio_large_sample_L_250GHz_e, high_z_lum_250GHz_e[np.array(high_z_lum_14GHz>0)])     # W/Hz\n",
    "medium_sample_f20cm     = np.append(radio_large_sample_f_20cm,     high_z_14[np.array(high_z_lum_14GHz>0)] * 1e-3)        # mJy\n",
    "medium_sample_f20cm_e   = np.append(radio_large_sample_f_20cm_e,   high_z_14_e[np.array(high_z_lum_14GHz>0)] * 1e-3)      # mJy\n",
    "medium_sample_f250GHz   = np.append(radio_large_sample_f_250GHz,   high_z_250GHz[np.array(high_z_lum_14GHz>0)] * 1e-3)    # mJy\n",
    "medium_sample_f250GHz_e = np.append(radio_large_sample_f_250GHz_e, high_z_250GHz_e[np.array(high_z_lum_14GHz>0)] * 1e-3)  # mJy\n",
    "medium_sample_z         = np.append(radio_large_sample_z,          high_z_zs[np.array(high_z_lum_14GHz>0)])\n",
    "medium_sample_z_e       = np.append(radio_large_sample_z_e,        high_z_zs_e[np.array(high_z_lum_14GHz>0)])\n",
    "medium_sample_mass_1450 = np.append(radio_large_sample_mass_1450,  high_z_mass_1450[np.array(high_z_lum_14GHz>0)])        # M_sun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging `SDSS+FIRST`+`radio`+**Inayoshi et al., 2020** and `COSMOS` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_sample_ra        = np.append(medium_sample_ra,        upper_cosmos_ra)          # deg\n",
    "large_sample_dec       = np.append(medium_sample_dec,       upper_cosmos_dec)         # deg\n",
    "large_sample_L         = np.append(medium_sample_L,         upper_cosmos_L)           # W/Hz\n",
    "large_sample_L_e       = np.append(medium_sample_L_e,       upper_cosmos_L_e)         # W/Hz\n",
    "large_sample_u_lim     = np.append(medium_sample_u_lim,     upper_cosmos_u_lim)\n",
    "large_sample_L_250     = np.append(medium_sample_L_250,     upper_cosmos_L_250GHz)    # W/Hz\n",
    "large_sample_L_250_e   = np.append(medium_sample_L_250_e,   upper_cosmos_L_250GHz_e)  # W/Hz\n",
    "large_sample_f20cm     = np.append(medium_sample_f20cm,     upper_cosmos_f_20cm)      # mJy\n",
    "large_sample_f20cm_e   = np.append(medium_sample_f20cm_e,   upper_cosmos_f_20cm_e)    # mJy\n",
    "large_sample_f250GHz   = np.append(medium_sample_f250GHz,   upper_cosmos_f_250GHz)    # mJy\n",
    "large_sample_f250GHz_e = np.append(medium_sample_f250GHz_e, upper_cosmos_f_250GHz_e)  # mJy\n",
    "large_sample_z         = np.append(medium_sample_z,         upper_cosmos_z)\n",
    "large_sample_z_e       = np.append(medium_sample_z_e,       upper_cosmos_z_e)\n",
    "large_sample_mass_1450 = np.append(medium_sample_mass_1450, upper_cosmos_mass_1450)   # M_sun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we also want to obtain more properties from the selected  \n",
    "sources (**Inayoshi et al., 2020** + **SDSS+FIRST**). We will use `astroquery` to  \n",
    "obtain information from `simbad`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we obtain the names and coordinates of our sources to query them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_radio_sdss    = np.append(radio_sources['sdss_name'], upper_sdss['name'])\n",
    "medium_sample_names = np.append(names_radio_sdss, upper_cosmos['cosm_name'])\n",
    "large_sample_names  = np.append(medium_sample_names, high_z_names[np.array(high_z_lum_14GHz>0)])\n",
    "large_sample_coords = SkyCoord(ra=large_sample_ra, dec=large_sample_dec, unit=(u.deg, u.deg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we correct/change the names for those which can be looked up in `Simbad` and `Ned`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_to_replace = ['SDSS J125507.61+463126.5', 'SDSS J160558.86+474300.1', 'SDSS J111036.32+481752.3', 'SDSS J163033.89+401209.6']\n",
    "new_to_replace = ['NVSS J125507+463128', '2MASS J16055893+4742596', 'NVSS J111036+481753', 'SDSS J163033.90+401209.6']\n",
    "\n",
    "for old, new in zip(old_to_replace, new_to_replace):\n",
    "    index = np.where(large_sample_names == old)\n",
    "    large_sample_names[index] = new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can query the database to obtain the desired data.  In this point,  \n",
    "we also add more columns to be queried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_simbad_flag               = False\n",
    "load_simbad_flag                = False\n",
    "create_simbad_inayoshi_flag     = False\n",
    "read_simbad_inayoshi_flag       = True\n",
    "query_ned_names_flag            = False\n",
    "query_ned_photometry_flag       = False\n",
    "order_ned_photometry_flag       = False\n",
    "create_phot_bands_list_flag     = False\n",
    "load_phot_bands_list_flag       = False\n",
    "create_simbad_inayoshi_ned_flag = False\n",
    "read_simbad_inayoshi_ned_flag   = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    customSimbad   = Simbad()\n",
    "    initial_fields = customSimbad.get_votable_fields()\n",
    "\n",
    "    if 'coordinates' in initial_fields:\n",
    "        customSimbad.remove_votable_fields('coordinates')\n",
    "        customSimbad.add_votable_fields('ra(d)', 'dec(d)')\n",
    "    if 'z_value' not in initial_fields:\n",
    "        customSimbad.add_votable_fields('z_value')\n",
    "    for band in ['B','V','R','I','J','K']:\n",
    "        if f'fluxdata({band})' not in initial_fields:\n",
    "            customSimbad.add_votable_fields(f'flux({band})', f'flux_error({band})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All sources but those from `COSMOS` catalog have meaningful (for `simbad`) names.  \n",
    "Thus, separate queries will be executed. And, to standardize results, queries  \n",
    "will only be based on coordinates (not names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    limit_set            = int(np.floor(np.shape(medium_sample_L)[0]/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    result_table_simbad  = Table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_table = customSimbad.query_objects(large_sample_names)\n",
    "# customSimbad.TIMEOUT = 240\n",
    "\n",
    "# result_table_job_a   = customSimbad.query_objects(large_sample_names[:limit_set])\n",
    "\n",
    "# result_table_job_b   = customSimbad.query_objects(large_sample_names[limit_set:(limit_set*2)])\n",
    "\n",
    "# result_table_job_c   = customSimbad.query_objects(large_sample_names[(limit_set*2):(limit_set*3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    query_error = 0\n",
    "    final_error = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    with progressbar.ProgressBar(min_value=0, max_value=(limit_set - 1)) as bar:\n",
    "        for index, coord in enumerate(large_sample_coords[:limit_set]):\n",
    "            temp_table      = Table()\n",
    "            try:\n",
    "                temp_table      = customSimbad.query_region(coord, radius=2.0*u.arcsec)\n",
    "                if len(temp_table) == 0:\n",
    "                    temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                    temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "            except:\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                #print(f'Error with element {(index)} of the sample')\n",
    "                query_error += 1\n",
    "            try:\n",
    "                if temp_table['RA_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['RA_d'].data, dtype=result_table_simbad['RA_d'].dtype, unit=result_table_simbad['RA_d'].unit, format=result_table_simbad['RA_d'].format, description=result_table_simbad['RA_d'].description)\n",
    "                    temp_table['RA_d']  = replacement_col\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "            except:\n",
    "                print(f'Error with element {(index)} of the sample')\n",
    "                final_error += 1\n",
    "            bar.update(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    with progressbar.ProgressBar(min_value=limit_set, max_value=(limit_set*2 - 1)) as bar:\n",
    "        for index, coord in enumerate(large_sample_coords[limit_set:(limit_set*2)]):\n",
    "            temp_table      = Table()\n",
    "            try:\n",
    "                temp_table      = customSimbad.query_region(coord, radius=2.0*u.arcsec)\n",
    "                if len(temp_table) == 0:\n",
    "                    temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                    temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "            except:\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                #print(f'Error with element {(index + limit_set)} of the sample')\n",
    "                query_error += 1\n",
    "            try:\n",
    "                if temp_table['RA_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['RA_d'].data, dtype=result_table_simbad['RA_d'].dtype, unit=result_table_simbad['RA_d'].unit, format=result_table_simbad['RA_d'].format, description=result_table_simbad['RA_d'].description)\n",
    "                    temp_table['RA_d']  = replacement_col\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "            except:\n",
    "                print(f'Error with element {(index + limit_set)} of the sample')\n",
    "                final_error += 1\n",
    "            bar.update(index + limit_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    with progressbar.ProgressBar(min_value=(limit_set*2), max_value=(limit_set*3)) as bar:\n",
    "        for index, coord in enumerate(large_sample_coords[(limit_set*2):(limit_set*3 + 1)]):\n",
    "            temp_table      = Table()\n",
    "            try:\n",
    "                temp_table      = customSimbad.query_region(coord, radius=2.0*u.arcsec)\n",
    "                if len(temp_table) == 0:\n",
    "                    temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                    temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "            except:\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                # print(f'Error with element {(index + limit_set*2)} of the sample')\n",
    "                query_error += 1\n",
    "            try:\n",
    "                if temp_table['RA_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['RA_d'].data, dtype=result_table_simbad['RA_d'].dtype, unit=result_table_simbad['RA_d'].unit, format=result_table_simbad['RA_d'].format, description=result_table_simbad['RA_d'].description)\n",
    "                    temp_table['RA_d']  = replacement_col\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "            except:\n",
    "                print(f'Error with element {(index + limit_set*2)} of the sample')\n",
    "                final_error += 1\n",
    "            bar.update(index + limit_set*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query for `COSMOS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    limit_set_cosmos = int(np.floor(np.shape(upper_cosmos_L)[0]/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    with progressbar.ProgressBar(min_value=(limit_set*3 + 1), max_value=(limit_set*3 + limit_set_cosmos)) as bar:\n",
    "        for index, coord in enumerate(large_sample_coords[(limit_set*3 + 1):(limit_set*3 + 1 + limit_set_cosmos)]):\n",
    "            temp_table      = Table()\n",
    "            try:\n",
    "                temp_table      = customSimbad.query_region(coord, radius=3.0*u.arcsec)\n",
    "                if len(temp_table) == 0:\n",
    "                    temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                    temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "            except:\n",
    "                # print(f'Error with element {(index + limit_set*3 + 1)} of the sample')\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                query_error += 1\n",
    "            try:\n",
    "                if temp_table['RA_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['RA_d'].data, dtype=result_table_simbad['RA_d'].dtype, unit=result_table_simbad['RA_d'].unit, format=result_table_simbad['RA_d'].format, description=result_table_simbad['RA_d'].description)\n",
    "                    temp_table['RA_d']  = replacement_col\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "            except:\n",
    "                print(f'Error with element {(index + limit_set*3 + 1)} of the sample')\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad query for radio objects'\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "                final_error += 1\n",
    "            bar.update(index + limit_set*3 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    with progressbar.ProgressBar(min_value=(limit_set*3 + 1 + limit_set_cosmos), max_value=(limit_set*3 + limit_set_cosmos*2)) as bar:\n",
    "        for index, coord in enumerate(large_sample_coords[(limit_set*3 + 1 + limit_set_cosmos):(limit_set*3 + 1 + limit_set_cosmos*2)]):\n",
    "            temp_table      = Table()\n",
    "            try:\n",
    "                temp_table      = customSimbad.query_region(coord, radius=3.0*u.arcsec)\n",
    "                if len(temp_table) == 0:\n",
    "                    temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                    temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "            except:\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                # print(f'Error with element {(index + limit_set*3 + 1 + limit_set_cosmos)} of the sample')\n",
    "                query_error += 1\n",
    "            try:\n",
    "                if temp_table['RA_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['RA_d'].data, dtype=result_table_simbad['RA_d'].dtype, unit=result_table_simbad['RA_d'].unit, format=result_table_simbad['RA_d'].format, description=result_table_simbad['RA_d'].description)\n",
    "                    temp_table['RA_d']  = replacement_col\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "            except:\n",
    "                print(f'Error with element {(index + limit_set*3 + 1 + limit_set_cosmos)} of the sample')\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad query for radio objects'\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "                final_error += 1\n",
    "            bar.update(index + limit_set*3 + 1 + limit_set_cosmos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    with progressbar.ProgressBar(min_value=(limit_set*3 + 1 + limit_set_cosmos*2), max_value=np.shape(large_sample_coords)[0]) as bar:\n",
    "        for index, coord in enumerate(large_sample_coords[(limit_set*3 + 1 + limit_set_cosmos*2):]):\n",
    "            temp_table      = Table()\n",
    "            try:\n",
    "                temp_table      = customSimbad.query_region(coord, radius=3.0*u.arcsec)\n",
    "                if len(temp_table) == 0:\n",
    "                    temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                    temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "            except:\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                # print(f'Error with element {(index + limit_set*3 + 1 + limit_set_cosmos*2)} of the sample')\n",
    "                query_error += 1\n",
    "            try:\n",
    "                if temp_table['RA_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['RA_d'].data, dtype=result_table_simbad['RA_d'].dtype, unit=result_table_simbad['RA_d'].unit, format=result_table_simbad['RA_d'].format, description=result_table_simbad['RA_d'].description)\n",
    "                    temp_table['RA_d']  = replacement_col\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "            except:\n",
    "                print(f'Error with element {(index + limit_set*3 + 1 + limit_set_cosmos*2)} of the sample')\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype, result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad query for radio objects'\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "                final_error += 1\n",
    "            bar.update(index + limit_set*3 + 1 + limit_set_cosmos*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the query to a file for future executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    result_table_simbad.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    result_table_simbad.write(cat_path + 'large_cat_simbad_query.csv', format='ascii.csv', overwrite=True, serialize_method='data_mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to create a copy of table to save it as `fits` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    copy_simbad_to_write = result_table_simbad\n",
    "    str_id = copy_simbad_to_write['MAIN_ID'].astype('str')\n",
    "    copy_simbad_to_write.replace_column('MAIN_ID', str_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    copy_simbad_to_write.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    copy_simbad_to_write.write(cat_path + 'large_cat_simbad_query.fits', format='fits', overwrite=True, serialize_method='data_mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save running time, we can load the data from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if load_simbad_flag:\n",
    "#    result_table_simbad = Table.read(cat_path + 'large_cat_simbad_query.csv', format='ascii.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_simbad_flag:\n",
    "    result_table_simbad     = Table.read(cat_path + 'large_cat_simbad_query.fits');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_table_simbad.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point, we merge the data from the query to `simbad` with the  \n",
    "values from this notebook (**Inayoshi et al., 2020** and **SDSS+FIRST**).  \n",
    "In order to do this, we convert the data into `astropy` columns, and then  \n",
    "into `astropy` tables. They will be ready to be exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_flag:\n",
    "    coords_simbad_inayoshi = SkyCoord(result_table_simbad['RA_d'], result_table_simbad['DEC_d'], unit=u.deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_flag:\n",
    "    column_cat_index    = MaskedColumn(np.arange(np.shape(large_sample_z)[0]), name='INDEX', dtype='int', description='Index number')\n",
    "    column_cat_name     = MaskedColumn(large_sample_names, name='CAT_NAME', dtype='str', description='Name used in this catalog', mask=np.array(large_sample_names == ''))\n",
    "    column_cat_coords   = MaskedColumn(coords_simbad_inayoshi.to_string('decimal'), name='COORD', dtype='str', description='Merged Coordinates', mask=np.array(coords_simbad_inayoshi.to_string('decimal') == ''))\n",
    "    column_z_own        = MaskedColumn(large_sample_z, name='Z_OWN', unit='', description='Redshift from Inayoshi+2020 or SDSS+FIRST', fill_value=np.nan, mask=np.array(large_sample_z == 0))\n",
    "    column_z_own_err    = MaskedColumn(large_sample_z_e, name='Z_OWN_ERR', unit='', description='Redshift error from Inayoshi+2020 or SDSS+FIRST', fill_value=np.nan, mask=np.array(large_sample_z_e == 0))\n",
    "    column_L_14GHz      = MaskedColumn(large_sample_L, name='L_20CM', unit='W/Hz', description='Luminosity in 1.4 GHz', fill_value=np.nan, mask=np.array(large_sample_L == 0))\n",
    "    column_L_14GHz_err  = MaskedColumn(large_sample_L_e, name='L_20CM_ERR', unit='W/Hz', description='Luminosity error in 1.4 GHz', fill_value=np.nan, mask=np.array(large_sample_L_e == 0))\n",
    "    column_L_14GHz_up   = MaskedColumn(large_sample_u_lim, name='L_20CM_UP_LIM', dtype='bool', description='True if L_20CM is upper limit')\n",
    "    column_L_250GHz     = MaskedColumn(large_sample_L_250, name='L_250GHZ', unit='W/Hz', description='Luminosity in 250 GHz', fill_value=np.nan, mask=np.array(large_sample_L_250 == 0))\n",
    "    column_L_250GHz_err = MaskedColumn(large_sample_L_250_e, name='L_250GHZ_ERR', unit='W/Hz', description='Luminosity error in 250 GHz', fill_value=np.nan, mask=np.array(large_sample_L_250_e == 0))\n",
    "    column_f_20cm       = MaskedColumn(large_sample_f20cm, name='F_20CM', unit='mJy', description='Flux in 20 cm', fill_value=np.nan, mask=np.array(large_sample_f20cm == 0))\n",
    "    column_f_20cm_err   = MaskedColumn(large_sample_f20cm_e, name='F_20CM_ERR', unit='mJy', description='Flux error in 20 cm', fill_value=np.nan, mask=np.array(large_sample_f20cm_e == 0))\n",
    "    column_f_250GHz     = MaskedColumn(large_sample_f250GHz, name='F_250GHZ', unit='mJy', description='Flux in 250 GHz', fill_value=np.nan, mask=np.array(large_sample_f250GHz == 0))\n",
    "    column_f_250GHz_err = MaskedColumn(large_sample_f250GHz_e, name='F_250GHZ_ERR', unit='mJy', description='Flux error in 250 GHz', fill_value=np.nan, mask=np.array(large_sample_f250GHz_e == 0))\n",
    "    column_mass_1450    = MaskedColumn(large_sample_mass_1450, name='MASS_1450', unit='Msun', description='Mass from mag_1450 (UV)', fill_value=np.nan, mask=np.array(large_sample_mass_1450 == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_flag:\n",
    "    result_table_simbad.add_columns([column_cat_index, column_cat_name, column_cat_coords, column_z_own, column_z_own_err, column_L_14GHz, column_L_14GHz_err, column_L_14GHz_up, column_L_250GHz, column_L_250GHz_err, column_f_20cm, column_f_20cm_err, column_f_250GHz, column_f_250GHz_err, column_mass_1450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_flag:\n",
    "    str_id = result_table_simbad['MAIN_ID'].astype('str')\n",
    "    result_table_simbad.replace_column('MAIN_ID', str_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_table = result_table_simbad.filled(fill_value=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the table into a file. It can be `.fits`, `.votable`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy_table.write('high_z_qsos.ecsv', format='ascii.ecsv', overwrite=True, serialize_method='data_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_flag:\n",
    "    result_table_simbad.write(cat_path + 'large_cat_simbad_query_inayoshi.fits', format='fits', overwrite=True, serialize_method='data_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_flag:\n",
    "    result_table_simbad.write(cat_path + 'large_cat_simbad_query_inayoshi.csv', format='ascii.csv', overwrite=True, serialize_method='data_mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save running time, we can load the data from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if read_simbad_inayoshi_flag:\n",
    "#    result_table_simbad = Table.read(cat_path + 'large_cat_simbad_query_inayoshi.csv', format='ascii.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_simbad_inayoshi_flag:\n",
    "    result_table_simbad = Table.read(cat_path + 'large_cat_simbad_query_inayoshi.fits');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Table length=18023>\n",
       "     name      dtype    unit   format                    description                   n_bad\n",
       "------------- ------- ------- -------- ----------------------------------------------- -----\n",
       "      MAIN_ID bytes41                                    Main identifier for an object   606\n",
       "         RA_d float64     deg {:11.8f}                                 Right ascension     0\n",
       "        DEC_d float64     deg {:12.8f}                                     Declination     0\n",
       "      Z_VALUE float64         {:16.7f}                                        Redshift  1644\n",
       "       FLUX_B float32     mag                                              Magnitude B  8961\n",
       " FLUX_ERROR_B float32         {:12.3f}                                      flux error 11228\n",
       "       FLUX_V float32     mag                                              Magnitude V  8857\n",
       " FLUX_ERROR_V float32         {:12.3f}                                      flux error 11189\n",
       "       FLUX_R float32     mag                                              Magnitude R 12483\n",
       " FLUX_ERROR_R float32         {:12.3f}                                      flux error 12782\n",
       "       FLUX_I float32     mag                                              Magnitude I 12167\n",
       " FLUX_ERROR_I float32         {:12.3f}                                      flux error 12862\n",
       "       FLUX_J float32     mag                                              Magnitude J 11448\n",
       " FLUX_ERROR_J float32         {:12.3f}                                      flux error 11500\n",
       "       FLUX_K float32     mag                                              Magnitude K  9921\n",
       " FLUX_ERROR_K float32         {:12.3f}                                      flux error 10033\n",
       "        INDEX   int64                                                     Index number     0\n",
       "     CAT_NAME bytes34                                        Name used in this catalog     0\n",
       "        COORD bytes19                                               Merged Coordinates     0\n",
       "        Z_OWN float64                        Redshift from Inayoshi+2020 or SDSS+FIRST     0\n",
       "    Z_OWN_ERR float64                  Redshift error from Inayoshi+2020 or SDSS+FIRST  9373\n",
       "       L_20CM float64  W / Hz                                    Luminosity in 1.4 GHz    66\n",
       "   L_20CM_ERR float64  W / Hz                              Luminosity error in 1.4 GHz    66\n",
       "L_20CM_UP_LIM    bool                                    True if L_20CM is upper limit     0\n",
       "     L_250GHZ float64  W / Hz                                    Luminosity in 250 GHz 18010\n",
       " L_250GHZ_ERR float64  W / Hz                              Luminosity error in 250 GHz 18010\n",
       "       F_20CM float64     mJy                                            Flux in 20 cm    66\n",
       "   F_20CM_ERR float64     mJy                                      Flux error in 20 cm    66\n",
       "     F_250GHZ float64     mJy                                          Flux in 250 GHz 18010\n",
       " F_250GHZ_ERR float64     mJy                                    Flux error in 250 GHz 18010\n",
       "    MASS_1450 float64 solMass                                  Mass from mag_1450 (UV) 18006"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_table_simbad.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the objects of the table in other catalogs and services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from astroquery.heasarc import Heasarc\n",
    "#Heasarc.query_mission_cols(mission='radio')\n",
    "#tabb = Heasarc.query(large_sample_names, mission='radio', timeout=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "customNed        = Ned()\n",
    "fields_to_remove = ['No.', 'Photometry Measurement', 'Uncertainty', 'Units', 'Significance', 'Published frequency', 'Frequency Mode', 'Coordinates Targeted', 'Spatial Mode', 'Qualifiers', 'Comments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying sources with name in `Ned`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty_counter = 0\n",
    "# res_tab       = {}\n",
    "# for name in large_sample_names:\n",
    "#     try:\n",
    "#         res_tab[name] = customNed.get_table(name, output_table_format=1)\n",
    "#         res_tab[name].remove_columns(fields_to_remove)\n",
    "#     except:\n",
    "#         res_tab[name] = Table()\n",
    "#         empty_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can do it with coordinates.  \n",
    "\n",
    "First, we query the coordinates. If we found something,  \n",
    "we use the name of the source to obtain it photometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_simbad_inayoshi = SkyCoord(result_table_simbad['RA_d'], result_table_simbad['DEC_d'], unit=u.deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_tab_name_counter  = 0\n",
    "empty_tab_photo_counter = 0\n",
    "error_tab_name_counter  = 0\n",
    "error_tab_photo_counter = 0\n",
    "ned_tables              = {}\n",
    "ned_info                = {}\n",
    "ned_names               = []\n",
    "#ned_names               = np.array([''  for x in np.arange(np.shape(large_sample_names)[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_ned_names_flag:\n",
    "    with progressbar.ProgressBar(min_value=0, max_value=np.shape(coords_simbad_inayoshi)[0]) as bar:\n",
    "        for index, coord in enumerate(coords_simbad_inayoshi):\n",
    "            try:\n",
    "                init_table            = customNed.query_region(coords_simbad_inayoshi[index], radius=3.0*u.arcsec)\n",
    "                if len(init_table) == 0:\n",
    "                    init_table        = Table(names=('Object Name', 'RA', 'DEC'), dtype=('str', 'float', 'float'), masked=True)\n",
    "                    init_table.add_row(('No Name', coords_simbad_inayoshi[index].ra.deg, coords_simbad_inayoshi[index].dec.deg), mask=[True, False, False])\n",
    "                    ned_info[index]   = init_table\n",
    "                    ned_names.append('No Name')\n",
    "                    empty_tab_name_counter += 1\n",
    "                    continue\n",
    "                init_table.remove_columns(['Magnitude and Filter', 'Positions', 'Diameter Points'])\n",
    "                ned_info[index]   = init_table\n",
    "                used_source_idx   = np.nanargmin(init_table['Separation'])  # Index of element with lowest separation from coords\n",
    "                init_name         = init_table['Object Name'][used_source_idx]\n",
    "                # ned_names[index]  = init_name\n",
    "                ned_names.append(init_name)\n",
    "            except:\n",
    "                init_table        = Table(names=('Object Name', 'RA', 'DEC'), dtype=('str', 'float', 'float'), masked=True)\n",
    "                init_table.add_row(('No Name', coords_simbad_inayoshi[index].ra.deg, coords_simbad_inayoshi[index].dec.deg), mask=[True, False, False])\n",
    "                ned_info[index]   = init_table\n",
    "                ned_names.append('No Name')\n",
    "                error_tab_name_counter += 1\n",
    "            bar.update(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ned_names = np.array(ned_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ned_redshifts = []\n",
    "for key in ned_info:\n",
    "    ned_redshifts.append(ned_info[index]['Redshift'])\n",
    "ned_redshifts = np.array(ned_redshifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_count = 0\n",
    "indices_non   = []\n",
    "for index, name in enumerate(ned_names):\n",
    "    if name != 'No Name':\n",
    "        counter_count += 1\n",
    "for index, name in enumerate(ned_names):\n",
    "    if name == 'No Name':\n",
    "        indices_non.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_ned_photometry_flag:\n",
    "    with progressbar.ProgressBar(min_value=0, max_value=np.shape(ned_names)[0]) as bar:\n",
    "        for index, name in enumerate(ned_names):\n",
    "            try:\n",
    "                if name == 'No Name':\n",
    "                    phot_table        = Table(names=('Observed Passband',), dtype=('str',), masked=True)\n",
    "                    phot_table.add_row(('No Passband',), mask=(True,))\n",
    "                    ned_tables[index] = phot_table\n",
    "                    empty_tab_photo_counter += 1\n",
    "                    continue\n",
    "                phot_table            = customNed.get_table(name, table='photometry', output_table_format=3)\n",
    "                phot_table.remove_columns(fields_to_remove)\n",
    "                ned_tables[index]     = phot_table\n",
    "            except:\n",
    "                phot_table            = Table(names=('Observed Passband',), dtype=('str',), masked=True)\n",
    "                phot_table.add_row(('No Passband',), mask=(True,))\n",
    "                ned_tables[index]     = phot_table\n",
    "                error_tab_photo_counter += 1\n",
    "            bar.update(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save multiple data tables into one file with `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_flag:\n",
    "    pass_band_name = ned_tables[key]['Observed Passband'].astype('str')\n",
    "    ned_tables[key].replace_column('Observed Passband', pass_band_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gather the names and frequencies of all columns present in tables from `Ned`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_phot_bands_list_flag:\n",
    "    band_names_array = []\n",
    "    \n",
    "    with progressbar.ProgressBar(min_value=0, max_value=np.shape(ned_names)[0]) as bar:\n",
    "        for key in ned_tables:\n",
    "            if np.shape(ned_tables[key].colnames)[0] < 2:\n",
    "                bar.update(key)\n",
    "                continue\n",
    "            pass_band_name = ned_tables[key]['Observed Passband'].astype('str')\n",
    "            ned_tables[key].replace_column('Observed Passband', pass_band_name)\n",
    "            temp_table    = ned_tables[key]['Observed Passband', 'Frequency'].as_array().data\n",
    "            for temp_pair in temp_table:\n",
    "                band_names_array.append([str(temp_pair[0]), f'\\t{temp_pair[1]:.4e}'])\n",
    "            bar.update(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique rows (name + frequency pair) are retrieved from the previous list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_phot_bands_list_flag:\n",
    "    unique_rows_band_names_array = np.unique(band_names_array, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the size of this new array. It represents the number of  \n",
    "unique passband configurations gathered from querying `Ned`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_phot_bands_list_flag:\n",
    "    print(np.shape(unique_rows_band_names_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving this list into a plain text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_phot_bands_list_flag:\n",
    "    np.savetxt(cat_path + 'all_ned_band_names.txt', unique_rows_band_names_array, fmt='%s %s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new file can also be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_phot_bands_list_flag:\n",
    "    unique_rows_band_names_array = np.genfromtxt(cat_path + 'all_ned_band_names.txt', delimiter='\\t', dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a unique table from all the individual photometry tables obtained after  \n",
    "querying `Ned`. We discard measurements which have already been reported (for each individual source)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leave uncertainty values as string to retain information about possible upper/lower limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "if order_ned_photometry_flag:\n",
    "    limit_set_ned = int(np.floor(np.shape(ned_names)[0]/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if order_ned_photometry_flag:\n",
    "    temp_table_ned_photo             = Table()\n",
    "    chunk_size                       = 300  # Number of elements to calculate before dumping results to external table\n",
    "    with progressbar.ProgressBar(min_value=0, max_value=np.shape(ned_names)[0]) as bar:\n",
    "        for index, source_name in enumerate(ned_names):  # Some names will be. 'No Name'\n",
    "            band_names_str           = []\n",
    "            column_names_str         = []\n",
    "            band_frequencies         = []\n",
    "            measure_names            = ned_tables[index].colnames[1:]\n",
    "            # init_table = Table(names=('MAIN_ID',), dtype=('str',), masked=True)\n",
    "            cord_str                 = coords_simbad_inayoshi[index].to_string('decimal')\n",
    "            init_table               = Table(data=np.array([index, cord_str, source_name]), names=('INDEX', 'COORD', 'MAIN_ID'), dtype=('int', 'str', 'str'), masked=True)\n",
    "            if source_name == 'No Name':\n",
    "                # init_table = Table(('No Name',), names=('MAIN_ID',), dtype=('str',), masked=True)\n",
    "                # init_table.add_row(('No Name',), mask=(True,)) # Mask values in the last step instead\n",
    "                if index == 0 or (index % chunk_size == 1 and index > 1):\n",
    "                    init_table_large     = Table(init_table)\n",
    "                    bar.update(index)\n",
    "                    continue\n",
    "                init_table_large     = vstack([init_table_large, init_table])\n",
    "                if index % chunk_size == 0 and index > 0:\n",
    "                    temp_table_ned_photo = vstack([temp_table_ned_photo, init_table_large])\n",
    "                    # result_table_simbad_copy = join(result_table_simbad_copy, init_table_large, keys='COORD', join_type='outer')\n",
    "                bar.update(index)\n",
    "                continue\n",
    "            if len(measure_names) == 0:\n",
    "                # init_table = Table(('No Name',), names=('MAIN_ID',), dtype=('str',), masked=True)\n",
    "                # init_table.add_row((source_name,), mask=(True,))\n",
    "                if index == 0 or (index % chunk_size == 1 and index > 1):\n",
    "                    init_table_large     = Table(init_table)\n",
    "                    bar.update(index)\n",
    "                    continue\n",
    "                init_table_large     = vstack([init_table_large, init_table])\n",
    "                if index % chunk_size == 0 and index > 0:\n",
    "                    temp_table_ned_photo = vstack([temp_table_ned_photo, init_table_large])\n",
    "                    # result_table_simbad_copy = join(result_table_simbad_copy, init_table_large, keys='COORD', join_type='outer')\n",
    "                bar.update(index)\n",
    "                continue\n",
    "            # init_table = Table((str(source_name),), names=('MAIN_ID',), dtype=('str',), masked=True)\n",
    "            # init_table.add_row((str(source_name),))\n",
    "            for row in ned_tables[index]:\n",
    "                band_name_str        = re.sub(r' \\(.*', '', str(row['Observed Passband'].decode('utf-8')))  # Eliminate differences\n",
    "                if str(band_name_str) not in band_names_str and row['Frequency'] not in band_frequencies:\n",
    "                    band_frequencies.append(row['Frequency'])\n",
    "                    band_names_str.append(str(band_name_str))\n",
    "                    column_name_flux = 'Flux Density ' + band_name_str\n",
    "                    column_name_err  = 'NED Uncertainty ' + band_name_str\n",
    "                    if column_name_flux not in column_names_str:\n",
    "                        column_names_str.append(column_name_flux)\n",
    "                        column_flux  = MaskedColumn(row['Flux Density'], name=column_name_flux, unit=ned_tables[index]['Flux Density'].unit, dtype='float')\n",
    "                        column_err   = MaskedColumn(row['NED Uncertainty'], name=column_name_err, dtype='str')\n",
    "                        init_table.add_columns((column_flux, column_err))\n",
    "            #init_table.remove_column('MAIN_ID')\n",
    "            if index == 0 or (index % chunk_size == 1 and index > 1):\n",
    "                init_table_large     = Table(init_table)\n",
    "                bar.update(index)\n",
    "                continue\n",
    "            init_table_large         = vstack([init_table_large, init_table])\n",
    "            if index % chunk_size == 0 and index > 0:\n",
    "                temp_table_ned_photo = vstack([temp_table_ned_photo, init_table_large])\n",
    "                # result_table_simbad_copy = join(result_table_simbad_copy, init_table_large, keys='COORD', join_type='outer')\n",
    "            bar.update(index)\n",
    "    temp_table_ned_photo = vstack([temp_table_ned_photo, init_table_large])\n",
    "    # result_table_simbad_copy = join(result_table_simbad_copy, init_table_large, keys='COORD', join_type='outer')\n",
    "    #result_table_simbad_copy         = Table(result_table_simbad)\n",
    "    #result_table_simbad_copy.add_column(coords_simbad_inayoshi.to_string('decimal'), name='COORD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new merged column shows measurements for the following number of bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "if order_ned_photometry_flag:\n",
    "    len(temp_table_ned_photo.colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data completeness, we mask entries for `MAIN_ID` and `COORD` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    column_coord = MaskedColumn(temp_table_ned_photo['COORD'], name='COORD', mask=np.array(temp_table_ned_photo['COORD'] == ''))\n",
    "    column_id = MaskedColumn(temp_table_ned_photo['MAIN_ID'], name='MAIN_ID', description='Main identifier for an object', mask=np.array(temp_table_ned_photo['MAIN_ID'] == 'No Name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    temp_table_ned_photo.replace_column('COORD', column_coord)\n",
    "    temp_table_ned_photo.replace_column('MAIN_ID', column_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform `astropy` tables into `pandas` data frames for further calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    df_ned    = temp_table_ned_photo.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    df_simbad = result_table_simbad.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this point, we can merge both `simbad` and `Ned` tables into a larger one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    merged_ned_simbad = pd.merge(df_simbad, df_ned, on='INDEX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_ned_simbad.columns[0:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save this new table into a `HDF5`-format file for future access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    merged_ned_simbad.to_hdf(cat_path + 'large_cat_simbad_inayoshi_ned.h5', 'df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, we can read the table from an external file to avoid extra running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_simbad_inayoshi_ned_flag:\n",
    "    power_test = pd.read_hdf(cat_path + 'large_cat_simbad_inayoshi_ned.h5', 'df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this `pandas` data frame can be transformed into an `astropy` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# power_table = Table.from_pandas(power_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
