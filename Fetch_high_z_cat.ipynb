{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to gather data from $z > 5.5$ QSOs.\n",
    "\n",
    "We want to obtain data from several surveys and catalogs in order to  \n",
    "manipulate them and try to obtain meaningful correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the packages to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the first line working, you need  \n",
    "to run the following lines:\n",
    "\n",
    "```bash\n",
    " conda install nodejs\n",
    " pip install ipympl\n",
    " pip install --upgrade jupyterlab\n",
    " jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    " jupyter labextension install jupyter-matplotlib\n",
    " jupyter nbextension enable --py widgetsnbextension\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Static plots\n",
    "#%matplotlib ipympl\n",
    "# Interactive plots\n",
    "import numpy as np\n",
    "# import matplotlib.cm as cm\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.colors as mcolors\n",
    "# import matplotlib.patheffects as mpe\n",
    "# import matplotlib.patheffects as path_effects\n",
    "# from matplotlib.ticker import FormatStrFormatter\n",
    "# from matplotlib.patches import Polygon\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy.table import Column\n",
    "from astropy.table import MaskedColumn\n",
    "from astropy.table import hstack\n",
    "from astropy.table import vstack\n",
    "from astropy.table import join\n",
    "from astropy.wcs import WCS\n",
    "from astropy import units as u\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astroquery.simbad import Simbad\n",
    "from astroquery.ned import Ned\n",
    "import getpass\n",
    "import progressbar\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the spectral index $\\alpha$ from different sources  \n",
    "to be used in the luminosity calculations (K-correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_first = 0.5  # From FIRST data (Bornancini+2010)\n",
    "alpha_RG    = 1.0  # For radio galaxies (Verkhodanov & Khabibullina, 2010)\n",
    "alpha_alex  = 0.8  # Star-forming galaxies (Alexander+2003)\n",
    "alpha_smol  = 0.7  # Mean value from VLA-COSMOS 3GHz sample (Smolčić et al. 2017)\n",
    "alpha_butl  = 0.75  # From Butler et al., 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose one of the spectral indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_used  = alpha_butl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the cosmological properties to calculate luminosity distances and other quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmo       = FlatLambdaCDM(H0=70, Om0=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lum_from_flux(flux, redshift):  # Flux in mJy\n",
    "    lum_distance = cosmo.luminosity_distance(redshift).to(u.m).value  # in m\n",
    "    luminosity   = 4 * np.pi * lum_distance**2 * flux * 1e-3  * 1e-26 * (1 + redshift)**(alpha_used - 1)  # in W/Hz\n",
    "    return luminosity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One function, to derive luminosity distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def luminosity_distance(z_i, H0=70., WM=0.3, WV=0.7):\n",
    "    z_i   = np.array([z_i], dtype='float64').flatten()\n",
    "    c     = 299792.458 # velocity of light in km/sec\n",
    "    h     = H0 / 100.\n",
    "    WR    = 4.165E-5 / (h * h)   # includes 3 massless neutrino species, T0 = 2.72528\n",
    "    WK    = 1 - WM - WR - WV\n",
    "    azs   = 1.0 / (1 + z_i)\n",
    "    DTT   = 0.0\n",
    "    DCMR  = 0.0\n",
    "    # do integral over a=1/(1+z) from az to 1 in n steps, midpoint rule\n",
    "    n     = 1000  # number of points in integrals\n",
    "    DL_Mpcs = np.zeros_like(z_i)\n",
    "    for count, az in enumerate(azs):\n",
    "        a     = az + (1 - az) * (np.arange(0, n) + 0.5) / n\n",
    "        adot  = np.sqrt(WK + (WM / a) + (WR / (a * a)) + (WV * a * a))\n",
    "        for i in range(n):\n",
    "            # a    = az + (1 - az) * (i + 0.5) / n\n",
    "            # adot = math.sqrt(WK + (WM / a) + (WR / (a * a)) + (WV * a * a))\n",
    "            DTT  = DTT + 1. / adot[i]\n",
    "            DCMR = DCMR + 1. / (a[i] * adot[i])\n",
    "        DTT   = (1. - az) * DTT / n\n",
    "        DCMR  = (1. - az) * DCMR / n\n",
    "        # tangential comoving distance\n",
    "        ratio = 1.00\n",
    "        x     = np.sqrt(abs(WK)) * DCMR\n",
    "        if x > 0.1:\n",
    "            if WK > 0:\n",
    "                ratio =  0.5 * (np.exp(x) - np.exp(-x)) / x \n",
    "            else:\n",
    "                ratio = np.sin(x) / x\n",
    "        else:\n",
    "            y = x * x\n",
    "        if WK < 0: y = -y\n",
    "        ratio  = 1. + y / 6. + y * y / 120.\n",
    "        DCMT   = ratio * DCMR\n",
    "        DA     = az * DCMT\n",
    "        DL     = DA / (az * az)\n",
    "        DL_Mpc = (c / H0) * DL\n",
    "        DL_Mpcs[count] = DL_Mpc\n",
    "    return DL_Mpcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step, reading our data.  \n",
    "Most of the data files have been created using Topcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine  = getpass.getuser()\n",
    "# cat_path = '/home/' + machine + '/Documentos/Data/'\n",
    "cat_path = ''  # relative path to the same directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Read data from FIRST+MILLIQUAS catalogs cross-matched.~~  \n",
    "Read data from FIRST (from its version `14dec17`) crossmatched with  \n",
    "SDSS Quasar Catalog (`SDSSDR14Q_v4_4`).\n",
    "\n",
    "**FIRST + (SDSSQ)**   \n",
    "\n",
    "Redshift values have been retrieved from the column `pipeline_redshift` in SDSS DR12  \n",
    "The procedure to obtain these values is explained in **Bolton+2012**  \n",
    "\n",
    "We also select sources with have explicitely data in both SDSS Quasar Catalog and  \n",
    "in the FIRST survey (`first_match_flag = 1`).  \n",
    "\n",
    "In order to get only the best redshift values, we select sources with  \n",
    "`pipeline_redshift_flag = 0`.\n",
    "\n",
    "Thus, we can use 11238 objects from the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdss_milli = Table.read(cat_path + 'tables_matches_milli_sdss_apr.fits')\n",
    "# sdss_milli = Table.read(cat_path + 'milliquas_sdssquasar_jun2020.fits')\n",
    "\n",
    "# L_14GHz_filter = np.array((sdss_milli['flux_20_cm'] > 0.0) * (sdss_milli['first_offset'] < 1.0)) ;  # sdss + milliquasar\n",
    "\n",
    "# sdss_milli = Table.read(cat_path + 'first_14dec17_DR14Q_v4_4_zwarn_0.fits')  # SDSSQDR14+FIRST17Dec14\n",
    "sdss_milli = Table.read(cat_path + 'first_14dec17_DR16Q_v4_zwarn_0.fits')  # SDSSQDR16+FIRST17Dec14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdss_milli.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load data from the VLA-COSMOS 3 GHz Large Project (**Smolčić et al. 2017**),  \n",
    "which has been correlated with the sources from the VLA-COSMOS Large Project 1.4-GHz Source Catalog (**Schinnerer et al., 2007**).\n",
    "\n",
    "Thus, we have only considered sources with observed (not estimated) $1.4$ GHz fluxes. That is, with valid  \n",
    "values in the table column `flux_20cm`.\n",
    "\n",
    "We can, also, separate sources which have an AGN X-ray counterpart\n",
    "with the flag `cosm_xray_flag`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: VerifyWarning: It is strongly recommended that column names contain only upper and lower-case ASCII letters, digits, or underscores for maximum compatibility with other software (got '---'). [astropy.io.fits.column]\n"
     ]
    }
   ],
   "source": [
    "# cosmos_data         = Table.read(cat_path + 'vla_xmmcosmos_smolcic.fits')\n",
    "cosmos_data         = Table.read(cat_path + 'vlacosxoid_vlacos3gh_xmmcosmos_ago2020.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want, also, to add $z > 6$ QSOs from the list in  \n",
    "Table 3 in the review of **Inayoshi, Visbal, and Haiman, 2020**.  \n",
    "Six of them have $z > 7$\n",
    "\n",
    "Not all of them have $1.4$ GHz measurements. Others have  \n",
    "measurements in different frequencies which can be translated  \n",
    "into the desired frequency using, for instance, the relation  \n",
    "from **Butler et al., 2018**:\n",
    "\n",
    "$$S_{a} = S_{b} \\times (\\frac{\\nu_{b}}{\\nu_{a}})^{\\alpha}$$  \n",
    "\n",
    "We load the data from these sources. Fluxes from different frequencies than $1.4$ GHz are translated to the needed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_z_file       = 'high_z_qso_props_extended.tsv'  # 'high_z_qso_props.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_z_ra         = np.char.replace(np.loadtxt(cat_path + high_z_file, usecols=[1],\\\n",
    "#                                                dtype='str', delimiter='\\t'), ',', '.').astype(np.float)\n",
    "# high_z_dec        = np.char.replace(np.loadtxt(cat_path + high_z_file, usecols=[2],\\\n",
    "#                                                dtype='str', delimiter='\\t'), ',', '.').astype(np.float)\n",
    "# high_z_zs         = np.char.replace(np.loadtxt(cat_path + high_z_file, usecols=[3],\\\n",
    "#                                                dtype='str', delimiter='\\t'), ',', '.').astype(np.float)\n",
    "# high_z_zs_e       = np.char.replace(np.loadtxt(cat_path + high_z_file, usecols=[4],\\\n",
    "#                                                dtype='str', delimiter='\\t'), ',', '.').astype(np.float)\n",
    "# high_z_f_14GHz    = np.char.replace(np.loadtxt(cat_path + high_z_file, usecols=[6],\\\n",
    "#                                                dtype='str', delimiter='\\t'), ',', '.').astype(np.float)\n",
    "# high_z_f_14GHz_e  = np.char.replace(np.loadtxt(cat_path + high_z_file, usecols=[7],\\\n",
    "#                                                dtype='str', delimiter='\\t'), ',', '.').astype(np.float)\n",
    "# high_z_f_3GHz     = np.char.replace(np.loadtxt(cat_path + high_z_file, usecols=[8],\\\n",
    "#                                                dtype='str', delimiter='\\t'), ',', '.').astype(np.float)\n",
    "# high_z_f_3GHz_e   = np.char.replace(np.loadtxt(cat_path + high_z_file, usecols=[9],\\\n",
    "#                                                dtype='str', delimiter='\\t'), ',', '.').astype(np.float)\n",
    "# high_z_f_15GHz    = np.char.replace(np.loadtxt(cat_path + high_z_file, usecols=[10],\\\n",
    "#                                                dtype='str', delimiter='\\t'), ',', '.').astype(np.float)\n",
    "# high_z_f_15GHz_e  = np.char.replace(np.loadtxt(cat_path + high_z_file, usecols=[11],\\\n",
    "#                                                dtype='str', delimiter='\\t'), ',', '.').astype(np.float)\n",
    "# high_z_f_250GHz   = np.char.replace(np.loadtxt(cat_path + high_z_file, usecols=[12],\\\n",
    "#                                                dtype='str', delimiter='\\t'), ',', '.').astype(np.float)\n",
    "# high_z_f_250GHz_e = np.char.replace(np.loadtxt(cat_path + high_z_file, usecols=[13],\\\n",
    "#                                                dtype='str', delimiter='\\t'), ',', '.').astype(np.float)\n",
    "# high_z_mass_1450  = np.char.replace(np.loadtxt(cat_path + high_z_file, usecols=[14],\\\n",
    "#                                                dtype='str', delimiter='\\t'), ',', '.').astype(np.float)\n",
    "# high_z_f_6cm      = np.zeros_like(high_z_zs)  # Keep empty\n",
    "# high_z_f_6cm_e    = np.zeros_like(high_z_zs)  # Keep empty\n",
    "# high_z_xmm_flux   = np.zeros_like(high_z_zs)\n",
    "# high_z_xmm_flux_e = np.zeros_like(high_z_zs)\n",
    "# high_z_names      = np.char.strip(np.loadtxt(cat_path + high_z_file, usecols=[0], dtype='str',\\\n",
    "#                                delimiter='\\t'))\n",
    "# high_z_names_alt  = np.char.strip(np.loadtxt(cat_path + high_z_file, usecols=[16], dtype='str',\\\n",
    "#                                delimiter='\\t'))\n",
    "# high_z_refs       = np.loadtxt(cat_path + high_z_file, usecols=[15], dtype='str',\\\n",
    "#                                delimiter='\\t', encoding='utf-8')  # Article references for data\n",
    "# # Limiting 1.4GHz flux from survey (mJy)\n",
    "# high_z_flx_lim    = np.char.replace(np.loadtxt(cat_path + high_z_file, usecols=[3],\\\n",
    "#                                                dtype='str', delimiter='\\t'), ',', '.').astype(np.float) * 1e-3\n",
    "# high_z_lum_d      = luminosity_distance(high_z_zs) * 3.086e22  # in m\n",
    "# high_z_up_lim     = np.array([val == '<' for val in np.loadtxt(cat_path + high_z_file,\\\n",
    "#                                                                usecols=[5], dtype='str', delimiter='\\t')])\n",
    "# # Flag to know origin of sources (0: SDSS, 1: COSMOS, 2: RADIO, 3: Stripe82, 4:Inayoshi)\n",
    "# high_z_origin_flg = np.full_like(high_z_zs, fill_value=4, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(high_z_zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_z_sources    = Table.read(cat_path + 'milliquas_inayoshi_sdssqdr16_z55.fits')  # z >= 5.5 Inayoshi+Milliquas+SDSSQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cumulate values into one array except 250GHz data.  \n",
    "Millimetre luminosities will be used separately since we cannot be completely  \n",
    "sure that they represent, fully, non-thermal emission (from AGN) and not dust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_z_14   = high_z_14GHz + high_z_3GHz + high_z_15GHz\n",
    "# high_z_14_e = high_z_14GHz_e + high_z_3GHz_e + high_z_15GHz_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complement the dataset, we also load four $z > 5.5$ sources which  \n",
    "come from the `radio` catalog in the `Heasarc` database.  \n",
    "We queried the objects which have $1.4$ GHz observations that  \n",
    "are within a $2.5$ arcsec of an object from the `SDSS QUASAR DR12`  \n",
    "catalog. We discard the sources that are already included in the `FIRST`  \n",
    "catalog, to avoid repetition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enlarge the size of the sample, we can also query  \n",
    "the same sample, but extending the redshift range to all positive  \n",
    "values. We can include, too, the exclusion of ***low quality*** redshift_values (or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# radio_sources        = Table.read(cat_path + 'radio_cat_sdss_z_55_spec.fits')       # high redshift\n",
    "# radio_sources        = Table.read(cat_path + 'radio_cat_sdss_z_all_spec.fits')      # all redshift, high z quality\n",
    "radio_sources        = Table.read(cat_path + 'radio_sdssquasar_jun2020.fits')      # all redshift, high z quality\n",
    "# radio_sources        = Table.read(cat_path + 'radio_cat_sdss_z_all_all_spec.fits')  # all redshift, all z quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also include sources from the field `Stripe82` with `VLA` observations.  \n",
    "The data has been obtained from **Hodge et al., 2011** (`VLASS821P4`; VLA SDSS Stripe 82 Survey 1.4-GHz Source Catalog).  \n",
    "These sources have been crossmatched with the `SDSSQUASAR` catalog to obtain their redshift value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stripe82_sources     = Table.read(cat_path + 'vlass821p4_sdssquasar_jun2020.fits')\n",
    "# stripe82_sources     = Table.read(cat_path + 'sdsss82cxo_sdsss82xmm_unique_jun2020.fits')\n",
    "stripe82_sources     = Table.read(cat_path + 'J_ApJ_817_172_chandra_xmmao10_xmmao13_unique_vlass821p4_sdssquasar.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate luminosities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future, we will calculate luminosities (in W/Hz) for different datasets  \n",
    "using the expression\n",
    "\n",
    "$$L_{1.4\\mathrm{GHz}} = 4 \\pi \\mathrm{d}^{2}_{L} f_{1.4\\mathrm{GHz}} (1 + z)^{\\alpha - 1}$$\n",
    "\n",
    "which comes from Alexander et al. 2003\n",
    "\n",
    "We can also obtain that luminosity from the flux in $3$ GHz as\n",
    "\n",
    "$$L_{1.4\\mathrm{GHz}} = 4 \\pi \\mathrm{d}^{2}_{L} {(\\frac{3}{1.4})}^{\\alpha} f_{3\\mathrm{GHz}} (1 + z)^{\\alpha - 1}$$\n",
    "\n",
    "This expression comes from Delhaize et al. 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining values from tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripe82_sources['RAdeg'][~(stripe82_sources['RAdeg'] > -999)] = 0.0\n",
    "stripe82_sources['sdss_ra'][~(stripe82_sources['sdss_ra'] > -.999)] = 0.0\n",
    "stripe82_sources['ra'] = stripe82_sources['RAdeg'].astype(np.float) + stripe82_sources['sdss_ra'].astype(np.float)\n",
    "\n",
    "stripe82_sources['DEdeg'][~(stripe82_sources['DEdeg'] > -999.)] = 0.0\n",
    "stripe82_sources['sdss_dec'][~(stripe82_sources['sdss_dec'] > -999.)] = 0.0\n",
    "stripe82_sources['dec'] = stripe82_sources['DEdeg'].astype(np.float) + stripe82_sources['sdss_dec'].astype(np.float)\n",
    "\n",
    "stripe82_sources['zsp'][~(stripe82_sources['zsp'] > 0)] = 0.0\n",
    "stripe82_sources['sdss_z'][~(stripe82_sources['sdss_z'] > 0)] = 0.0\n",
    "stripe82_sources['redshift'] = stripe82_sources['zsp'].astype(np.float) + stripe82_sources['sdss_z'].astype(np.float)\n",
    "stripe82_sources['redshift'][(stripe82_sources['redshift'] <= 0)] = None\n",
    "stripe82_sources['F1.4GHz'][~(stripe82_sources['F1.4GHz'] > 0)] = 0.0\n",
    "stripe82_sources['strp82_20cm_flux'][~(stripe82_sources['strp82_20cm_flux'] > 0)] = 0.0\n",
    "stripe82_sources['flux_20_cm'] = stripe82_sources['F1.4GHz'].astype(np.float) +\\\n",
    "stripe82_sources['strp82_20cm_flux'].astype(np.float)\n",
    "stripe82_sources['e_F1.4GHz'][~(stripe82_sources['e_F1.4GHz'] > 0)] = 0.0\n",
    "stripe82_sources['strp82_20cm_flux_err'][~(stripe82_sources['strp82_20cm_flux_err'] > 0)] = 0.0\n",
    "stripe82_sources['flux_20_cm_error'] = stripe82_sources['e_F1.4GHz'].astype(np.float) +\\\n",
    "                                        stripe82_sources['strp82_20cm_flux_err'].astype(np.float)\n",
    "\n",
    "# Do not use following filter. Include all sources\n",
    "# filter_strp82    = np.array((stripe82_sources['flux_20_cm'] > 0) & (stripe82_sources['redshift'] > 0))\n",
    "filter_strp82    = np.array(stripe82_sources['redshift'] > 0)\n",
    "stripe82_sources = stripe82_sources[filter_strp82]\n",
    "\n",
    "# L_14GHz_strp82   = lum_from_flux(stripe82_sources['strp82_20cm_flux'], stripe82_sources['sdss_z'])\n",
    "# L_14GHz_strp82_e = np.abs(L_14GHz_strp82) * stripe82_sources['strp82_20cm_flux_err'] /\\\n",
    "# stripe82_sources['strp82_20cm_flux'];\n",
    "# L_14GHz_strp82_e[~np.isfinite(L_14GHz_strp82_e)] = 0.0  # Repair (make zero) error values\n",
    "L_14GHz_strp82   = lum_from_flux(stripe82_sources['flux_20_cm'], stripe82_sources['redshift'])\n",
    "L_14GHz_strp82_e = np.abs(L_14GHz_strp82) * stripe82_sources['flux_20_cm_error'] / stripe82_sources['flux_20_cm'];\n",
    "# L_14GHz_strp82_e[~np.isfinite(L_14GHz_strp82_e)] = 0.0  # Repair (make zero) error values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the points we are interested in. Our sample from **Inayoshi et al., 2020** and the  \n",
    "sources from **SDSS+FIRST** with $z>5.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option to display the data is, instead of showing redshift in the  \n",
    "horizontal axis, have the mass of the observed objects.\n",
    "\n",
    "**Inayoshi et al., 2020** use the rest-frame UV magnitude $\\mathrm{M}_{1450}$  \n",
    "to calculate the mass as:\n",
    "\n",
    "$$M = 10^{[-(\\mathrm{M}_{1450} + 3.459) / 2.5]} [\\mathrm{M}_{\\odot}]$$\n",
    "\n",
    "which yields, on average, the published virial mass estimates for those available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create formal arrays from catalogs to merge them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays from `SDSS+FIRST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit_z_sdss_up       = 5.5\n",
    "limit_z_sdss          = 0.0\n",
    "filter_sdss_z         = np.array(sdss_milli['Z'] > limit_z_sdss)\n",
    "xmm_freq              = 1.47e18  # Hz. Frequency for 0.2-12 keV observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_sdss            = sdss_milli[np.array(sdss_milli['Z'] > limit_z_sdss)]\n",
    "upper_sdss_ra         = upper_sdss['RA_1']\n",
    "upper_sdss_dec        = upper_sdss['DEC_1']\n",
    "# upper_sdss_z          = upper_sdss['Z_PIPE']\n",
    "# upper_sdss_z_e        = upper_sdss['Z_PIPE_ERR']\n",
    "upper_sdss_z          = upper_sdss['Z']\n",
    "upper_sdss_z_e        = np.zeros_like(upper_sdss_z)\n",
    "upper_sdss_f_20cm     = upper_sdss['FPEAK']  # mJy\n",
    "upper_sdss_f_20cm_e   = upper_sdss['RMS']  # mJy\n",
    "upper_sdss_u_lim      = np.zeros_like(upper_sdss_z, dtype=np.bool)\n",
    "upper_sdss_f_6cm      = np.zeros_like(upper_sdss_z)\n",
    "upper_sdss_f_6cm_e    = np.zeros_like(upper_sdss_z)\n",
    "upper_sdss_f_3GHz     = np.zeros_like(upper_sdss_z)\n",
    "upper_sdss_f_3GHz_e   = np.zeros_like(upper_sdss_z)\n",
    "upper_sdss_f_15GHz    = np.zeros_like(upper_sdss_z)  # 1.5 GHz\n",
    "upper_sdss_f_15GHz_e  = np.zeros_like(upper_sdss_z)\n",
    "upper_sdss_f_250GHz   = np.zeros_like(upper_sdss_z)\n",
    "upper_sdss_f_250GHz_e = np.zeros_like(upper_sdss_z)\n",
    "upper_sdss_xmm_flux   = upper_sdss['XMM_TOTAL_FLUX'] / xmm_freq * 1e23 * 1e3     # erg cm-2 s-1. Convert to mJy\n",
    "upper_sdss_xmm_flux_e = upper_sdss['XMM_TOTAL_FLUX_ERR'] / xmm_freq * 1e23 * 1e3 # erg cm-2 s-1. Convert to mJy\n",
    "upper_sdss_mass_1450  = np.zeros_like(upper_sdss_z)\n",
    "upper_sdss_flx_lim    = np.array([0.150]*np.shape(upper_sdss_z)[0])  # Limiting 1.4GHz flux from survey (mJy)\n",
    "# Flag to know origin of sources (0: SDSS, 1: COSMOS, 2: RADIO, 3: Stripe82, 4:Inayoshi)\n",
    "upper_sdss_origin_flg = np.zeros_like(upper_sdss_z, dtype=np.int)\n",
    "upper_sdss_refs       = np.array(['Lyke+2020,Paris+2018,Helfand+2015'.encode('utf-8')] * np.shape(upper_sdss_z)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13396,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(upper_sdss_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays from the `COSMOS` Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_cosmos            = cosmos_data[np.array(cosmos_data['cosm_z'] > limit_z_sdss)]\n",
    "upper_cosmos_ra         = upper_cosmos['cosm_ra']\n",
    "upper_cosmos_dec        = upper_cosmos['cosm_dec']\n",
    "upper_cosmos_z          = upper_cosmos['cosm_z']\n",
    "upper_cosmos_z_e        = np.zeros_like(upper_cosmos_z)\n",
    "upper_cosmos_f_20cm     = upper_cosmos['flux_20cm']      # mJy\n",
    "upper_cosmos_f_20cm_e   = upper_cosmos['flux_20cm_err']  # mJy\n",
    "upper_cosmos_u_lim      = np.zeros_like(upper_cosmos_z, dtype=np.bool)\n",
    "upper_cosmos_f_3GHz     = upper_cosmos['cosm_f_3ghz']\n",
    "upper_cosmos_f_3GHz_e   = upper_cosmos['cosm_f_3ghz_err']\n",
    "upper_cosmos_f_6cm      = np.zeros_like(upper_cosmos_z)\n",
    "upper_cosmos_f_6cm_e    = np.zeros_like(upper_cosmos_z)\n",
    "upper_cosmos_f_15GHz    = np.zeros_like(upper_cosmos_z)  # 1.5 GHz\n",
    "upper_cosmos_f_15GHz_e  = np.zeros_like(upper_cosmos_z)\n",
    "upper_cosmos_f_250GHz   = np.zeros_like(upper_cosmos_z)\n",
    "upper_cosmos_f_250GHz_e = np.zeros_like(upper_cosmos_z)\n",
    "upper_cosmos_xmm_flux   = upper_cosmos['S.5-10'] * 1e-17 / xmm_freq * 1e23 * 1e3    # 1e-17 W m-2. Convert to mJy\n",
    "upper_cosmos_xmm_flux_e = upper_cosmos['e_S.5-10'] * 1e-17 / xmm_freq * 1e23 * 1e3  # 1e-17 W m-2. Convert to mJy\n",
    "upper_cosmos_mass_1450  = np.zeros_like(upper_cosmos_z)\n",
    "upper_cosmos_flx_lim    = np.array([0.045] * np.shape(upper_cosmos_z)[0])  # Limiting 1.4GHz flux from survey (mJy)\n",
    "# Flag to know origin of sources (0: SDSS, 1: COSMOS, 2: Stripe82, 3:Inayoshi)\n",
    "upper_cosmos_origin_flg = np.ones_like(upper_cosmos_z, dtype=np.int)\n",
    "upper_cosmos_refs       = np.array(['Smolcic+2017,Cappelluti+2009'.encode('utf-8')] * np.shape(upper_cosmos_z)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2433,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(upper_cosmos_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays from `vlass821p4` catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "strp82_ra                     = stripe82_sources['RA']                # in degree\n",
    "strp82_dec                    = stripe82_sources['DEC']               # in degree\n",
    "strp82_u_lim                  = np.zeros_like(strp82_ra, dtype=np.bool)\n",
    "strp82_z                      = stripe82_sources['redshift']\n",
    "strp82_z.mask                 = np.array(strp82_z <= 0)               # Fix format for column\n",
    "strp82_z.fill_value           = np.nan                                # Fix format for column\n",
    "# strp82_z_e           = stripe82_sources['sdss_z_err']\n",
    "strp82_z_e                    = np.zeros_like(strp82_z)               # Spectroscopic redshift\n",
    "strp82_20cm_flux              = stripe82_sources['flux_20_cm']        # in mJy\n",
    "strp82_20cm_flux.mask         = np.array(strp82_20cm_flux <= 0)\n",
    "strp82_20cm_flux.fill_value   = np.nan\n",
    "strp82_20cm_flux_e            = stripe82_sources['flux_20_cm_error']  # in mJy\n",
    "strp82_20cm_flux_e.mask       = np.array(strp82_20cm_flux <= 0)\n",
    "strp82_20cm_flux_e.fill_value = np.nan\n",
    "strp82_f_3GHz                 = np.zeros_like(strp82_z)\n",
    "strp82_f_3GHz_e               = np.zeros_like(strp82_z)\n",
    "strp82_f_6cm                  = np.zeros_like(strp82_z)\n",
    "strp82_f_6cm_e                = np.zeros_like(strp82_z)\n",
    "strp82_f_15GHz                = np.zeros_like(strp82_z)  # 1.5 GHz\n",
    "strp82_f_15GHz_e              = np.zeros_like(strp82_z)\n",
    "strp82_f_250GHz               = np.zeros_like(strp82_z)\n",
    "strp82_f_250GHz_e             = np.zeros_like(strp82_z)\n",
    "\n",
    "stripe82_sources['FFull'][~(stripe82_sources['FFull'] > 0)] = 0.0\n",
    "stripe82_sources['xmm_flux'][~(stripe82_sources['xmm_flux'] > 0)] = 0.0\n",
    "stripe82_sources['fb_flux'] = stripe82_sources['FFull'] * 1e3 * 1e-17 + stripe82_sources['xmm_flux']  # 1e-17 W/m2\n",
    "stripe82_sources['e_FFull'][~(stripe82_sources['e_FFull'] > 0)] = 0.0\n",
    "stripe82_sources['xmm_flux_err'][~(stripe82_sources['xmm_flux_err'] > 0)] = 0.0\n",
    "stripe82_sources['fb_flux_e'] = stripe82_sources['e_FFull'] * 1e3 * 1e-17 + stripe82_sources['xmm_flux_err']\n",
    "\n",
    "strp82_xmm_flux               = stripe82_sources['fb_flux'] / xmm_freq * 1e23 * 1e3  # erg cm-2 s-1. Convert to mJy\n",
    "strp82_xmm_flux.mask          = np.array(strp82_xmm_flux <= 0)\n",
    "strp82_xmm_flux.fill_value    = np.nan\n",
    "strp82_xmm_flux_e             = stripe82_sources['fb_flux_e'] / xmm_freq * 1e23 * 1e3  # erg cm-2 s-1. Convert to mJy\n",
    "strp82_xmm_flux_e.mask        = np.array(strp82_xmm_flux_e <= 0)\n",
    "strp82_xmm_flux_e.fill_value  = np.nan\n",
    "# strp82_spec_index    = stripe82_sources['spec_index']\n",
    "strp82_spec_index   = np.zeros_like(strp82_z)\n",
    "strp82_mass_1450    = np.zeros_like(strp82_z)\n",
    "strp82_flx_lim      = np.array([0.052] * np.shape(strp82_z)[0])  # Limiting 1.4GHz flux from survey (mJy)\n",
    "# Flag to know origin of sources (0: SDSS, 1: COSMOS, 2: Stripe82, 3:Inayoshi)\n",
    "strp82_origin_flg   = np.full_like(strp82_z, fill_value=2, dtype=np.int)\n",
    "strp82_refs         = np.array(['Lyke+2020,Hodge+2011,LaMassa+2016,Paris+2017'.encode('utf-8')] * np.shape(strp82_z)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1838,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(strp82_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays from the Extended **Inayoshi et al., 2020** catalog ($z > 5.5$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_z_ra         = high_z_sources['RA']\n",
    "high_z_dec        = high_z_sources['DEC']\n",
    "high_z_zs         = high_z_sources['Z']\n",
    "high_z_zs_e       = high_z_sources['Z_e']\n",
    "high_z_f_14GHz    = high_z_sources['F20cm'] * 1e-3\n",
    "high_z_f_14GHz_e  = high_z_sources['F20cmE'] * 1e-3\n",
    "filter_yes14      = np.array(high_z_f_14GHz > 0.)\n",
    "filter_good_f     = np.array(high_z_sources['FIRST_FLUX'] > 0.)\n",
    "# Add FIRST measurements where available\n",
    "high_z_f_14GHz[~filter_yes14 & filter_good_f]   = high_z_sources['FIRST_FLUX'][~filter_yes14 & filter_good_f]\n",
    "high_z_f_14GHz_e[~filter_yes14 & filter_good_f] = (high_z_sources['FIRST_FLUX'][~filter_yes14 & filter_good_f]\\\n",
    "                                                   - 0.25) / high_z_sources['FIRST_SNR'][~filter_yes14 & filter_good_f]\n",
    "high_z_f_3GHz     = high_z_sources['F3GHz'] * 1e-3\n",
    "high_z_f_3GHz_e   = high_z_sources['F3GHzE'] * 1e-3\n",
    "high_z_f_15GHz    = high_z_sources['F1.5GHz'] * 1e-3\n",
    "high_z_f_15GHz_e  = high_z_sources['F1.5GHzE'] * 1e-3\n",
    "high_z_f_250GHz   = high_z_sources['F250GHz'] * 1e-3  # mJy\n",
    "high_z_f_250GHz_e = high_z_sources['F250GHzE'] * 1e-3  # mJy\n",
    "high_z_mass_1450  = high_z_sources['BH_MASS']\n",
    "high_z_f_6cm      = np.zeros_like(high_z_zs)  # Keep empty\n",
    "high_z_f_6cm_e    = np.zeros_like(high_z_zs)  # Keep empty\n",
    "high_z_xmm_flux   = high_z_sources['XMM_TOTAL_FLUX'] / xmm_freq * 1e23 * 1e3  # erg cm-2 s-1. Convert to mJy\n",
    "high_z_xmm_flux_e = high_z_sources['XMM_TOTAL_FLUX_ERR'] / xmm_freq * 1e23 * 1e3  # erg cm-2 s-1. Convert to mJy\n",
    "high_z_names      = high_z_sources['Name_2']\n",
    "high_z_names_alt  = high_z_sources['Alt_Name']\n",
    "filter_empty_ref  = np.array(np.char.strip(high_z_sources['REF'].astype(np.str)) == '+')\n",
    "high_z_refs       = np.char.strip(high_z_sources['REF'].astype(np.str))  # Article references for data\n",
    "high_z_refs[filter_empty_ref] = 'Lyke+2020,Paris+2018,Helfand+2015'.encode('utf-8')  # They come from SDSS+FIRST\n",
    "high_z_flx_lim    = high_z_sources['OBS_LIM'] * 1e-3 # Limiting 1.4GHz flux from survey (mJy)\n",
    "high_z_lum_d      = luminosity_distance(high_z_zs) * 3.086e22  # in m\n",
    "high_z_up_lim     = high_z_sources['up20cm']  # Boolean values (True if it is an upper limit)\n",
    "# Flag to know origin of sources (0: SDSS, 1: COSMOS, 2: Stripe82, 3:Inayoshi)\n",
    "high_z_origin_flg = np.full_like(high_z_zs, fill_value=3, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2312,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(high_z_zs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging `SDSS+FIRST` and the extended catalog from **Inayoshi et al., 2020**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_sample_ra         = np.append(upper_sdss_ra,         high_z_ra)          # deg\n",
    "medium_sample_dec        = np.append(upper_sdss_dec,        high_z_dec)         # deg\n",
    "medium_sample_u_lim      = np.append(upper_sdss_u_lim,      high_z_up_lim)\n",
    "medium_sample_f20cm      = np.append(upper_sdss_f_20cm,     high_z_f_14GHz)     # mJy\n",
    "medium_sample_f20cm_e    = np.append(upper_sdss_f_20cm_e,   high_z_f_14GHz_e)   # mJy\n",
    "\n",
    "medium_sample_f_6cm      = np.append(upper_sdss_f_6cm,      high_z_f_6cm)       # mJy\n",
    "medium_sample_f_6cm_e    = np.append(upper_sdss_f_6cm_e,    high_z_f_6cm_e)     # mJy\n",
    "medium_sample_f_3GHz     = np.append(upper_sdss_f_3GHz,     high_z_f_3GHz)      # mJy\n",
    "medium_sample_f_3GHz_e   = np.append(upper_sdss_f_3GHz_e,   high_z_f_3GHz_e)    # mJy\n",
    "medium_sample_f_15GHz    = np.append(upper_sdss_f_15GHz,    high_z_f_15GHz)     # mJy\n",
    "medium_sample_f_15GHz_e  = np.append(upper_sdss_f_15GHz_e,  high_z_f_15GHz_e)   # mJy\n",
    "\n",
    "medium_sample_f250GHz    = np.append(upper_sdss_f_250GHz,   high_z_f_250GHz)    # mJy\n",
    "medium_sample_f250GHz_e  = np.append(upper_sdss_f_250GHz_e, high_z_f_250GHz_e)  # mJy\n",
    "medium_sample_z          = np.append(upper_sdss_z,          high_z_zs)\n",
    "medium_sample_z_e        = np.append(upper_sdss_z_e,        high_z_zs_e)\n",
    "medium_sample_xmm_flux   = np.append(upper_sdss_xmm_flux,   high_z_xmm_flux)    # erg cm-2 s-1\n",
    "medium_sample_xmm_flux_e = np.append(upper_sdss_xmm_flux_e, high_z_xmm_flux_e)  # erg cm-2 s-1\n",
    "medium_sample_mass_1450  = np.append(upper_sdss_mass_1450,  high_z_mass_1450)   # M_sun\n",
    "medium_sample_flx_lim    = np.append(upper_sdss_flx_lim,    high_z_flx_lim)     # mJy\n",
    "medium_sample_origin_flg = np.append(upper_sdss_origin_flg, high_z_origin_flg)\n",
    "medium_sample_refs       = np.append(upper_sdss_refs,       high_z_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15708,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(medium_sample_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging `SDSS+FIRST`+**Inayoshi et al., 2020** and `VLASS821P4` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium82_sample_ra         = np.append(medium_sample_ra,         strp82_ra)                     # deg\n",
    "medium82_sample_dec        = np.append(medium_sample_dec,        strp82_dec)                    # deg\n",
    "medium82_sample_u_lim      = np.append(medium_sample_u_lim,      strp82_u_lim)\n",
    "medium82_sample_f20cm      = np.append(medium_sample_f20cm,      strp82_20cm_flux.filled(0))    # mJy\n",
    "medium82_sample_f20cm_e    = np.append(medium_sample_f20cm_e,    strp82_20cm_flux_e.filled(0))  # mJy\n",
    "\n",
    "medium82_sample_f_6cm      = np.append(medium_sample_f_6cm,      strp82_f_6cm.filled(0))        # mJy\n",
    "medium82_sample_f_6cm_e    = np.append(medium_sample_f_6cm_e,    strp82_f_6cm_e.filled(0))      # mJy\n",
    "medium82_sample_f_3GHz     = np.append(medium_sample_f_3GHz,     strp82_f_3GHz.filled(0))       # mJy\n",
    "medium82_sample_f_3GHz_e   = np.append(medium_sample_f_3GHz_e,   strp82_f_3GHz_e.filled(0))     # mJy\n",
    "medium82_sample_f_15GHz    = np.append(medium_sample_f_15GHz,    strp82_f_15GHz.filled(0))      # mJy\n",
    "medium82_sample_f_15GHz_e  = np.append(medium_sample_f_15GHz_e,  strp82_f_15GHz_e.filled(0))    # mJy\n",
    "\n",
    "medium82_sample_f250GHz    = np.append(medium_sample_f250GHz,    strp82_f_250GHz.filled(0))     # mJy\n",
    "medium82_sample_f250GHz_e  = np.append(medium_sample_f250GHz_e,  strp82_f_250GHz_e.filled(0))   # mJy\n",
    "medium82_sample_z          = np.append(medium_sample_z,          strp82_z.filled(0))\n",
    "medium82_sample_z_e        = np.append(medium_sample_z_e,        strp82_z_e.filled(0))\n",
    "medium82_sample_xmm_flux   = np.append(medium_sample_xmm_flux,   strp82_xmm_flux.filled(0))     # in erg cm-2 s-1\n",
    "medium82_sample_xmm_flux_e = np.append(medium_sample_xmm_flux_e, strp82_xmm_flux_e.filled(0))   # in erg cm-2 s-1\n",
    "medium82_sample_mass_1450  = np.append(medium_sample_mass_1450,  strp82_mass_1450.filled(0))    # M_sun\n",
    "medium82_sample_flx_lim    = np.append(medium_sample_flx_lim,    strp82_flx_lim)                # mJy\n",
    "medium82_sample_origin_flg = np.append(medium_sample_origin_flg, strp82_origin_flg)\n",
    "medium82_sample_refs       = np.append(medium_sample_refs,       strp82_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17546,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(medium82_sample_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging `SDSS+FIRST`+**Inayoshi et al., 2020**+`VLASS821P4` and `COSMOS` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_sample_ra         = np.append(medium82_sample_ra,         upper_cosmos_ra)          # deg\n",
    "large_sample_dec        = np.append(medium82_sample_dec,        upper_cosmos_dec)         # deg\n",
    "large_sample_u_lim      = np.append(medium82_sample_u_lim,      upper_cosmos_u_lim)\n",
    "large_sample_f20cm      = np.append(medium82_sample_f20cm,      upper_cosmos_f_20cm)      # mJy\n",
    "large_sample_f20cm_e    = np.append(medium82_sample_f20cm_e,    upper_cosmos_f_20cm_e)    # mJy\n",
    "\n",
    "large_sample_f_6cm      = np.append(medium82_sample_f_6cm,      upper_cosmos_f_6cm)       # mJy\n",
    "large_sample_f_6cm_e    = np.append(medium82_sample_f_6cm_e,    upper_cosmos_f_6cm_e)     # mJy\n",
    "large_sample_f_3GHz     = np.append(medium82_sample_f_3GHz,     upper_cosmos_f_3GHz)      # mJy\n",
    "large_sample_f_3GHz_e   = np.append(medium82_sample_f_3GHz_e,   upper_cosmos_f_3GHz_e)    # mJy\n",
    "large_sample_f_15GHz    = np.append(medium82_sample_f_15GHz,    upper_cosmos_f_15GHz)     # mJy\n",
    "large_sample_f_15GHz_e  = np.append(medium82_sample_f_15GHz_e,  upper_cosmos_f_15GHz_e)   # mJy\n",
    "\n",
    "large_sample_f250GHz    = np.append(medium82_sample_f250GHz,    upper_cosmos_f_250GHz)    # mJy\n",
    "large_sample_f250GHz_e  = np.append(medium82_sample_f250GHz_e,  upper_cosmos_f_250GHz_e)  # mJy\n",
    "large_sample_z          = np.append(medium82_sample_z,          upper_cosmos_z)\n",
    "large_sample_z_e        = np.append(medium82_sample_z_e,        upper_cosmos_z_e)\n",
    "large_sample_xmm_flux   = np.append(medium82_sample_xmm_flux,   upper_cosmos_xmm_flux)    # in erg cm-2 s-1\n",
    "large_sample_xmm_flux_e = np.append(medium82_sample_xmm_flux_e, upper_cosmos_xmm_flux_e)  # in erg cm-2 s-1\n",
    "large_sample_mass_1450  = np.append(medium82_sample_mass_1450,  upper_cosmos_mass_1450)   # M_sun\n",
    "large_sample_flx_lim    = np.append(medium82_sample_flx_lim,    upper_cosmos_flx_lim)     # mJy\n",
    "large_sample_origin_flg = np.append(medium82_sample_origin_flg, upper_cosmos_origin_flg)\n",
    "large_sample_refs       = np.append(medium82_sample_refs,       upper_cosmos_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19979,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(large_sample_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we also want to obtain more properties from the selected  \n",
    "sources (**Inayoshi et al., 2020** + **SDSS+FIRST**). We will use `astroquery` to  \n",
    "obtain information from `simbad`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we obtain the names and coordinates of our sources to query them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sample_names  = np.append(upper_sdss['SDSS_NAME'], high_z_names)\n",
    "medium_sample_names = np.append(small_sample_names, stripe82_sources['Coord_name'])\n",
    "large_sample_names  = np.append(medium_sample_names, upper_cosmos['cosm_name'])\n",
    "large_sample_coords = SkyCoord(ra=large_sample_ra, dec=large_sample_dec, unit=(u.deg, u.deg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19979,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(large_sample_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `MaskedColumns` (from `Astropy`) using the data just collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    column_cat_index    = MaskedColumn(np.arange(np.shape(large_sample_z)[0]), name='INDEX', dtype='int',\\\n",
    "                                       description='Idx_n')\n",
    "    column_cat_name     = MaskedColumn(large_sample_names.data, name='CAT_NAME', dtype='str', description='Cat_name',\\\n",
    "                                       mask=np.array(large_sample_names.data == ''))\n",
    "    column_cat_ra       = MaskedColumn(large_sample_coords.ra.degree, name='RA', description='RA',\\\n",
    "                                       mask=~np.isfinite(large_sample_coords.ra.degree), fill_value=np.nan)\n",
    "    column_cat_dec      = MaskedColumn(large_sample_coords.dec.degree, name='DEC', description='DEC',\\\n",
    "                                       mask=~np.isfinite(large_sample_coords.dec.degree), fill_value=np.nan)\n",
    "    column_z_own        = MaskedColumn(large_sample_z.data, name='Z_OWN', unit='', description='z', fill_value=np.nan,\\\n",
    "                                       mask=np.array(large_sample_z.data == 0))\n",
    "    column_z_own_err    = MaskedColumn(large_sample_z_e.data, name='Z_OWN_ERR', unit='', description='z_err',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_z_e.data == 0))\n",
    "    column_L_14GHz_up   = MaskedColumn(large_sample_u_lim, name='L_20CM_UP_LIM', dtype='bool', description='L20cmUp')\n",
    "    column_f_20cm       = MaskedColumn(large_sample_f20cm.data, name='F_20CM', unit='mJy', description='20cmF',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_f20cm.data == 0))\n",
    "    column_f_20cm_err   = MaskedColumn(large_sample_f20cm_e.data, name='F_20CM_ERR', unit='mJy', description='20cmFe',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_f20cm_e.data == 0))\n",
    "    column_f_6cm        = MaskedColumn(large_sample_f_6cm.data, name='F_6CM', unit='mJy', description='6cmF',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_f_6cm.data == 0))\n",
    "    column_f_6cm_err    = MaskedColumn(large_sample_f_6cm_e.data, name='F_6CM_ERR', unit='mJy', description='6cmFe',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_f_6cm_e.data == 0))\n",
    "    column_f_3GHz       = MaskedColumn(large_sample_f_3GHz.data, name='F_3GHZ', unit='mJy', description='3GHzF',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_f_3GHz.data == 0))\n",
    "    column_f_3GHz_err   = MaskedColumn(large_sample_f_3GHz_e.data, name='F_3GHZ_ERR', unit='mJy', description='3GHzFe',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_f_3GHz_e.data == 0))\n",
    "    column_f_15GHz      = MaskedColumn(large_sample_f_15GHz.data, name='F_1.5GHZ', unit='mJy', description='1.5GHzF',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_f_15GHz.data == 0))\n",
    "    column_f_15GHz_err  = MaskedColumn(large_sample_f_15GHz_e.data, name='F_1.5GHZ_ERR', unit='mJy', description='1.5GHzFe',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_f_15GHz_e.data == 0))\n",
    "    column_f_xmm        = MaskedColumn(large_sample_xmm_flux.data, name='F_XMM', unit='mJy', description='F_xmm',\\\n",
    "                                       fill_value=np.nan,\\\n",
    "                                       mask=np.array(large_sample_xmm_flux.data == 0 & np.isnan(large_sample_xmm_flux.data)))\n",
    "    column_f_xmm_err    = MaskedColumn(large_sample_xmm_flux_e.data, name='F_XMM_ERR', unit='mJy', description='F_xmme',\\\n",
    "                                       fill_value=np.nan,\\\n",
    "                                       mask=np.array(large_sample_xmm_flux_e.data == 0 & np.isnan(large_sample_xmm_flux_e.data)))\n",
    "    column_f_250GHz     = MaskedColumn(large_sample_f250GHz.data, name='F_250GHZ', unit='mJy', description='250GHzF',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_f250GHz.data == 0))\n",
    "    column_f_250GHz_err = MaskedColumn(large_sample_f250GHz_e.data, name='F_250GHZ_ERR', unit='mJy', description='250GHzFe',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_f250GHz_e.data == 0))\n",
    "    column_mass_1450    = MaskedColumn(large_sample_mass_1450.data, name='MASS_1450', unit='Msun', description='UVmass',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_mass_1450.data == 0))\n",
    "    column_flx_lim      = MaskedColumn(large_sample_flx_lim.data, name='FLX_LIM', unit='mJy', description='flx_lim',\\\n",
    "                                       fill_value=np.nan, mask=np.array(large_sample_flx_lim.data == 0))\n",
    "    column_origin       = MaskedColumn(large_sample_origin_flg.data, name='ORIGIN', unit='', description='origin',\\\n",
    "                                       dtype='int', mask=~np.isfinite(large_sample_origin_flg.data))\n",
    "    column_refs         = MaskedColumn(large_sample_refs.data, name='REFS', unit='', description='Refs',\\\n",
    "                                       dtype=np.str, fill_value=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can query the database to obtain the desired data.  In this point,  \n",
    "we also add more columns to be queried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_simbad_flag               = False\n",
    "load_simbad_flag                = False\n",
    "create_simbad_inayoshi_flag     = False\n",
    "read_simbad_inayoshi_flag       = False\n",
    "query_ned_names_flag            = False\n",
    "query_ned_photometry_flag       = False\n",
    "create_phot_bands_list_flag     = False\n",
    "load_phot_bands_list_flag       = False\n",
    "order_ned_photometry_flag       = False\n",
    "create_simbad_inayoshi_ned_flag = False\n",
    "read_simbad_inayoshi_ned_flag   = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    customSimbad   = Simbad()\n",
    "    initial_fields = customSimbad.get_votable_fields()\n",
    "\n",
    "    if 'coordinates' in initial_fields:\n",
    "        customSimbad.remove_votable_fields('coordinates')\n",
    "        customSimbad.add_votable_fields('ra(d)', 'dec(d)')\n",
    "    if 'z_value' not in initial_fields:\n",
    "        customSimbad.add_votable_fields('z_value')\n",
    "    for band in ['B','V','R','I','J','K']:\n",
    "        if f'fluxdata({band})' not in initial_fields:\n",
    "            customSimbad.add_votable_fields(f'flux({band})', f'flux_error({band})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All sources but those from `COSMOS` catalog have meaningful (for `simbad`) names.  \n",
    "Thus, separate queries will be executed. And, to standardize results, queries  \n",
    "will only be based on coordinates (not names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    limit_set            = int(np.floor(np.shape(medium82_sample_z)[0]/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    result_table_simbad  = Table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_table = customSimbad.query_objects(large_sample_names)\n",
    "# customSimbad.TIMEOUT = 240\n",
    "\n",
    "# result_table_job_a   = customSimbad.query_objects(large_sample_names[:limit_set])\n",
    "\n",
    "# result_table_job_b   = customSimbad.query_objects(large_sample_names[limit_set:(limit_set*2)])\n",
    "\n",
    "# result_table_job_c   = customSimbad.query_objects(large_sample_names[(limit_set*2):(limit_set*3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    query_error = 0\n",
    "    final_error = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query `SDSS+FIRST` (`milliquasar`) + **Inayoshi et al., 2020** + `VLASS821P4` sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    with progressbar.ProgressBar(min_value=0, max_value=(limit_set - 1)) as bar:\n",
    "        for index, coord in enumerate(large_sample_coords[:limit_set]):\n",
    "            temp_table      = Table()\n",
    "            try:\n",
    "                temp_table      = customSimbad.query_region(coord, radius=2.0*u.arcsec)\n",
    "                if len(temp_table) == 0:\n",
    "                    temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype,\\\n",
    "                                                                            result_table_simbad['DEC_d'].info.dtype))\n",
    "                    temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "            except:\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype,\\\n",
    "                                                                        result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                #print(f'Error with element {(index)} of the sample')\n",
    "                query_error += 1\n",
    "            try:\n",
    "                if temp_table['RA_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['RA_d'].data, dtype=result_table_simbad['RA_d'].dtype,\\\n",
    "                                                       unit=result_table_simbad['RA_d'].unit,\\\n",
    "                                                       format=result_table_simbad['RA_d'].format,\\\n",
    "                                                       description=result_table_simbad['RA_d'].description)\n",
    "                    temp_table['RA_d']  = replacement_col\n",
    "                if temp_table['DEC_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['DEC_d'].data, dtype=result_table_simbad['DEC_d'].dtype,\\\n",
    "                                                       unit=result_table_simbad['DEC_d'].unit,\\\n",
    "                                                       format=result_table_simbad['DEC_d'].format,\\\n",
    "                                                       description=result_table_simbad['DEC_d'].description)\n",
    "                    temp_table['DEC_d']  = replacement_col\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "            except:\n",
    "                print(f'Error with element {(index)} of the sample')\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype,\\\n",
    "                                                                        result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "                final_error += 1\n",
    "            bar.update(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    with progressbar.ProgressBar(min_value=limit_set, max_value=(limit_set*2 - 1)) as bar:\n",
    "        for index, coord in enumerate(large_sample_coords[limit_set:(limit_set*2)]):\n",
    "            temp_table      = Table()\n",
    "            try:\n",
    "                temp_table      = customSimbad.query_region(coord, radius=2.0*u.arcsec)\n",
    "                if len(temp_table) == 0:\n",
    "                    temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype,\\\n",
    "                                                                            result_table_simbad['DEC_d'].info.dtype))\n",
    "                    temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "            except:\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype,\\\n",
    "                                                                        result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                #print(f'Error with element {(index + limit_set)} of the sample')\n",
    "                query_error += 1\n",
    "            try:\n",
    "                if temp_table['RA_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['RA_d'].data, dtype=result_table_simbad['RA_d'].dtype,\\\n",
    "                                                       unit=result_table_simbad['RA_d'].unit,\\\n",
    "                                                       format=result_table_simbad['RA_d'].format,\\\n",
    "                                                       description=result_table_simbad['RA_d'].description)\n",
    "                    temp_table['RA_d']  = replacement_col\n",
    "                if temp_table['DEC_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['DEC_d'].data, dtype=result_table_simbad['DEC_d'].dtype,\\\n",
    "                                                       unit=result_table_simbad['DEC_d'].unit,\\\n",
    "                                                       format=result_table_simbad['DEC_d'].format,\\\n",
    "                                                       description=result_table_simbad['DEC_d'].description)\n",
    "                    temp_table['DEC_d']  = replacement_col\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "            except:\n",
    "                print(f'Error with element {(index + limit_set)} of the sample')\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype,\\\n",
    "                                                                        result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "                final_error += 1\n",
    "            bar.update(index + limit_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    with progressbar.ProgressBar(min_value=(limit_set*2), max_value=(limit_set*3)) as bar:\n",
    "        for index, coord in enumerate(large_sample_coords[(limit_set*2):(limit_set*3 + 1)]):\n",
    "            temp_table      = Table()\n",
    "            try:\n",
    "                temp_table      = customSimbad.query_region(coord, radius=2.0*u.arcsec)\n",
    "                if len(temp_table) == 0:\n",
    "                    temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype,\\\n",
    "                                                                            result_table_simbad['DEC_d'].info.dtype))\n",
    "                    temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "            except:\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype,\\\n",
    "                                                                        result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                # print(f'Error with element {(index + limit_set*2)} of the sample')\n",
    "                query_error += 1\n",
    "            try:\n",
    "                if temp_table['RA_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['RA_d'].data, dtype=result_table_simbad['RA_d'].dtype,\\\n",
    "                                                       unit=result_table_simbad['RA_d'].unit,\\\n",
    "                                                       format=result_table_simbad['RA_d'].format,\\\n",
    "                                                       description=result_table_simbad['RA_d'].description)\n",
    "                    temp_table['RA_d']  = replacement_col\n",
    "                if temp_table['DEC_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['DEC_d'].data, dtype=result_table_simbad['DEC_d'].dtype,\\\n",
    "                                                       unit=result_table_simbad['DEC_d'].unit,\\\n",
    "                                                       format=result_table_simbad['DEC_d'].format,\\\n",
    "                                                       description=result_table_simbad['DEC_d'].description)\n",
    "                    temp_table['DEC_d']  = replacement_col\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "            except:\n",
    "                print(f'Error with element {(index + limit_set*2)} of the sample')\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype,\\\n",
    "                                                                        result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "                final_error += 1\n",
    "            bar.update(index + limit_set*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query for `COSMOS` sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    limit_set_cosmos = int(np.floor((np.shape(large_sample_z)[0] - np.shape(medium82_sample_z)[0])/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    with progressbar.ProgressBar(min_value=(limit_set*3 + 1), max_value=(limit_set*3 + limit_set_cosmos)) as bar:\n",
    "        for index, coord in enumerate(large_sample_coords[(limit_set*3 + 1):(limit_set*3 + 1 + limit_set_cosmos)]):\n",
    "            temp_table      = Table()\n",
    "            try:\n",
    "                temp_table      = customSimbad.query_region(coord, radius=2.0*u.arcsec)\n",
    "                if len(temp_table) == 0:\n",
    "                    temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype,\\\n",
    "                                                                            result_table_simbad['DEC_d'].info.dtype))\n",
    "                    temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "            except:\n",
    "                # print(f'Error with element {(index + limit_set*3 + 1)} of the sample')\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype,\\\n",
    "                                                                        result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                query_error += 1\n",
    "            try:\n",
    "                if temp_table['RA_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['RA_d'].data, dtype=result_table_simbad['RA_d'].dtype,\\\n",
    "                                                       unit=result_table_simbad['RA_d'].unit,\\\n",
    "                                                       format=result_table_simbad['RA_d'].format,\\\n",
    "                                                       description=result_table_simbad['RA_d'].description)\n",
    "                    temp_table['RA_d']  = replacement_col\n",
    "                if temp_table['DEC_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['DEC_d'].data, dtype=result_table_simbad['DEC_d'].dtype,\\\n",
    "                                                       unit=result_table_simbad['DEC_d'].unit,\\\n",
    "                                                       format=result_table_simbad['DEC_d'].format,\\\n",
    "                                                       description=result_table_simbad['DEC_d'].description)\n",
    "                    temp_table['DEC_d']  = replacement_col\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "            except:\n",
    "                print(f'Error with element {(index + limit_set*3 + 1)} of the sample')\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype,\\\n",
    "                                                                        result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "                final_error += 1\n",
    "            bar.update(index + limit_set*3 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    with progressbar.ProgressBar(min_value=(limit_set*3 + 1 + limit_set_cosmos), max_value=(limit_set*3 + limit_set_cosmos*2)) as bar:\n",
    "        for index, coord in enumerate(large_sample_coords[(limit_set*3 + 1 + limit_set_cosmos):(limit_set*3 + 1 + limit_set_cosmos*2)]):\n",
    "            temp_table      = Table()\n",
    "            try:\n",
    "                temp_table      = customSimbad.query_region(coord, radius=2.0*u.arcsec)\n",
    "                if len(temp_table) == 0:\n",
    "                    temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype,\\\n",
    "                                                                            result_table_simbad['DEC_d'].info.dtype))\n",
    "                    temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "            except:\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype,\\\n",
    "                                                                        result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                # print(f'Error with element {(index + limit_set*3 + 1 + limit_set_cosmos)} of the sample')\n",
    "                query_error += 1\n",
    "            try:\n",
    "                if temp_table['RA_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['RA_d'].data, dtype=result_table_simbad['RA_d'].dtype,\\\n",
    "                                                       unit=result_table_simbad['RA_d'].unit,\\\n",
    "                                                       format=result_table_simbad['RA_d'].format,\\\n",
    "                                                       description=result_table_simbad['RA_d'].description)\n",
    "                    temp_table['RA_d']  = replacement_col\n",
    "                if temp_table['DEC_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['DEC_d'].data, dtype=result_table_simbad['DEC_d'].dtype,\\\n",
    "                                                       unit=result_table_simbad['DEC_d'].unit,\\\n",
    "                                                       format=result_table_simbad['DEC_d'].format,\\\n",
    "                                                       description=result_table_simbad['DEC_d'].description)\n",
    "                    temp_table['DEC_d']  = replacement_col\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "            except:\n",
    "                print(f'Error with element {(index + limit_set*3 + 1 + limit_set_cosmos)} of the sample')\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype,\\\n",
    "                                                                        result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "                final_error += 1\n",
    "            bar.update(index + limit_set*3 + 1 + limit_set_cosmos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    with progressbar.ProgressBar(min_value=(limit_set*3 + 1 + limit_set_cosmos*2), max_value=np.shape(large_sample_coords)[0]) as bar:\n",
    "        for index, coord in enumerate(large_sample_coords[(limit_set*3 + 1 + limit_set_cosmos*2):]):\n",
    "            temp_table      = Table()\n",
    "            try:\n",
    "                temp_table      = customSimbad.query_region(coord, radius=2.0*u.arcsec)\n",
    "                if len(temp_table) == 0:\n",
    "                    temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype,\\\n",
    "                                                                            result_table_simbad['DEC_d'].info.dtype))\n",
    "                    temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "            except:\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype,\\\n",
    "                                                                        result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                # print(f'Error with element {(index + limit_set*3 + 1 + limit_set_cosmos*2)} of the sample')\n",
    "                query_error += 1\n",
    "            try:\n",
    "                if temp_table['RA_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['RA_d'].data, dtype=result_table_simbad['RA_d'].dtype,\\\n",
    "                                                       unit=result_table_simbad['RA_d'].unit,\\\n",
    "                                                       format=result_table_simbad['RA_d'].format,\\\n",
    "                                                       description=result_table_simbad['RA_d'].description)\n",
    "                    temp_table['RA_d']  = replacement_col\n",
    "                if temp_table['DEC_d'].unit != 'deg':\n",
    "                    replacement_col     = MaskedColumn(temp_table['DEC_d'].data, dtype=result_table_simbad['DEC_d'].dtype,\\\n",
    "                                                       unit=result_table_simbad['DEC_d'].unit,\\\n",
    "                                                       format=result_table_simbad['DEC_d'].format,\\\n",
    "                                                       description=result_table_simbad['DEC_d'].description)\n",
    "                    temp_table['DEC_d']  = replacement_col\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "            except:\n",
    "                print(f'Error with element {(index + limit_set*3 + 1 + limit_set_cosmos*2)} of the sample')\n",
    "                temp_table      = Table(names=('RA_d', 'DEC_d'), dtype=(result_table_simbad['RA_d'].info.dtype,\\\n",
    "                                                                        result_table_simbad['DEC_d'].info.dtype))\n",
    "                temp_table.add_row((coord.ra.deg, coord.dec.deg))\n",
    "                temp_table.meta['description'] = 'Simbad_q'\n",
    "                result_table_simbad = vstack([result_table_simbad, temp_table[0]])\n",
    "                final_error += 1\n",
    "            bar.update(index + limit_set*3 + 1 + limit_set_cosmos*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query remaining sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the query to a file for future executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    result_table_simbad.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    result_table_simbad.write(cat_path + 'large_cat_simbad_query_ago2020.csv', format='ascii.csv',\\\n",
    "                              overwrite=True, serialize_method='data_mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to create a copy of table to save it as `fits` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    copy_simbad_to_write = result_table_simbad\n",
    "    str_id = copy_simbad_to_write['MAIN_ID'].astype('str')\n",
    "    copy_simbad_to_write.replace_column('MAIN_ID', str_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    copy_simbad_to_write.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_simbad_flag:\n",
    "    copy_simbad_to_write.write(cat_path + 'large_cat_simbad_query_ago2020.fits', format='fits',\\\n",
    "                               overwrite=True, serialize_method='data_mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save running time, we can load the data from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if load_simbad_flag:\n",
    "#    result_table_simbad = Table.read(cat_path + 'large_cat_simbad_query.csv', format='ascii.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_simbad_flag:\n",
    "    result_table_simbad     = Table.read(cat_path + 'large_cat_simbad_query_ago2020.fits');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_table_simbad.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point, we merge the data from the query to `simbad` with the  \n",
    "values from this notebook (**Inayoshi et al., 2020** and **SDSS+FIRST**).  \n",
    "In order to do this, we convert the data into `astropy` columns, and then  \n",
    "into `astropy` tables. They will be ready to be exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_flag:\n",
    "    coords_simbad_inayoshi = SkyCoord(result_table_simbad['RA_d'], result_table_simbad['DEC_d'], unit=u.deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_flag:\n",
    "    np.shape(result_table_simbad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_flag:\n",
    "    result_table_simbad.add_columns([column_cat_index, column_cat_name, column_cat_ra, column_cat_dec, column_z_own,\\\n",
    "                                     column_z_own_err, column_L_14GHz_up, column_f_20cm, column_f_20cm_err, column_f_6cm,\\\n",
    "                                     column_f_6cm_err, column_f_3GHz, column_f_3GHz_err, column_f_15GHz, column_f_15GHz_err,\\\n",
    "                                     column_f_xmm, column_f_xmm_err, column_f_250GHz, column_f_250GHz_err, column_mass_1450,\\\n",
    "                                     column_flx_lim, column_origin, column_refs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_flag:\n",
    "    str_id = result_table_simbad['MAIN_ID'].astype('str')\n",
    "    result_table_simbad.replace_column('MAIN_ID', str_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_table = result_table_simbad.filled(fill_value=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the table into a file. It can be `.fits`, `.votable`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy_table.write('high_z_qsos.ecsv', format='ascii.ecsv', overwrite=True, serialize_method='data_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_flag:\n",
    "    result_table_simbad.write(cat_path + 'large_cat_simbad_query_inayoshi_ago2020.fits', format='fits',\\\n",
    "                              overwrite=True, serialize_method='data_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_flag:\n",
    "    result_table_simbad.write(cat_path + 'large_cat_simbad_query_inayoshi_ago2020.csv', format='ascii.csv',\\\n",
    "                              overwrite=True, serialize_method='data_mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save running time, we can load the data from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if read_simbad_inayoshi_flag:\n",
    "#    result_table_simbad = Table.read(cat_path + 'large_cat_simbad_query_inayoshi.csv', format='ascii.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_simbad_inayoshi_flag:\n",
    "    result_table_simbad = Table.read(cat_path + 'large_cat_simbad_query_inayoshi_ago2020.fits');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_simbad_inayoshi_flag or create_simbad_inayoshi_flag:\n",
    "    result_table_simbad.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the objects of the table in other catalogs and services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from astroquery.heasarc import Heasarc\n",
    "#Heasarc.query_mission_cols(mission='radio')\n",
    "#tabb = Heasarc.query(large_sample_names, mission='radio', timeout=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "customNed        = Ned()\n",
    "fields_to_remove = ['No.', 'Photometry Measurement', 'Uncertainty', 'Units', 'Significance',\\\n",
    "                    'Published frequency', 'Frequency Mode', 'Coordinates Targeted', 'Spatial Mode', 'Qualifiers', 'Comments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying sources with name in `Ned`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty_counter = 0\n",
    "# res_tab       = {}\n",
    "# for name in large_sample_names:\n",
    "#     try:\n",
    "#         res_tab[name] = customNed.get_table(name, output_table_format=1)\n",
    "#         res_tab[name].remove_columns(fields_to_remove)\n",
    "#     except:\n",
    "#         res_tab[name] = Table()\n",
    "#         empty_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can do it with coordinates.  \n",
    "\n",
    "First, we query the coordinates. If we found something,  \n",
    "we use the name of the source to obtain it photometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_simbad_inayoshi_flag or create_simbad_inayoshi_flag:\n",
    "    coords_simbad_inayoshi = SkyCoord(result_table_simbad['RA_d'], result_table_simbad['DEC_d'], unit=u.deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_tab_name_counter  = 0\n",
    "empty_tab_photo_counter = 0\n",
    "error_tab_name_counter  = 0\n",
    "error_tab_photo_counter = 0\n",
    "ned_tables              = {}\n",
    "ned_info                = {}\n",
    "ned_names               = []\n",
    "#ned_names               = np.array([''  for x in np.arange(np.shape(large_sample_names)[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_ned_names_flag:\n",
    "    with progressbar.ProgressBar(min_value=0, max_value=np.shape(large_sample_coords)[0]) as bar:\n",
    "        for index, coord in enumerate(large_sample_coords):\n",
    "            try:\n",
    "                init_table            = customNed.query_region(large_sample_coords[index], radius=2.0*u.arcsec)\n",
    "                if len(init_table) == 0:\n",
    "                    # init_table        = Table(names=('Object Name', 'RA', 'DEC'), dtype=('str', 'float', 'float'), masked=True)\n",
    "                    # init_table.add_row(('No Name', large_sample_coords[index].ra.deg,\\\n",
    "                    # large_sample_coords[index].dec.deg), mask=[True, False, False])\n",
    "                    first_row         = [['No Name'], [large_sample_coords[index].ra.deg], [large_sample_coords[index].dec.deg]]\n",
    "                    init_table        = Table(first_row, names=('Object Name', 'RA', 'DEC'), dtype=('str', 'float', 'float'), masked=True)\n",
    "                    ned_info[index]   = init_table\n",
    "                    ned_names.append('No Name')\n",
    "                    empty_tab_name_counter += 1\n",
    "                    bar.update(index)\n",
    "                    continue\n",
    "                # init_table.remove_columns(['Magnitude and Filter', 'Positions', 'Diameter Points'])\n",
    "                used_source_idx   = np.nanargmin(init_table['Separation'])  # Index of element with lowest separation from coords\n",
    "                ned_info[index]   = Table(init_table[used_source_idx])\n",
    "                init_name         = init_table['Object Name'][used_source_idx]\n",
    "                # ned_names[index]  = init_name\n",
    "                ned_names.append(init_name)\n",
    "            except:\n",
    "                # init_table        = Table(names=('Object Name', 'RA', 'DEC'), dtype=('str', 'float', 'float'), masked=True)\n",
    "                # init_table.add_row(('No Name', large_sample_coords[index].ra.deg,\\\n",
    "                # large_sample_coords[index].dec.deg), mask=[True, False, False])\n",
    "                first_row         = [['No Name'], [large_sample_coords[index].ra.deg], [large_sample_coords[index].dec.deg]]\n",
    "                init_table        = Table(first_row, names=('Object Name', 'RA', 'DEC'), dtype=('str', 'float', 'float'), masked=True)\n",
    "                ned_info[index]   = init_table\n",
    "                ned_names.append('No Name')\n",
    "                error_tab_name_counter += 1\n",
    "            bar.update(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_table            = customNed.query_region(coords_simbad_inayoshi[18022], radius=4.0*u.arcsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_ned_names_flag:\n",
    "    ned_names = np.array(ned_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_ned_names_flag:\n",
    "    ned_redshifts     = []\n",
    "    ned_redshifts_unc = []\n",
    "    with progressbar.ProgressBar(min_value=0, max_value=np.shape(ned_names)[0]) as bar:\n",
    "        for key in ned_info:\n",
    "            if np.shape(ned_info[key].colnames)[0] < 4:\n",
    "                ned_redshifts.append(np.nan)\n",
    "                ned_redshifts_unc.append(np.nan)\n",
    "                bar.update(key)\n",
    "                continue\n",
    "            ned_redshifts.append(ned_info[key]['Redshift'])\n",
    "            ned_name_temp = ned_info[key]['Object Name'][0]\n",
    "            if ned_name_temp == 'No Name':\n",
    "                ned_redshifts.append(np.nan)\n",
    "                ned_redshifts_unc.append(np.nan)\n",
    "                bar.update(key)\n",
    "                continue\n",
    "            try:\n",
    "                temp_z_table = customNed.get_table(ned_name_temp, table='redshifts')\n",
    "                ned_redshifts_unc.append(temp_z_table['Published Redshift Uncertainty'][0])\n",
    "            except:\n",
    "                ned_redshifts_unc.append(np.nan)\n",
    "            bar.update(key)\n",
    "    ned_redshifts     = np.array(ned_redshifts)\n",
    "    ned_redshifts_unc = np.array(ned_redshifts_unc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(np.isfinite(ned_redshifts_unc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(ned_redshifts_unc >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_count = 0\n",
    "indices_non   = []\n",
    "for index, name in enumerate(ned_names):\n",
    "    if name != 'No Name':\n",
    "        counter_count += 1\n",
    "for index, name in enumerate(ned_names):\n",
    "    if name == 'No Name':\n",
    "        indices_non.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_ned_names_flag:\n",
    "    ned_names[1234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_tab_temp   = customNed.get_table(str(ned_names[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_ned_photometry_flag:\n",
    "    with progressbar.ProgressBar(min_value=0, max_value=np.shape(ned_names)[0]) as bar:\n",
    "        for index, name in enumerate(ned_names):\n",
    "            try:\n",
    "                if name == 'No Name':\n",
    "                    # phot_table        = Table(names=('Observed Passband',), dtype=('str',), masked=True)\n",
    "                    # phot_table.add_row(('No Passband',), mask=(True,))\n",
    "                    first_row         = [[0], ['No Passband']]\n",
    "                    phot_table        = Table(first_row, names=('No.', 'Observed Passband'), dtype=('int', 'str'), masked=True)\n",
    "                    ned_tables[index] = phot_table\n",
    "                    empty_tab_photo_counter += 1\n",
    "                    bar.update(index)\n",
    "                    continue\n",
    "                phot_table            = customNed.get_table(name, table='photometry', output_table_format=3)\n",
    "                # phot_table.remove_columns(fields_to_remove)\n",
    "                ned_tables[index]     = phot_table\n",
    "            except:\n",
    "                # phot_table            = Table(names=('Observed Passband',), dtype=('str',), masked=True)\n",
    "                # phot_table.add_row(('No Passband',), mask=(True,))\n",
    "                first_row             = [[0], ['No Passband']]\n",
    "                phot_table            = Table(first_row, names=('No.', 'Observed Passband'), dtype=('int', 'str'), masked=True)\n",
    "                ned_tables[index]     = phot_table\n",
    "                error_tab_photo_counter += 1\n",
    "            bar.update(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_tab_temp   = customNed.get_table('Himiko', table='photometry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save multiple data tables into one file with `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ned_names[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    pass_band_name = ned_tables[key]['Observed Passband'].astype('str')\n",
    "    ned_tables[key].replace_column('Observed Passband', pass_band_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gather the names and frequencies of all columns present in tables from `Ned`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_phot_bands_list_flag:\n",
    "    band_names_array = []\n",
    "    \n",
    "    n_t_str          = '\\t'\n",
    "    \n",
    "    with progressbar.ProgressBar(min_value=0, max_value=np.shape(ned_names)[0]) as bar:\n",
    "        for key in ned_tables:\n",
    "            if ned_tables[key]['Observed Passband'][0] == 'No Passband':\n",
    "                bar.update(key)\n",
    "                continue\n",
    "            pass_band_name = ned_tables[key]['Observed Passband'].astype('str')\n",
    "            ned_tables[key].replace_column('Observed Passband', pass_band_name)\n",
    "            temp_table    = ned_tables[key]['Observed Passband', 'Frequency'].as_array().data\n",
    "            for temp_pair in temp_table:\n",
    "                band_names_array.append([str(temp_pair[0]), f'{temp_pair[1]:.4e}'])\n",
    "            bar.update(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique rows (name + frequency pair) are retrieved from the previous list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "useful_number = 0\n",
    "for key in ned_tables:\n",
    "    if np.shape(ned_tables[key].colnames)[0] >= 3:\n",
    "        useful_number += 1\n",
    "print(useful_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_phot_bands_list_flag:\n",
    "    unique_rows_band_names_array = np.unique(band_names_array, axis=0)\n",
    "    # Order by frequency\n",
    "    unique_rows_band_names_array = unique_rows_band_names_array[unique_rows_band_names_array[:, 1].astype(np.float).argsort()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the size of this new array. It represents the number of  \n",
    "unique passband configurations gathered from querying `Ned`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_phot_bands_list_flag:\n",
    "    print(np.shape(unique_rows_band_names_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_rows_band_names_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving this list into a plain text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_phot_bands_list_flag:\n",
    "    # np.savetxt(cat_path + 'all_ned_band_names.txt', unique_rows_band_names_array, fmt='%s %s', delimiter='\\t')\n",
    "    # np.savetxt(cat_path + 'all_ned_band_names_2_5arcsec.txt', unique_rows_band_names_array, fmt='%s %s', delimiter='\\t')\n",
    "    np.savetxt(cat_path + 'all_ned_band_names_ago2020.txt', unique_rows_band_names_array, fmt='%s %s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new file can also be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_phot_bands_list_flag:\n",
    "    unique_rows_band_names_array = np.genfromtxt(cat_path + 'all_ned_band_names_ago2020.txt', delimiter='\\t', dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a unique table from all the individual photometry tables obtained after  \n",
    "querying `Ned`. We discard measurements which have already been reported (for each individual source)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leave uncertainty values as string to retain information about possible upper/lower limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "if order_ned_photometry_flag:\n",
    "    limit_set_ned = int(np.floor(np.shape(ned_names)[0]/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order_ned_photometry_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if order_ned_photometry_flag:\n",
    "    temp_table_ned_photo             = Table()\n",
    "    chunk_size                       = 300  # Number of elements to calculate before dumping results to external table\n",
    "    with progressbar.ProgressBar(min_value=0, max_value=np.shape(ned_names)[0]) as bar:\n",
    "        for index, source_name in enumerate(ned_names):  # Some names will be 'No Name'\n",
    "            band_names_str           = []\n",
    "            column_names_str         = []\n",
    "            band_frequencies         = []\n",
    "            measure_names            = ned_tables[index].colnames[1:]\n",
    "            # init_table = Table(names=('MAIN_ID',), dtype=('str',), masked=True)\n",
    "            cord_str                 = large_sample_coords[index].to_string('decimal')\n",
    "            init_table               = Table(data=np.array([index, cord_str, source_name]),\\\n",
    "                                             names=('INDEX', 'COORD', 'MAIN_ID'),\\\n",
    "                                             dtype=('int', 'str', 'str'), masked=True)\n",
    "            if source_name == 'No Name':\n",
    "                if index == 0 or (index % chunk_size == 1 and index > 1):\n",
    "                    init_table_large     = Table(init_table)\n",
    "                    bar.update(index)\n",
    "                    continue\n",
    "                init_table_large     = vstack([init_table_large, init_table])\n",
    "                if index % chunk_size == 0 and index > 0:\n",
    "                    temp_table_ned_photo = vstack([temp_table_ned_photo, init_table_large])\n",
    "                bar.update(index)\n",
    "                continue\n",
    "            if len(measure_names) == 0 or len(measure_names) == 1:\n",
    "                # init_table = Table(('No Name',), names=('MAIN_ID',), dtype=('str',), masked=True)\n",
    "                # init_table.add_row((source_name,), mask=(True,))\n",
    "                if index == 0 or (index % chunk_size == 1 and index > 1):\n",
    "                    init_table_large     = Table(init_table)\n",
    "                    bar.update(index)\n",
    "                    continue\n",
    "                init_table_large     = vstack([init_table_large, init_table])\n",
    "                if index % chunk_size == 0 and index > 0:\n",
    "                    temp_table_ned_photo = vstack([temp_table_ned_photo, init_table_large])\n",
    "                    # result_table_simbad_copy = join(result_table_simbad_copy, init_table_large, keys='COORD', join_type='outer')\n",
    "                bar.update(index)\n",
    "                continue\n",
    "            # init_table = Table((str(source_name),), names=('MAIN_ID',), dtype=('str',), masked=True)\n",
    "            # init_table.add_row((str(source_name),))\n",
    "            for row in ned_tables[index]:\n",
    "                # band_name_str        = re.sub(r' \\(.*', '', str(row['Observed Passband'].decode('utf-8')))  # Eliminate differences\n",
    "                # band_name_str        = str(row['Observed Passband'].decode('utf-8'))  # Eliminate differences\n",
    "                if type(row['Observed Passband']) != bytes:\n",
    "                    band_name_str        = str(row['Observed Passband'])  # Eliminate differences\n",
    "                if type(row['Observed Passband']) == bytes:\n",
    "                    band_name_str        = str(row['Observed Passband'].decode('utf-8'))  # Eliminate differences\n",
    "                # if str(band_name_str) not in band_names_str and row['Frequency'] not in band_frequencies:\n",
    "                if str(band_name_str) not in band_names_str:\n",
    "                    band_frequencies.append(row['Frequency'])\n",
    "                    band_names_str.append(str(band_name_str))\n",
    "                    column_name_flux = 'Flux Density ' + band_name_str\n",
    "                    column_name_err  = 'NED Uncertainty ' + band_name_str\n",
    "                    if column_name_flux not in column_names_str:\n",
    "                        column_names_str.append(column_name_flux)\n",
    "                        column_flux  = MaskedColumn(row['Flux Density'], name=column_name_flux,\\\n",
    "                                                    unit=ned_tables[index]['Flux Density'].unit, dtype='float')\n",
    "                        column_err   = MaskedColumn(row['NED Uncertainty'], name=column_name_err, dtype='str')\n",
    "                        init_table.add_columns((column_flux, column_err))\n",
    "            #init_table.remove_column('MAIN_ID')\n",
    "            if index == 0 or (index % chunk_size == 1 and index > 1):\n",
    "                init_table_large     = Table(init_table)\n",
    "                bar.update(index)\n",
    "                continue\n",
    "            init_table_large         = vstack([init_table_large, init_table])\n",
    "            if index % chunk_size == 0 and index > 0:\n",
    "                temp_table_ned_photo = vstack([temp_table_ned_photo, init_table_large])\n",
    "                # result_table_simbad_copy = join(result_table_simbad_copy, init_table_large, keys='COORD', join_type='outer')\n",
    "            bar.update(index)\n",
    "    temp_table_ned_photo = vstack([temp_table_ned_photo, init_table_large])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "if order_ned_photometry_flag:\n",
    "    len(measure_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new merged column shows measurements for the following number of bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "if order_ned_photometry_flag:\n",
    "    len(temp_table_ned_photo.colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data completeness, we mask entries for `MAIN_ID` and `COORD` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    column_coord = MaskedColumn(temp_table_ned_photo['COORD'], name='COORD',\\\n",
    "                                mask=np.array(temp_table_ned_photo['COORD'] == ''))\n",
    "    column_id    = MaskedColumn(temp_table_ned_photo['MAIN_ID'], name='MAIN_ID',\\\n",
    "                                description='id_obj', mask=np.array(temp_table_ned_photo['MAIN_ID'] == 'No Name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    temp_table_ned_photo.replace_column('COORD', column_coord)\n",
    "    temp_table_ned_photo.replace_column('MAIN_ID', column_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a column with the redshift uncertainties from Ned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    temp_table_ned_photo.add_column(MaskedColumn(ned_redshifts_unc, name='ned_z_err', mask=np.array(ned_redshifts_unc <= 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform `astropy` tables into `pandas` data frames for further calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    df_ned    = temp_table_ned_photo.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    df_ned.to_hdf(cat_path + 'large_cat_inayoshi_ned_ago2020.h5', 'df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    df_simbad = result_table_simbad.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this point, we can merge both `simbad` and `Ned` tables into a larger one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    merged_ned_simbad = pd.merge(df_simbad, df_ned, on='INDEX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_ned_simbad.columns[0:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates in name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    removed_dup = pd.concat([ \n",
    "        merged_ned_simbad[merged_ned_simbad['MAIN_ID_x'].isna()], \n",
    "        merged_ned_simbad[~merged_ned_simbad['MAIN_ID_x'].isna()].drop_duplicates(subset='MAIN_ID_x', keep='first') \n",
    "    ]).reset_index(drop=True, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    print(np.shape(removed_dup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save this new table into a `HDF5`-format file for future access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_simbad_inayoshi_ned_flag:\n",
    "    removed_dup.to_hdf(cat_path + 'large_cat_simbad_inayoshi_ned_ago2020.h5', 'df')\n",
    "    # merged_ned_simbad.to_hdf(cat_path + 'large_cat_simbad_inayoshi_ned_all_cols.h5', 'df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, we can read the table from an external file to avoid extra running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_simbad_inayoshi_ned_flag:\n",
    "    power_test = pd.read_hdf(cat_path + 'large_cat_simbad_inayoshi_ned_ago2020.h5', 'df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this `pandas` data frame can be transformed into an `astropy` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# power_table = Table.from_pandas(power_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
